\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 28}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle
\noindent\textbf{Exercise 28.1-1}\\
We get the solution:

\[
\left(
\begin{array}{c}
3\\
14 - 4\cdot3\\
-7 -5 \cdot( 14 - 4 \cdot 3) + 6\cdot 3\\
\end{array}\right) = \left(
\begin{array}{c}
3\\
2\\
1 \\
\end{array}\right)
\]



\noindent\textbf{Exercise 28.1-3}\\
First, we find the LUP decomposition of the given matrix

\[
\left(\begin{array}{ccc}
1&5&4\\
2&0&3\\
5&8&2\\
\end{array} \right)
\]
 we bring the 5 to the top, and then divide the first column by 5, and use schur complements to change the rest of the matrix to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .4&-3.2&2.2\\
 .2&3.4&3.6\\
 \end{array}\right)
 \]
 Then, we swap the third and second rows, and apply the schur complement to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .2&3.4&3.6\\
 .4&-\frac{3.2}{3.4}&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
 \]
This gets us the LUP decomposition that 
\[
L =  \left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
\]
\[
U =  \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
\]
\[
P =  \left(\begin{array}{ccc}
 0&0&1\\
 1&0&0\\
 0&1&0\\
 \end{array}\right) 
\]
Using this, we can get that the solution must be
\[
\left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 12\\
 9\\
 \end{array}\right) 
\]
Which, by forward substitution,
\[
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 11\\
 7 + \frac{35.2}{3.4}\\
 \end{array}\right) 
\]
So, finally by back substitution,
\[
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 -\frac{3}{19}\\
 -\frac{1}{19}\\
 \frac{49}{19}\\
 \end{array}\right) 
\]



\noindent\textbf{Exercise 28.1-5}\\
A LU decomposition of a permutation matrix is letting $P$ be the inverse permutation matrix, and let both L and U be the identity matrix. Now, we need show that this representation is unique. We know that the permutation matrix $A$ is non-singular. This means that $U$ has nonzero elements all along its diagonal. Now, suppose that there were some nonzero element off of the diagonal in $L$, which is to say $L_{i,j}\neq 0$ for $i\neq j$. Then, look at row $i$ in the product $LU$. This has a nonzero entry both at column $j$ and at column $i$. Since it has more than one non-zero entry, it cannot be transformed into a permutation matrix by permuting the rows. Similarly, we have that $U$ cannot have any off-diagonal elements. Lastly, since we know that both $L$ and $U$ are diagonal matrices, we know that $L$ is the identity. Since $A$ only has ones as its nonzero entries, and $LU =U$. U must also only have ones as its nonzero entries. So, we have $U$ is the identity. This means that $PA =I$, which means that $P = A^{-1}$. This completes showing that the given decomposition is unique.\\



\noindent\textbf{Exercise 28.1-7}\\
For LU decomposition, it is indeed necessary. If we didn't run the last run of the outermost for loop, $u_{nn}$ would be left its initial value of zero instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the LU decomposition of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn}=0$, the determinant of $U$ will be zero by problem D.2-2.

For LUP-decomposition, the iteration of the outermost for loop that occurs with $k=n$ will not change the final answer. Since $\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.\\



\noindent\textbf{Exercise 28.2-1}\\
Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself.

The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. \begin{comment}
This can be done with a trick smelling faintly of Karatsuba's algorithm (see problem 30-1). Suppose that we wanted to multiply matrices A and B. Then, we have that
\[
(A+B)^2 = A^2 + AB +BA + B^2
\]
So,
\[
AB+BA = (A+B)^2 -A^2 -B^2
\]
and so, can be computed in time $3S(n)$. So, in time $O(S(n))$ we can compute $AB$ given $BA$. Now, we consider dividing up the matrix into four equal parts, each of which are $n/2 \times n/2$ matrices.
\[
A = \left(\begin{array}{cc} A_{11}&A_{12}\\A_{21}&A_{22}\end{array}\right)
\]
and
\[
B = \left(\begin{array}{cc} B_{11}&B_{12}\\B_{21}&B_{22}\end{array}\right)
\]
Then, our end goal will be to compute the four following expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21}\\
A_{11}B_{12} + A_{12}B_{22}\\
A_{21}B_{11} + A_{22}B_{21}\\
A_{21}B_{12} + A_{22}B_{22}\\
\end{align*}
Then, since we can compute $AB + BA$, we can look at the four entries of that to know that we can compute The following four expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + B_{11}A_{11} + B_{12}A_{21}\\
A_{11}B_{12} + A_{12}B_{22} + B_{11}A_{12} + B_{12}A_{22}\\
A_{21}B_{11} + A_{22}B_{21} + B_{21}A_{11} + B_{22}A_{21}\\
A_{21}B_{12} + A_{22}B_{22} + B_{21}A_{12} + B_{22}A_{22}\\
\end{align*}
However, since we can compute the sum of each of these products summed with the multiplication in the other order. That is, we replace $B_{ij}A_{pq}$ with $A_{pq}B_{ij} +A_{pq}^2 + B_{ij}^2 -(A_{pq} + B_{ij})^2$. Since the last three terms are things we can compute with our squaring algorithm, we know the values of the following expressions:
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + A_{11}B_{11} + A_{21}B_{12}\\
A_{11}B_{12} + A_{12}B_{22} + A_{12}B_{11} + A_{22}B_{12}\\
A_{21}B_{11} + A_{22}B_{21} + A_{11}B_{21} + A_{21}B_{22}\\
A_{21}B_{12} + A_{22}B_{22} + A_{12}B_{21} + A_{22}B_{22}\\
\end{align*}
\end{comment}

As we do this, we apply the same regularity condition that $S(2n)\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix 
\[
C = \left( \begin{array}{cc}I&A\\0&B\end{array}\right)
\]
Then, we can find $C^2$ in time $S(2n) \in O(S(n))$. Since 
\[
C^2 = \left( \begin{array}{cc}I&A+AB\\0&B\end{array}\right)
\]
Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$.\\



\noindent\textbf{Exercise 28.2-3}\\
From problem 28.2-2, we can find a LU-decomposition algorithm that only takes time $O(M(n))$. So, we run that algorithm and multiply together all of the entries along the diagonal of $U$, this will be the determinant of the original matrix.

Now, suppose that we have a determinant algorithm that takes time $D(n)$, we will show that we can find a matrix multiplication algorithm that takes time $O(D(n))$.
%notdone



\noindent\textbf{Exercise 28.2-5}\\
It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^TA$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^TA$ is positive semi-definite instead of the positive definiteness that the algorithm requires.\\



\noindent\textbf{Exercise 28.3-1}\\
To see this, let $e_i$ be the vector that is zeroes except for a one in the $i$th position. Then, we consider the quantity $e_i^TAe_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the ith column of it, and puts those values into a column vector. Then, multiplying that on the left by $e_i^T$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^TAe_i$ is exactly the value of $A_{i,i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every i, we have that every entry along the diagonal must be positive.\\



\noindent\textbf{Exercise 28.3-3}\\
Suppose to a contradiction that there were some element $a_{ij}$ with $i\neq j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a 1 at position i. Then, we consider the value $(e_i-e_j)^T A (e_i-e_j)$. When we compute $A(e_i-e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, 
\begin{align*}
(e_i-e_j)^T A (e_i-e_j) &= a_{ii} - a_{ij} - a_{ji}  +a_{jj}\\
&=a_{ii} + a_{jj} - 2 a_{ij}\le0
\end{align*}

Where we used symmetry to get that $a_{ij} =a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false.\\



\noindent\textbf{Exercise 28.3-5}\\
We will try to proceed by induction on the value of $k$. To show it for $k=0$, we need to get that the upper left corner is the largest of all the elements in the leftmost column. Suppose that there were some entry in the the left column that was larger than the element in the top row.
%not done



\noindent\textbf{Exercise 28.3-7}\\

\begin{align*}
AA^+A &= A ((A^TA)^{-1}A^T)A\\
&= A(A^TA)^{-1}(A^T A)\\
&= A
\end{align*}
\begin{align*}
A^+AA^+ &=  ((A^TA)^{-1}A^T)A((A^TA)^{-1}A^T)\\
 &= (A^TA)^{-1} (A^T A)(A^TA)^{-1} A^T\\ 
 &= (A^TA)^{-1} A^T\\
 &= A^+
\end{align*}
\begin{align*}
(AA^+)^T &= (A (A^TA)^{-1}A^T)^T\\
&= A ((A^T A)^{-1})^T A^T\\
&= A((A^T A)^T)^{-1} A^T\\
&= A(A^T A)^{-1} A^T\\
&= A A^+
\end{align*}
\begin{align*}
(AA^+)^T &= ((A^TA)^{-1}A^T A)^T\\
&= ((A^T A)^{-1}(A^T A))^T \\
&= I^T\\
&= I\\
&= (A^T A)^{-1} (A^TA)\\
&= A^+ A
\end{align*}



\noindent\textbf{Problem 28-1}\\
\begin{enumerate}[a.]
\item
By applying the procedure of the chapter, we obtain that
\[
L = \left(\begin{array}{ccccc}
1&0&0&0&0\\
-1&1&0&0&0\\
0&-1&1&0&0\\
0&0&-1&1&0\\
0&0&0&-1&1\\
\end{array}\right)
\]
\[
U = \left(\begin{array}{ccccc}
1&-1&0&0&0\\
0&1&-1&0&0\\
0&0&1&-1&0\\
0&0&0&1&-1\\
0&0&0&0&1\\
\end{array}\right)
\]
\[
P = \left(\begin{array}{ccccc}
1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0\\
0&0&0&1&0\\
0&0&0&0&1\\
\end{array}\right)
\]

\item
We first do back substitution to obtain that
\[
Ux = \left(\begin{array}{c}
5\\
4\\
3\\
2\\
1\\
\end{array}\right)
\]
So, by forward substitution, we have that
\[
x = \left(\begin{array}{c}
5\\
9\\
12\\
14\\
15\\
\end{array}\right)
\]
\item
We will set $Ax = e_i$ for each $i$, where $e_i$ is the vector that is all zeroes except for a one in the $i$th position. Then, we will just concatenate all of these solutions together to get the desired inverse.

\[
\begin{array}{|c|c|}
\hline
\hbox{equation}&\hbox{solution}\\
\hline
\hline
Ax_1 = e_1
&x_1 = \left(\begin{array}{c}
1\\
1\\
1\\
1\\
1\\
\end{array}\right)
\\
\hline
Ax_2 = e_2
&
x_2 = \left(\begin{array}{c}
1\\
2\\
2\\
2\\
2\\
\end{array}\right)
\\
\hline
Ax_3 = e_3
&
x_3 = \left(\begin{array}{c}
1\\
2\\
3\\
3\\
3\\
\end{array}\right)
\\
\hline
Ax_4 = e_4
&
x_4 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
4\\
\end{array}\right)
\\
\hline
Ax_5 = e_5
&
x_5 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
5\\
\end{array}\right)
\\
\hline
\end{array}
\]

This gets us the solution that

\[
A^{-1} = \left(\begin{array}{ccccc}
1&1&1&1&1\\
1&2&2&2&2\\
1&2&3&3&3\\
1&2&3&4&4\\
1&2&3&4&5\\
\end{array}\right)
\]
\item
When performing the LU decomposition, we only need to take the max over at most two different rows, so the loop on line 7 of LUP-DECOMPOSITION drops to $O(1)$. There are only some constant number of nonzero entries in each row, so the loop on line 14 can also be reduced to being $O(1)$. Lastly, there are only some constant number of nonzero entries of the form $a_{ik}$ and $a_{kj}$. since the square of a constant is also a constant, this means that the nested for loops on lines 16-19 also only take time O(1) to run. Since the for loops on lines 3 and 5 both run $O(n)$ times and take $O(1)$ time each to run(provided we are smart to not consider a bunch of zero entries in the matrix), the total runtime can be brought down to $O(n)$.

Since for a Tridiagonal matrix, it will only ever have finitely many nonzero entries in any row, we can do both the forward and back substitution each in time only $O(n)$.

Since the asymptotics of performing the LU decomposition on a positive definite tridiagonal matrix is $O(n)$, and this decomposition can be used to solve the equation in time $O(n)$, the total time for solving it with this method is $O(n)$. However, to simply record the inverse of the tridiagonal matrix would take time $O(n^2)$ since there are that many entries, so, any method based on computing the inverse of the matrix would take time $\Omega(n^2)$ which is clearly slower than the previous method.

\item
This was done at the beginning of the previous part.
%I think
\end{enumerate}



\end{document}
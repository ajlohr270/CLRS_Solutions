\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 28}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle
\noindent\textbf{Exercise 28.1-1}\\

We get the solution:

\[
\left(
\begin{array}{c}
3\\
14 - 4\cdot3\\
-7 -5 \cdot( 14 - 4 \cdot 3) + 6\cdot 3\\
\end{array}\right) = \left(
\begin{array}{c}
3\\
2\\
1 \\
\end{array}\right)
\]

\noindent\textbf{Exercise 28.1-2}\\

An LU decomposition of the matrix is given by 

\[ \left(\begin{array}{ccc} 4 & -5 & 6 \\ 8 & -6 & 7 \\ 12 & -7 & 12 \end{array} \right) =  \left(\begin{array}{ccc} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 2 & 1 \end{array} \right) \left(\begin{array}{ccc} 4 & -5 & 6 \\ 0 & 4 & -5 \\ 0 & 0 & 4 \end{array} \right).\]

\noindent\textbf{Exercise 28.1-3}\\

First, we find the LUP decomposition of the given matrix

\[
\left(\begin{array}{ccc}
1&5&4\\
2&0&3\\
5&8&2\\
\end{array} \right)
\]
 we bring the 5 to the top, and then divide the first column by 5, and use schur complements to change the rest of the matrix to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .4&-3.2&2.2\\
 .2&3.4&3.6\\
 \end{array}\right)
 \]
 Then, we swap the third and second rows, and apply the schur complement to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .2&3.4&3.6\\
 .4&-\frac{3.2}{3.4}&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
 \]
This gets us the LUP decomposition that 
\[
L =  \left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
\]
\[
U =  \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
\]
\[
P =  \left(\begin{array}{ccc}
 0&0&1\\
 1&0&0\\
 0&1&0\\
 \end{array}\right) 
\]
Using this, we can get that the solution must be
\[
\left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 12\\
 9\\
 \end{array}\right) 
\]
Which, by forward substitution,
\[
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 11\\
 7 + \frac{35.2}{3.4}\\
 \end{array}\right) 
\]
So, finally by back substitution,
\[
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 -\frac{3}{19}\\
 -\frac{1}{19}\\
 \frac{49}{19}\\
 \end{array}\right) 
\]

\noindent\textbf{Exercise 28.1-4}\\

The LUP decomposition of a diagonal matrix $D$ is $D = I D I$ where $I$ is the identity matrix.\\

\noindent\textbf{Exercise 28.1-5}\\

A LU decomposition of a permutation matrix is letting $P$ be the inverse permutation matrix, and let both L and U be the identity matrix. Now, we need show that this representation is unique. We know that the permutation matrix $A$ is non-singular. This means that $U$ has nonzero elements all along its diagonal. Now, suppose that there were some nonzero element off of the diagonal in $L$, which is to say $L_{i,j}\neq 0$ for $i\neq j$. Then, look at row $i$ in the product $LU$. This has a nonzero entry both at column $j$ and at column $i$. Since it has more than one non-zero entry, it cannot be transformed into a permutation matrix by permuting the rows. Similarly, we have that $U$ cannot have any off-diagonal elements. Lastly, since we know that both $L$ and $U$ are diagonal matrices, we know that $L$ is the identity. Since $A$ only has ones as its nonzero entries, and $LU =U$. U must also only have ones as its nonzero entries. So, we have $U$ is the identity. This means that $PA =I$, which means that $P = A^{-1}$. This completes showing that the given decomposition is unique.\\

\noindent\textbf{Exercise 28.1-6}\\

The zero matrix always has an LU decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular. \\

\noindent\textbf{Exercise 28.1-7}\\

For LU decomposition, it is indeed necessary. If we didn't run the last run of the outermost for loop, $u_{nn}$ would be left its initial value of zero instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the LU decomposition of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn}=0$, the determinant of $U$ will be zero by problem D.2-2.

For LUP-decomposition, the iteration of the outermost for loop that occurs with $k=n$ will not change the final answer. Since $\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.\\



\noindent\textbf{Exercise 28.2-1}\\

Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself.

The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. \begin{comment}
This can be done with a trick smelling faintly of Karatsuba's algorithm (see problem 30-1). Suppose that we wanted to multiply matrices A and B. Then, we have that
\[
(A+B)^2 = A^2 + AB +BA + B^2
\]
So,
\[
AB+BA = (A+B)^2 -A^2 -B^2
\]
and so, can be computed in time $3S(n)$. So, in time $O(S(n))$ we can compute $AB$ given $BA$. Now, we consider dividing up the matrix into four equal parts, each of which are $n/2 \times n/2$ matrices.
\[
A = \left(\begin{array}{cc} A_{11}&A_{12}\\A_{21}&A_{22}\end{array}\right)
\]
and
\[
B = \left(\begin{array}{cc} B_{11}&B_{12}\\B_{21}&B_{22}\end{array}\right)
\]
Then, our end goal will be to compute the four following expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21}\\
A_{11}B_{12} + A_{12}B_{22}\\
A_{21}B_{11} + A_{22}B_{21}\\
A_{21}B_{12} + A_{22}B_{22}\\
\end{align*}
Then, since we can compute $AB + BA$, we can look at the four entries of that to know that we can compute The following four expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + B_{11}A_{11} + B_{12}A_{21}\\
A_{11}B_{12} + A_{12}B_{22} + B_{11}A_{12} + B_{12}A_{22}\\
A_{21}B_{11} + A_{22}B_{21} + B_{21}A_{11} + B_{22}A_{21}\\
A_{21}B_{12} + A_{22}B_{22} + B_{21}A_{12} + B_{22}A_{22}\\
\end{align*}
However, since we can compute the sum of each of these products summed with the multiplication in the other order. That is, we replace $B_{ij}A_{pq}$ with $A_{pq}B_{ij} +A_{pq}^2 + B_{ij}^2 -(A_{pq} + B_{ij})^2$. Since the last three terms are things we can compute with our squaring algorithm, we know the values of the following expressions:
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + A_{11}B_{11} + A_{21}B_{12}\\
A_{11}B_{12} + A_{12}B_{22} + A_{12}B_{11} + A_{22}B_{12}\\
A_{21}B_{11} + A_{22}B_{21} + A_{11}B_{21} + A_{21}B_{22}\\
A_{21}B_{12} + A_{22}B_{22} + A_{12}B_{21} + A_{22}B_{22}\\
\end{align*}
\end{comment}

As we do this, we apply the same regularity condition that $S(2n)\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix 
\[
C = \left( \begin{array}{cc}I&A\\0&B\end{array}\right)
\]
Then, we can find $C^2$ in time $S(2n) \in O(S(n))$. Since 
\[
C^2 = \left( \begin{array}{cc}I&A+AB\\0&B\end{array}\right)
\]
Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$.\\

\noindent\textbf{Exercise 28.2-2}\\

Let $A$ be an $n \times n$ matrix.  Without loss of generality we'll assume $A$ is a power of 2, and impose the regularity condition that $L(n/2) \leq c L(n)$ where $c < 1/2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \times n$ matrix.  \\

%Not done.  For the LU case, the thought was to write A = {{A_1, A_2},{A_3, A_4}}, do a decomp of A_1 and A_4 in good time by a regularity assumption, and write A = {{L_1, 0}, {A_3U_1^{-1}, L_4}} * {{U_1, L_1^{-1}A_2}, {0, U_4 - L_4^{-1}(A_3U_1^{-1}L_1^{-1}A_2)}}.  This gives an LU decomp recursively which is nice, but it doesn't deal with the pivoting, which can't only be considered on a part of the matrix.  We need to pivot everywhere!

\noindent\textbf{Exercise 28.2-3}\\

From problem 28.2-2, we can find a LU-decomposition algorithm that only takes time $O(M(n))$. So, we run that algorithm and multiply together all of the entries along the diagonal of $U$, this will be the determinant of the original matrix.

Now, suppose that we have a determinant algorithm that takes time $D(n)$, we will show that we can find a matrix multiplication algorithm that takes time $O(D(n))$.  By the previous problem, it is enough to show that we can find matrix inverses using at most 3 determinants and an additional $\Theta(n^2)$ work. 
%notdone

Authors note on the second half of this problem: Typically we strive to complete all the exercises independently as a way of gaining a greater understanding ourselves. For the second half of this problem, this was unfortunately not something that we achieved. After been stuck for several weeks, I asked a number of other graduate students, and even two professors, none were able to help with how the proof went. One did however refer me to the book \underline{Algebraic Complexity Theory} by Burgisser, Claussen, and Shokrollahi. The proof given here for the second half of the exercise was lifted from their section 16.4. This was one of the hardest exercises in the book.

\noindent\textbf{Exercise 28.2-4}\\

Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1}\wedge b_{1j}) \vee \ldots \vee (a_{in} \wedge b_{nj})$.  To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{th}$ power of $A$.  We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2)$, and so on.  This requires only $\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\lg n)$.  

For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices.  We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$.  Start by placing $3n$ vertices down, labeling them $1,2,\ldots, n, 1', 2', \ldots, n', 1'', 2'', \ldots, n''$.  Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$.  Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which ``looks like'' an edge in $A$, then take an edge which ``looks like '' an edge in $B$. In particular, the transitive closure of this graph is:

\[ \left[ \begin{array}{ccc} I & A & AB \\ 0 & I & B \\ 0 & 0 & I \end{array} \right].\]

Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition.  Therefore multiplying matrices and finding transitive closure are are equally hard. \\

\noindent\textbf{Exercise 28.2-5}\\

It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^TA$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^TA$ is positive semi-definite instead of the positive definiteness that the algorithm requires.\\

\noindent\textbf{Exercise 28.2-6}\\

We may again assume that our matrix is a power of 2, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positive-definite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positive-definite matrices are invertible.  Finally, we need to justify that we can obtain the same asymptotic running time for amtrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \langle Ax, Ax \rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1} (A^*)^{-1}$.  Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time. \\

\noindent\textbf{Exercise 28.3-1}\\

To see this, let $e_i$ be the vector that is zeroes except for a one in the $i$th position. Then, we consider the quantity $e_i^TAe_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the ith column of it, and puts those values into a column vector. Then, multiplying that on the left by $e_i^T$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^TAe_i$ is exactly the value of $A_{i,i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every i, we have that every entry along the diagonal must be positive.\\

\noindent\textbf{Exercise 28.3-2}\\

Let $x = -by/a$.  Since $A$ is positive-definite we must have 

\begin{align*}
0 & < [x y ]^T A  \left[ \begin{array}{c} x \\ y \end{array} \right] \\
&= [x y]^T \left[ \begin{array}{c} ax + by \\ bx + cy \end{array}\right] \\
&= ax^2 + 2bxy + cy^2 \\
&= cy^2 - \frac{b^2 y^2}{a} \\
&= (c-b^2/a)y^2.
\end{align*}

Thus, $c-b^2/a > 0$ which implies $ca - b^2 > 0$, since $a>0$.  \\

\noindent\textbf{Exercise 28.3-3}\\

Suppose to a contradiction that there were some element $a_{ij}$ with $i\neq j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a 1 at position i. Then, we consider the value $(e_i-e_j)^T A (e_i-e_j)$. When we compute $A(e_i-e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, 
\begin{align*}
(e_i-e_j)^T A (e_i-e_j) &= a_{ii} - a_{ij} - a_{ji}  +a_{jj}\\
&=a_{ii} + a_{jj} - 2 a_{ij}\le0
\end{align*}

Where we used symmetry to get that $a_{ij} =a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false.\\

\noindent\textbf{Exercise 28.3-4}\\

The claim clearly holds for matrices of size 1 because the single entry in the matrix is positive the only leading submatrix is the matrix itself.  Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n+1) \times (n+1)$ symmetric positive-definite matrix. We can write $A$ as 

\[A = \left[ \begin{array}{ccc|c} & & & \\  & A' & & w \\ & & & \\ \hline & v & & c\end{array} \right]. \]

Then $A'$ is clearly symmetric, and for any $x$ we have $x^T A' x = [x 0] A \left[ \begin{array}{c} x \\ 0 \end{array} \right] > 0$, so $A'$ is positive-definite.  By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant.  By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix 

\[ B = \left[ \begin{array}{c|ccc}  c &  & v & \\ \hline & & & \\  w & & A' &  \\ & &  & \\ \end{array} \right]. \]

Theorem D.4 also tells us that  the determinant is unchanged if we add a multiple of one column of a matrix to another.  Since $0 < e_{n+1}^T A e_{n+1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$.  Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like

\[C = \left[ \begin{array}{c|ccc}  c &  & 0 & \\ \hline & & & \\  w & & A'' &  \\ & &  & \\ \end{array} \right]. \]

By definition, $\det(A) = c\det(A'')$.  By our induction hypothesis, $\det(A'') > 0$.  Since $c > 0$ as well, we conclude that $\det(A) > 0$, which completes the proof. \\

\noindent\textbf{Exercise 28.3-5}\\
When we do an LU decomposition of a positive definite symmetric matrix, we never need to permute the rows. This means that the pivot value being used from the first operation is the entry in the upper left corner. This gets us that for the case $k=1$, it holds because we were told to define $det(A_0) =1$, getting us, $a_{11} = det(A_1)/det(A_0)$. When Diagonalizing a matrix, the product of the pivot values used gives the determinant of the matrix. So, we have that the determinant of $A_k$ is a product of the $k$th pivot value with all the previous values. By induction, the product of all the previous values is $det(A_{k-1})$. So, we have that if $x$ is the kth pivot value, $det(A_k) = x det(A_{k-1})$, giving us the desired result that the $k$ th pivot value is $det(A_{k})/det(A_{k-1})$.\\



\noindent\textbf{Exercise 28.3-6}\\

First we form the $A$ matrix 

\[ A = \left[ \begin{array}{ccc} 1 & 0 & e \\ 1 & 2 & e^2 \\ 1 & 3 \lg 3 & e^3 \\ 1 & 8 & e^4 \end{array} \right]. \]

We'll compute the pseudoinverse using Wolfram Alpha, then multiply it by $y$, to obtain the coefficient vector 

\[ c = \left[ \begin{array}{c} .411741 \\ -.20487 \\ .16546 \end{array} \right].\]

\noindent\textbf{Exercise 28.3-7}\\

\begin{align*}
AA^+A &= A ((A^TA)^{-1}A^T)A\\
&= A(A^TA)^{-1}(A^T A)\\
&= A
\end{align*}
\begin{align*}
A^+AA^+ &=  ((A^TA)^{-1}A^T)A((A^TA)^{-1}A^T)\\
 &= (A^TA)^{-1} (A^T A)(A^TA)^{-1} A^T\\ 
 &= (A^TA)^{-1} A^T\\
 &= A^+
\end{align*}
\begin{align*}
(AA^+)^T &= (A (A^TA)^{-1}A^T)^T\\
&= A ((A^T A)^{-1})^T A^T\\
&= A((A^T A)^T)^{-1} A^T\\
&= A(A^T A)^{-1} A^T\\
&= A A^+
\end{align*}
\begin{align*}
(AA^+)^T &= ((A^TA)^{-1}A^T A)^T\\
&= ((A^T A)^{-1}(A^T A))^T \\
&= I^T\\
&= I\\
&= (A^T A)^{-1} (A^TA)\\
&= A^+ A
\end{align*}



\noindent\textbf{Problem 28-1}\\

\begin{enumerate}[a.]
\item
By applying the procedure of the chapter, we obtain that
\[
L = \left(\begin{array}{ccccc}
1&0&0&0&0\\
-1&1&0&0&0\\
0&-1&1&0&0\\
0&0&-1&1&0\\
0&0&0&-1&1\\
\end{array}\right)
\]
\[
U = \left(\begin{array}{ccccc}
1&-1&0&0&0\\
0&1&-1&0&0\\
0&0&1&-1&0\\
0&0&0&1&-1\\
0&0&0&0&1\\
\end{array}\right)
\]
\[
P = \left(\begin{array}{ccccc}
1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0\\
0&0&0&1&0\\
0&0&0&0&1\\
\end{array}\right)
\]

\item
We first do back substitution to obtain that
\[
Ux = \left(\begin{array}{c}
5\\
4\\
3\\
2\\
1\\
\end{array}\right)
\]
So, by forward substitution, we have that
\[
x = \left(\begin{array}{c}
5\\
9\\
12\\
14\\
15\\
\end{array}\right)
\]
\item
We will set $Ax = e_i$ for each $i$, where $e_i$ is the vector that is all zeroes except for a one in the $i$th position. Then, we will just concatenate all of these solutions together to get the desired inverse.

\[
\begin{array}{|c|c|}
\hline
\hbox{equation}&\hbox{solution}\\
\hline
\hline
Ax_1 = e_1
&x_1 = \left(\begin{array}{c}
1\\
1\\
1\\
1\\
1\\
\end{array}\right)
\\
\hline
Ax_2 = e_2
&
x_2 = \left(\begin{array}{c}
1\\
2\\
2\\
2\\
2\\
\end{array}\right)
\\
\hline
Ax_3 = e_3
&
x_3 = \left(\begin{array}{c}
1\\
2\\
3\\
3\\
3\\
\end{array}\right)
\\
\hline
Ax_4 = e_4
&
x_4 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
4\\
\end{array}\right)
\\
\hline
Ax_5 = e_5
&
x_5 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
5\\
\end{array}\right)
\\
\hline
\end{array}
\]

This gets us the solution that

\[
A^{-1} = \left(\begin{array}{ccccc}
1&1&1&1&1\\
1&2&2&2&2\\
1&2&3&3&3\\
1&2&3&4&4\\
1&2&3&4&5\\
\end{array}\right)
\]
\item
When performing the LU decomposition, we only need to take the max over at most two different rows, so the loop on line 7 of LUP-DECOMPOSITION drops to $O(1)$. There are only some constant number of nonzero entries in each row, so the loop on line 14 can also be reduced to being $O(1)$. Lastly, there are only some constant number of nonzero entries of the form $a_{ik}$ and $a_{kj}$. since the square of a constant is also a constant, this means that the nested for loops on lines 16-19 also only take time O(1) to run. Since the for loops on lines 3 and 5 both run $O(n)$ times and take $O(1)$ time each to run(provided we are smart to not consider a bunch of zero entries in the matrix), the total runtime can be brought down to $O(n)$.

Since for a Tridiagonal matrix, it will only ever have finitely many nonzero entries in any row, we can do both the forward and back substitution each in time only $O(n)$.

Since the asymptotics of performing the LU decomposition on a positive definite tridiagonal matrix is $O(n)$, and this decomposition can be used to solve the equation in time $O(n)$, the total time for solving it with this method is $O(n)$. However, to simply record the inverse of the tridiagonal matrix would take time $O(n^2)$ since there are that many entries, so, any method based on computing the inverse of the matrix would take time $\Omega(n^2)$ which is clearly slower than the previous method.

\item

The runtime of our $LUP$ decomposition algorithm drops to being $O(n)$ because we know there are only ever a constant number of nonzero entries in each row and column, as before. Once we have an $LUP decomposition, we also know that that decomposition have both $L$ and $U$ having only a constant number of non-zero entries in each row and column. This means that when we perform the forward and backward substitution, we only spend a constant amount of time per entry in $x$, and so, only takes $O(n)$ time.
\end{enumerate}

\noindent\textbf{Problem 28-2}\\

\begin{enumerate}[a.]
\item We have $a_i = f_i(0) = y_i$ and $b_i = f_i'(0) = f'(x_i) = D_i$.  Since $f_i(1) = a_i + b_i + c_i + d_i$ and $f_i'(1) = b_i + 2c_i + 3d_i$ we have $d_i = D_{i+1} - 2y_{i+1} + 2y_i + D_i$ which implies $c_i = 3y_{i+1} - 3 y_i - D_{i+1} - 2D_i$. Since each coefficient can be computed in constant time from the known values, we can compute the $4n$ coefficients in linear time.  

\item By the continuity constraints we have $f_i''(1) = f_{i+1}''(0)$ which implies that $2c_i + 6d_i = 2c_{i+1}$, or $c_i + 3d_i = c_{i+1}$. Using our equations from above, this is equivalent to 
\[ D_i + 2D_{i+1} + 3y_i - 3y_{i+1} = 3y_{i+2} - 3y_{i+1} - D_{i+2} - 2D_{i+1}.\]
Rearranging gives the desired equation
\[ D_i + 4D_{i+1} + D_{i+2} = 3(y_{i+2} - y_i).\]

\item The condition on the left endpoint tells us that $f_0''(0) = 0$, which implies $2c_0 = 0$.  By part a, this means $3(y_1-y_0) = 2D_0 + D_1$.  The condition on the right endpoint tells us that $f_{n-1}''(1) = 0$, which implies $c_{n-1} + 3d_{n-1} = 0$.  By part a, this means $3(y_n - y_{n-1}) = D_{n-1} + 2D_n$. 

\item The matrix equation has the form $AD = Y$, where $A$ is symmetric and tridiagonal.  It looks like this:

\[ \left[ \begin{array}{cccccc} 2&1&0&0&\cdots&0\\1&4&1&0&\cdots&0\\ 0&\ddots&\ddots&\ddots&\cdots&\vdots\\ \vdots&\cdots&1&4&1&0\\
0&\cdots&0&1&4&1\\0&\cdots&0&0&1&2 \end{array}\right]
\left[ \begin{array}{c} D_0 \\ D_1 \\ D_2 \\ \vdots \\ D_{n-1} \\ D_{n} \end{array}\right] = 
\left[ \begin{array}{c} 3(y_1 - y_0) \\ 3(y_2 - y_0) \\ 3(y_3 - y_1) \\ \vdots \\ 3(y_n - y_{n-2}) \\ 3(y_n - y_{n-1}) \end{array} \right]
.\]

\item Since the matrix is symmetric and tridiagonal, Problem 28-1 e tells us that we can solve the equation in $O(n)$ time by performing an LUP decomposition.  By part a of this problem, once we know each $D_i$ we can compute each $f_i$ in $O(n)$ time. 

\item For the general case of solving the nonuniform natural cubic spline problem, we require that $f(x_{i+1}) = f_i(x_{i+1} - x_i) = y_{i+1}$, $f'(x_{i+1}) = f_i'(x_{i+1}-x_i) = f_{i+1}'(0)$ and $f''(x_{i+1}) = f_i''(x_{i+1} - x_i) = f_{i+1}''(0)$. We can still solve for each of $a_i$, $b_i$, $c_i$, and $d_i$ in terms of $y_i$, $y_{i+1}$, $D_i$, and $D_{i+1}$, so we still get a tridiagonal matrix equation.  The solution will be slightly messier, but ultimately it is solved just like the simpler case, in $O(n)$ time. 
\end{enumerate}


\end{document}
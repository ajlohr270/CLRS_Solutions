\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 28}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle
\noindent\textbf{Exercise 28.1-1}\\

We get the solution:

\[
\left(
\begin{array}{c}
3\\
14 - 4\cdot3\\
-7 -5 \cdot( 14 - 4 \cdot 3) + 6\cdot 3\\
\end{array}\right) = \left(
\begin{array}{c}
3\\
2\\
1 \\
\end{array}\right)
\]

\noindent\textbf{Exercise 28.1-2}\\

An LU decomposition of the matrix is given by 

\[ \left(\begin{array}{ccc} 4 & -5 & 6 \\ 8 & -6 & 7 \\ 12 & -7 & 12 \end{array} \right) =  \left(\begin{array}{ccc} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 2 & 1 \end{array} \right) \left(\begin{array}{ccc} 4 & -5 & 6 \\ 0 & 4 & -5 \\ 0 & 0 & 4 \end{array} \right).\]

\noindent\textbf{Exercise 28.1-3}\\

First, we find the LUP decomposition of the given matrix

\[
\left(\begin{array}{ccc}
1&5&4\\
2&0&3\\
5&8&2\\
\end{array} \right)
\]
 we bring the 5 to the top, and then divide the first column by 5, and use schur complements to change the rest of the matrix to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .4&-3.2&2.2\\
 .2&3.4&3.6\\
 \end{array}\right)
 \]
 Then, we swap the third and second rows, and apply the schur complement to get
 
 \[
 \left(\begin{array}{ccc}
 5&8&2\\
 .2&3.4&3.6\\
 .4&-\frac{3.2}{3.4}&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
 \]
This gets us the LUP decomposition that 
\[
L =  \left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
\]
\[
U =  \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
\]
\[
P =  \left(\begin{array}{ccc}
 0&0&1\\
 1&0&0\\
 0&1&0\\
 \end{array}\right) 
\]
Using this, we can get that the solution must be
\[
\left(\begin{array}{ccc}
 1&0&0\\
 .2&1&0\\
 .4&-\frac{3.2}{3.4}&1\\
 \end{array}\right) 
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 12\\
 9\\
 \end{array}\right) 
\]
Which, by forward substitution,
\[
 \left(\begin{array}{ccc}
 5&8&2\\
 0&3.4&3.6\\
 0&0&2.2 +\frac{11.52}{3.4}\\
 \end{array}\right) 
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 5\\
 11\\
 7 + \frac{35.2}{3.4}\\
 \end{array}\right) 
\]
So, finally by back substitution,
\[
  \left(\begin{array}{c}
 x_1\\
 x_2\\
 x_3\\
 \end{array}\right) 
=
  \left(\begin{array}{c}
 -\frac{3}{19}\\
 -\frac{1}{19}\\
 \frac{49}{19}\\
 \end{array}\right) 
\]

\noindent\textbf{Exercise 28.1-4}\\

The LUP decomposition of a diagonal matrix $D$ is $D = I D I$ where $I$ is the identity matrix.\\

\noindent\textbf{Exercise 28.1-5}\\

A LU decomposition of a permutation matrix is letting $P$ be the inverse permutation matrix, and let both L and U be the identity matrix. Now, we need show that this representation is unique. We know that the permutation matrix $A$ is non-singular. This means that $U$ has nonzero elements all along its diagonal. Now, suppose that there were some nonzero element off of the diagonal in $L$, which is to say $L_{i,j}\neq 0$ for $i\neq j$. Then, look at row $i$ in the product $LU$. This has a nonzero entry both at column $j$ and at column $i$. Since it has more than one non-zero entry, it cannot be transformed into a permutation matrix by permuting the rows. Similarly, we have that $U$ cannot have any off-diagonal elements. Lastly, since we know that both $L$ and $U$ are diagonal matrices, we know that $L$ is the identity. Since $A$ only has ones as its nonzero entries, and $LU =U$. U must also only have ones as its nonzero entries. So, we have $U$ is the identity. This means that $PA =I$, which means that $P = A^{-1}$. This completes showing that the given decomposition is unique.\\

\noindent\textbf{Exercise 28.1-6}\\

The zero matrix always has an LU decomposition by taking $L$ to be any unit lower-triangular matrix and $U$ to be the zero matrix, which is upper triangular. \\

\noindent\textbf{Exercise 28.1-7}\\

For LU decomposition, it is indeed necessary. If we didn't run the last run of the outermost for loop, $u_{nn}$ would be left its initial value of zero instead of being set equal to $a_{nn}$. This can clearly produce incorrect results, because the LU decomposition of any non-singular matrix must have both $L$ and $U$ having positive determinant. However, if $u_{nn}=0$, the determinant of $U$ will be zero by problem D.2-2.

For LUP-decomposition, the iteration of the outermost for loop that occurs with $k=n$ will not change the final answer. Since $\pi$ would have to be a permutation on a single element, it cannot be non-trivial. and the for loop on line 16 will not run at all.\\



\noindent\textbf{Exercise 28.2-1}\\

Showing that being able to multiply matrices in time $M(n)$ implies being able to square matrices in time $M(n)$ is trivial because squaring a matrix is just multiplying it by itself.

The more tricky direction is showing that being able to square matrices in time $S(n)$ implies being able to multiply matrices in time $O(S(n))$. \begin{comment}
This can be done with a trick smelling faintly of Karatsuba's algorithm (see problem 30-1). Suppose that we wanted to multiply matrices A and B. Then, we have that
\[
(A+B)^2 = A^2 + AB +BA + B^2
\]
So,
\[
AB+BA = (A+B)^2 -A^2 -B^2
\]
and so, can be computed in time $3S(n)$. So, in time $O(S(n))$ we can compute $AB$ given $BA$. Now, we consider dividing up the matrix into four equal parts, each of which are $n/2 \times n/2$ matrices.
\[
A = \left(\begin{array}{cc} A_{11}&A_{12}\\A_{21}&A_{22}\end{array}\right)
\]
and
\[
B = \left(\begin{array}{cc} B_{11}&B_{12}\\B_{21}&B_{22}\end{array}\right)
\]
Then, our end goal will be to compute the four following expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21}\\
A_{11}B_{12} + A_{12}B_{22}\\
A_{21}B_{11} + A_{22}B_{21}\\
A_{21}B_{12} + A_{22}B_{22}\\
\end{align*}
Then, since we can compute $AB + BA$, we can look at the four entries of that to know that we can compute The following four expressions
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + B_{11}A_{11} + B_{12}A_{21}\\
A_{11}B_{12} + A_{12}B_{22} + B_{11}A_{12} + B_{12}A_{22}\\
A_{21}B_{11} + A_{22}B_{21} + B_{21}A_{11} + B_{22}A_{21}\\
A_{21}B_{12} + A_{22}B_{22} + B_{21}A_{12} + B_{22}A_{22}\\
\end{align*}
However, since we can compute the sum of each of these products summed with the multiplication in the other order. That is, we replace $B_{ij}A_{pq}$ with $A_{pq}B_{ij} +A_{pq}^2 + B_{ij}^2 -(A_{pq} + B_{ij})^2$. Since the last three terms are things we can compute with our squaring algorithm, we know the values of the following expressions:
\begin{align*}
A_{11}B_{11} + A_{12}B_{21} + A_{11}B_{11} + A_{21}B_{12}\\
A_{11}B_{12} + A_{12}B_{22} + A_{12}B_{11} + A_{22}B_{12}\\
A_{21}B_{11} + A_{22}B_{21} + A_{11}B_{21} + A_{21}B_{22}\\
A_{21}B_{12} + A_{22}B_{22} + A_{12}B_{21} + A_{22}B_{22}\\
\end{align*}
\end{comment}

As we do this, we apply the same regularity condition that $S(2n)\in O(S(n))$. Suppose that we are trying to multiply the matrices, $A$ and $B$, that is, find $AB$. Then, define the matrix 
\[
C = \left( \begin{array}{cc}I&A\\0&B\end{array}\right)
\]
Then, we can find $C^2$ in time $S(2n) \in O(S(n))$. Since 
\[
C^2 = \left( \begin{array}{cc}I&A+AB\\0&B\end{array}\right)
\]
Then we can just take the upper right quarter of $C^2$ and subtract $A$ from it to obtain the desired result. Apart from the squaring, we've only done work that is $O(n^2)$. Since $S(n)$ is $\Omega(n^2)$ anyways, we have that the total amount of work we've done is $O(n^2)$.\\

\noindent\textbf{Exercise 28.2-2}\\

Let $A$ be an $n \times n$ matrix.  Without loss of generality we'll assume $A$ is a power of 2, and impose the regularity condition that $L(n/2) \leq c L(n)$ where $c < 1/2$ and $L(n)$ is the time it takes to find an LUP decomposition of an $n \times n$ matrix.  \\

%Not done.  For the LU case, the thought was to write A = {{A_1, A_2},{A_3, A_4}}, do a decomp of A_1 and A_4 in good time by a regularity assumption, and write A = {{L_1, 0}, {A_3U_1^{-1}, L_4}} * {{U_1, L_1^{-1}A_2}, {0, U_4 - L_4^{-1}(A_3U_1^{-1}L_1^{-1}A_2)}}.  This gives an LU decomp recursively which is nice, but it doesn't deal with the pivoting, which can't only be considered on a part of the matrix.  We need to pivot everywhere!

\noindent\textbf{Exercise 28.2-3}\\

From problem 28.2-2, we can find a LU-decomposition algorithm that only takes time $O(M(n))$. So, we run that algorithm and multiply together all of the entries along the diagonal of $U$, this will be the determinant of the original matrix.

Now, suppose that we have a determinant algorithm that takes time $D(n)$, we will show that we can find a matrix multiplication algorithm that takes time $O(D(n))$.  By the previous problem, it is enough to show that we can find matrix inverses using at most 3 determinants and an additional $\Theta(n^2)$ work. 
%notdone

Authors note on the second half of this problem: Typically we strive to complete all the exercises independently as a way of gaining a greater understanding ourselves. For the second half of this problem, this was unfortunately not something that we achieved. After been stuck for several weeks, I asked a number of other graduate students, and even two professors, none were able to help with how the proof went. One did however refer me to the book \underline{Algebraic Complexity Theory} by Burgisser, Claussen, and Shokrollahi. The proof given here for the second half of the exercise was lifted from their section 16.4. This was one of the hardest exercises in the book.

\noindent\textbf{Exercise 28.2-4}\\

Suppose we can multiply boolean matrices in $M(n)$ time, where we assume this means that if we're multiplying boolean matrices $A$ and $B$, then $(AB)_{ij} = (a_{i1}\wedge b_{1j}) \vee \ldots \vee (a_{in} \wedge b_{nj})$.  To find the transitive closure of a boolean matrix $A$ we just need to find the $n^{th}$ power of $A$.  We can do this by computing $A^2$, then $(A^2)^2$, then $((A^2)^2)^2)$, and so on.  This requires only $\lg n$ multiplications, so the transitive closure can be computed in $O(M(n)\lg n)$.  

For the other direction, first view $A$ and $B$ as adjacency matrices, and impose the regularity condition $T(3n) = O(T(n))$, where $T(n)$ is the time to compute the transitive closure of a graph on $n$ vertices.  We will define a new graph whose transitive closure matrix contains the boolean product of $A$ and $B$.  Start by placing $3n$ vertices down, labeling them $1,2,\ldots, n, 1', 2', \ldots, n', 1'', 2'', \ldots, n''$.  Connect vertex $i$ to vertex $j'$ if and only if $A_{ij} = 1$.  Connect vertex $j'$ to vertex $k''$ if and only if $B_{jk} = 1$. In the resulting graph, the only way to get from the first set of $n$ vertices to the third set is to first take an edge which ``looks like'' an edge in $A$, then take an edge which ``looks like '' an edge in $B$. In particular, the transitive closure of this graph is:

\[ \left[ \begin{array}{ccc} I & A & AB \\ 0 & I & B \\ 0 & 0 & I \end{array} \right].\]

Since the graph is only of size $3n$, computing its transitive closure can be done in $O(T(3n)) = O(T(n))$ by the regularity condition.  Therefore multiplying matrices and finding transitive closure are are equally hard. \\

\noindent\textbf{Exercise 28.2-5}\\

It does not work necessarily over the field of two elements. The problem comes in in applying theorem D.6 to conclude that $A^TA$ is positive definite. In the proof of that theorem they obtain that $||Ax||^2 \ge 0$ and only zero if every entry of $Ax$ is zero. This second part is not true over the field with two elements, all that would be required is that there is an even number of ones in $Ax$. This means that we can only say that $A^TA$ is positive semi-definite instead of the positive definiteness that the algorithm requires.\\

\noindent\textbf{Exercise 28.2-6}\\

We may again assume that our matrix is a power of 2, this time with complex entries. For the moment we assume our matrix $A$ is Hermitian and positive-definite. The proof goes through exactly as before, with matrix transposes replaced by conjugate transposes, and using the fact that Hermitian positive-definite matrices are invertible.  Finally, we need to justify that we can obtain the same asymptotic running time for amtrix multiplication as for matrix inversion when $A$ is invertible, but not Hermitian positive-definite. For any nonsingular matrix $A$, the matrix $A^*A$ is Hermitian and positive definite, since for any $x$ we have $x^*A^*Ax = \langle Ax, Ax \rangle > 0$ by the definition of inner product. To invert $A$, we first compute $(A^*A)^{-1} = A^{-1} (A^*)^{-1}$.  Then we need only multiply this result on the right by $A^*$. Each of these steps takes $O(M(n))$ time, so we can invert any nonsingluar matrix with complex entries in $O(M(n))$ time. \\

\noindent\textbf{Exercise 28.3-1}\\

To see this, let $e_i$ be the vector that is zeroes except for a one in the $i$th position. Then, we consider the quantity $e_i^TAe_i$ for every $i$. $Ae_i$ takes each row of $A$ and pulls out the ith column of it, and puts those values into a column vector. Then, multiplying that on the left by $e_i^T$, pulls out the $i$th row of this quantity, which means that the quantity $e_i^TAe_i$ is exactly the value of $A_{i,i}$. So, we have that by positive definiteness, since $e_i$ is nonzero, that quantity must be positive. Since we do this for every i, we have that every entry along the diagonal must be positive.\\

\noindent\textbf{Exercise 28.3-2}\\

Let $x = -by/a$.  Since $A$ is positive-definite we must have 

\begin{align*}
0 & < [x y ]^T A  \left[ \begin{array}{c} x \\ y \end{array} \right] \\
&= [x y]^T \left[ \begin{array}{c} ax + by \\ bx + cy \end{array}\right] \\
&= ax^2 + 2bxy + cy^2 \\
&= cy^2 - \frac{b^2 y^2}{a} \\
&= (c-b^2/a)y^2.
\end{align*}

Thus, $c-b^2/a > 0$ which implies $ca - b^2 > 0$, since $a>0$.  \\

\noindent\textbf{Exercise 28.3-3}\\

Suppose to a contradiction that there were some element $a_{ij}$ with $i\neq j$ so that $a_{ij}$ were a largest element. We will use $e_i$ to denote the vector that is all zeroes except for having a 1 at position i. Then, we consider the value $(e_i-e_j)^T A (e_i-e_j)$. When we compute $A(e_i-e_j)$ this will return a vector which is column $i$ minus column $j$. Then, when we do the last multiplication, we will get the quantity which is the $i$th row minus the $j$th row. So, 
\begin{align*}
(e_i-e_j)^T A (e_i-e_j) &= a_{ii} - a_{ij} - a_{ji}  +a_{jj}\\
&=a_{ii} + a_{jj} - 2 a_{ij}\le0
\end{align*}

Where we used symmetry to get that $a_{ij} =a_{ji}$. This result contradicts the fact that $A$ was positive definite. So, our assumption that there was a element tied for largest off the diagonal must of been false.\\

\noindent\textbf{Exercise 28.3-4}\\

The claim clearly holds for matrices of size 1 because the single entry in the matrix is positive the only leading submatrix is the matrix itself.  Now suppose the claim holds for matrices of size $n$, and let $A$ be an $(n+1) \times (n+1)$ symmetric positive-definite matrix. We can write $A$ as 

\[A = \left[ \begin{array}{ccc|c} & & & \\  & A' & & w \\ & & & \\ \hline & v & & c\end{array} \right]. \]

Then $A'$ is clearly symmetric, and for any $x$ we have $x^T A' x = [x 0] A \left[ \begin{array}{c} x \\ 0 \end{array} \right] > 0$, so $A'$ is positive-definite.  By our induction hypothesis, every leading submatrix of $A'$ has positive determinant, so we are left only to show that $A$ has positive determinant.  By Theorem D.4, the determinant of $A$ is equal to the determinant of the matrix 

\[ B = \left[ \begin{array}{c|ccc}  c &  & v & \\ \hline & & & \\  w & & A' &  \\ & &  & \\ \end{array} \right]. \]

Theorem D.4 also tells us that  the determinant is unchanged if we add a multiple of one column of a matrix to another.  Since $0 < e_{n+1}^T A e_{n+1} = c$, we can use multiples of the first column to zero out every entry in the first row other than $c$.  Specifically, the determinant of $B$ is the same as the determinant of the matrix obtained in this way, which looks like

\[C = \left[ \begin{array}{c|ccc}  c &  & 0 & \\ \hline & & & \\  w & & A'' &  \\ & &  & \\ \end{array} \right]. \]

By definition, $\det(A) = c\det(A'')$.  By our induction hypothesis, $\det(A'') > 0$.  Since $c > 0$ as well, we conclude that $\det(A) > 0$, which completes the proof. \\

\noindent\textbf{Exercise 28.3-5}\\

We will try to proceed by induction on the value of $k$. To show it for $k=0$, we need to get that the upper left corner is the largest of all the elements in the leftmost column. Suppose that there were some entry (say in row $i$) in the the left column that was larger than the element in the top row. Since we have that the matrix is positive definite, we could pick $x = e_1 -  e_i$, then we would have that $x^T A x$ would be negative. \\
%not done

\noindent\textbf{Exercise 28.3-6}\\

First we form the $A$ matrix 

\[ A = \left[ \begin{array}{ccc} 1 & 0 & e \\ 1 & 2 & e^2 \\ 1 & 3 \lg 3 & e^3 \\ 1 & 8 & e^4 \end{array} \right]. \]

We'll compute the pseudoinverse using Wolfram Alpha, then multiply it by $y$, to obtain the coefficient vector 

\[ c = \left[ \begin{array}{c} .411741 \\ -.20487 \\ .16546 \end{array} \right].\]

\noindent\textbf{Exercise 28.3-7}\\

\begin{align*}
AA^+A &= A ((A^TA)^{-1}A^T)A\\
&= A(A^TA)^{-1}(A^T A)\\
&= A
\end{align*}
\begin{align*}
A^+AA^+ &=  ((A^TA)^{-1}A^T)A((A^TA)^{-1}A^T)\\
 &= (A^TA)^{-1} (A^T A)(A^TA)^{-1} A^T\\ 
 &= (A^TA)^{-1} A^T\\
 &= A^+
\end{align*}
\begin{align*}
(AA^+)^T &= (A (A^TA)^{-1}A^T)^T\\
&= A ((A^T A)^{-1})^T A^T\\
&= A((A^T A)^T)^{-1} A^T\\
&= A(A^T A)^{-1} A^T\\
&= A A^+
\end{align*}
\begin{align*}
(AA^+)^T &= ((A^TA)^{-1}A^T A)^T\\
&= ((A^T A)^{-1}(A^T A))^T \\
&= I^T\\
&= I\\
&= (A^T A)^{-1} (A^TA)\\
&= A^+ A
\end{align*}



\noindent\textbf{Problem 28-1}\\

\begin{enumerate}[a.]
\item
By applying the procedure of the chapter, we obtain that
\[
L = \left(\begin{array}{ccccc}
1&0&0&0&0\\
-1&1&0&0&0\\
0&-1&1&0&0\\
0&0&-1&1&0\\
0&0&0&-1&1\\
\end{array}\right)
\]
\[
U = \left(\begin{array}{ccccc}
1&-1&0&0&0\\
0&1&-1&0&0\\
0&0&1&-1&0\\
0&0&0&1&-1\\
0&0&0&0&1\\
\end{array}\right)
\]
\[
P = \left(\begin{array}{ccccc}
1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0\\
0&0&0&1&0\\
0&0&0&0&1\\
\end{array}\right)
\]

\item
We first do back substitution to obtain that
\[
Ux = \left(\begin{array}{c}
5\\
4\\
3\\
2\\
1\\
\end{array}\right)
\]
So, by forward substitution, we have that
\[
x = \left(\begin{array}{c}
5\\
9\\
12\\
14\\
15\\
\end{array}\right)
\]
\item
We will set $Ax = e_i$ for each $i$, where $e_i$ is the vector that is all zeroes except for a one in the $i$th position. Then, we will just concatenate all of these solutions together to get the desired inverse.

\[
\begin{array}{|c|c|}
\hline
\hbox{equation}&\hbox{solution}\\
\hline
\hline
Ax_1 = e_1
&x_1 = \left(\begin{array}{c}
1\\
1\\
1\\
1\\
1\\
\end{array}\right)
\\
\hline
Ax_2 = e_2
&
x_2 = \left(\begin{array}{c}
1\\
2\\
2\\
2\\
2\\
\end{array}\right)
\\
\hline
Ax_3 = e_3
&
x_3 = \left(\begin{array}{c}
1\\
2\\
3\\
3\\
3\\
\end{array}\right)
\\
\hline
Ax_4 = e_4
&
x_4 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
4\\
\end{array}\right)
\\
\hline
Ax_5 = e_5
&
x_5 = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
5\\
\end{array}\right)
\\
\hline
\end{array}
\]

This gets us the solution that

\[
A^{-1} = \left(\begin{array}{ccccc}
1&1&1&1&1\\
1&2&2&2&2\\
1&2&3&3&3\\
1&2&3&4&4\\
1&2&3&4&5\\
\end{array}\right)
\]
\item
When performing the LU decomposition, we only need to take the max over at most two different rows, so the loop on line 7 of LUP-DECOMPOSITION drops to $O(1)$. There are only some constant number of nonzero entries in each row, so the loop on line 14 can also be reduced to being $O(1)$. Lastly, there are only some constant number of nonzero entries of the form $a_{ik}$ and $a_{kj}$. since the square of a constant is also a constant, this means that the nested for loops on lines 16-19 also only take time O(1) to run. Since the for loops on lines 3 and 5 both run $O(n)$ times and take $O(1)$ time each to run(provided we are smart to not consider a bunch of zero entries in the matrix), the total runtime can be brought down to $O(n)$.

Since for a Tridiagonal matrix, it will only ever have finitely many nonzero entries in any row, we can do both the forward and back substitution each in time only $O(n)$.

Since the asymptotics of performing the LU decomposition on a positive definite tridiagonal matrix is $O(n)$, and this decomposition can be used to solve the equation in time $O(n)$, the total time for solving it with this method is $O(n)$. However, to simply record the inverse of the tridiagonal matrix would take time $O(n^2)$ since there are that many entries, so, any method based on computing the inverse of the matrix would take time $\Omega(n^2)$ which is clearly slower than the previous method.

\item

The runtime of our $LUP$ decomposition algorithm drops to being $O(n)$ because we know there are only ever a constant number of nonzero entries in each row and column, as before. Once we have an $LUP decomposition, we also know that that decomposition have both $L$ and $U$ having only a constant number of non-zero entries in each row and column. This means that when we perform the forward and backward substitution, we only spend a constant amount of time per entry in $x$, and so, only takes $O(n)$ time.
\end{enumerate}

\noindent\textbf{Problem 28-2}\\

\begin{enumerate}[a.]
\item We have $a_i = f_i(0) = y_i$ and $b_i = f_i'(0) = f'(x_i) = D_i$.  Since $f_i(1) = a_i + b_i + c_i + d_i$ and $f_i'(1) = b_i + 2c_i + 3d_i$ we have $d_i = D_{i+1} - 2y_{i+1} + 2y_i + D_i$ which implies $c_i = 3y_{i+1} - 3 y_i - D_{i+1} - 2D_i$. Since each coefficient can be computed in constant time from the known values, we can compute the $4n$ coefficients in linear time.  

\item By the continuity constraints we have $f_i''(1) = f_{i+1}''(0)$ which implies that $2c_i + 6d_i = 2c_{i+1}$, or $c_i + 3d_i = c_{i+1}$. Using our equations from above, this is equivalent to 
\[ D_i + 2D_{i+1} + 3y_i - 3y_{i+1} = 3y_{i+2} - 3y_{i+1} - D_{i+2} - 2D_{i+1}.\]
Rearranging gives the desired equation
\[ D_i + 4D_{i+1} + D_{i+2} = 3(y_{i+2} - y_i).\]

\item The condition on the left endpoint tells us that $f_0''(0) = 0$, which implies $2c_0 = 0$.  By part a, this means $3(y_1-y_0) = 2D_0 + D_1$.  The condition on the right endpoint tells us that $f_{n-1}''(1) = 0$, which implies $c_{n-1} + 3d_{n-1} = 0$.  By part a, this means $3(y_n - y_{n-1}) = D_{n-1} + 2D_n$. 

\item The matrix equation has the form $AD = Y$, where $A$ is symmetric and tridiagonal.  It looks like this:

\[ \left[ \begin{array}{cccccc} 2&1&0&0&\cdots&0\\1&4&1&0&\cdots&0\\ 0&\ddots&\ddots&\ddots&\cdots&\vdots\\ \vdots&\cdots&1&4&1&0\\
0&\cdots&0&1&4&1\\0&\cdots&0&0&1&2 \end{array}\right]
\left[ \begin{array}{c} D_0 \\ D_1 \\ D_2 \\ \vdots \\ D_{n-1} \\ D_{n} \end{array}\right] = 
\left[ \begin{array}{c} 3(y_1 - y_0) \\ 3(y_2 - y_0) \\ 3(y_3 - y_1) \\ \vdots \\ 3(y_n - y_{n-2}) \\ 3(y_n - y_{n-1}) \end{array} \right]
.\]

\item Since the matrix is symmetric and tridiagonal, Problem 28-1 e tells us that we can solve the equation in $O(n)$ time by performing an LUP decomposition.  By part a of this problem, once we know each $D_i$ we can compute each $f_i$ in $O(n)$ time. 

\item For the general case of solving the nonuniform natural cubic spline problem, we require that $f(x_{i+1}) = f_i(x_{i+1} - x_i) = y_{i+1}$, $f'(x_{i+1}) = f_i'(x_{i+1}-x_i) = f_{i+1}'(0)$ and $f''(x_{i+1}) = f_i''(x_{i+1} - x_i) = f_{i+1}''(0)$. We can still solve for each of $a_i$, $b_i$, $c_i$, and $d_i$ in terms of $y_i$, $y_{i+1}$, $D_i$, and $D_{i+1}$, so we still get a tridiagonal matrix equation.  The solution will be slightly messier, but ultimately it is solved just like the simpler case, in $O(n)$ time. 
\end{enumerate}


\end{document}
\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 25}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}

\begin{document}
\maketitle
\noindent\textbf{Exercise 25.1-1}\\
First, the slow way:

\[
L^{(1)} = \left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&\infty&\infty\\
\infty&2&0&\infty&\infty&-8\\
-4&\infty&\infty&0&3&\infty\\
\infty&7&\infty&\infty&0&\infty\\
\infty&5&10&\infty&\infty&0\\
\end{array}\right)
\]

\[
L^{(2)} = \left(\begin{array}{cccccc}
0&6&\infty&\infty&-1&\infty\\
-2&0&\infty&2&0&\infty\\
3&-3&0&4&\infty&-8\\
-4&10&\infty&0&-5&\infty\\
8&7&\infty&9&0&\infty\\
6&5&10&7&\infty&0\\
\end{array}\right)
\]

\[
L^{(3)} = \left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-2&-3&0&-1&2&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&5&0\\
\end{array}\right)
\]

\[
L^{(4)} = \left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-5&-3&0&-1&-3&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}\right)
\]

\[
L^{(5)} = \left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-5&-3&0&-1&-6&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}\right)
\]

Then, since we have reached $L^{(n-1)}$ we can stop since we know that since there are no negative weight cycles, taking higher powers will not cause the matrix to change at all. By the fast method, we get that

\[
L^{(1)} = \left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&\infty&\infty\\
\infty&2&0&\infty&\infty&-8\\
-4&\infty&\infty&0&3&\infty\\
\infty&7&\infty&\infty&0&\infty\\
\infty&5&10&\infty&\infty&0\\
\end{array}\right)
\]

\[
L^{(2)} = \left(\begin{array}{cccccc}
0&6&\infty&\infty&-1&\infty\\
-2&0&\infty&2&0&\infty\\
3&-3&0&4&\infty&-8\\
-4&10&\infty&0&-5&\infty\\
8&7&\infty&9&0&\infty\\
6&5&10&7&\infty&0\\
\end{array}\right)
\]

\[
L^{(4)} = \left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-5&-3&0&-1&-3&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}\right)
\]

\[
L^{(8)} = \left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-5&-3&0&-1&-6&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}\right)
\]

We stop since $8\ge 5 =n-1$.

\noindent\textbf{Exercise 25.1-3}\\
This matrix corresponds to the identity matrix in normal matrix multiplication. This is because anytime that we take a min of any number with infinity, we get that same number back. Another way of seeing this is that we can interpret this strange version of matrix multipllication as allowing any of our shortest paths to be a path described by one matrix followed by a path described by the other. However, the given matrix corresponds to there being no paths between any of the vertices. This means that we won't change any of the shortest paths that are described by the matrix we are multiplying it by since we are allowing nothing more for those paths.\\


\noindent\textbf{Exercise 25.1-5}\\
We can express finding the shortest path from a single vertex with the modified version of matrix multiplication described in the section. We initially let $v_1$ be a vector indexed by the vertices of the graph. It is infinity when the corresponding vertex has no edge going to it from th esource vertex, $s$. It is the weight of the edge going to it from $s$ if there is one. Lastly, it is zero in the entry corresponding to $s$ itself. Essentially, we are only taking the row of the $W$ matrix that corresponds to $s$. Then, we define $v_{i+1} = v_i W$. Then, we stop computing $v_i$ once we compute $v_{n-1}$. This vector then contains the correct shortest distances from the source to each vertex. Since each time we multiply the vector by the matrix, we only have to consider the entries which are non-infinite in $W$. There are only $|E|+|V|$ of these non-finfinite entries. So, we have that the time required for each time we mutiply the vector by the matrix, we take time at most $O(E)$. So, the total runtime would be $O(EV)$ just as in Bellman-Ford. The similarities don't stop there however. This is because $v_i$ represents the shortest distance to each vector from $s$ using at most $i$ edges, and each time we multiply by $W$ corresponds to relaxing every edge.\\


\noindent\textbf{Exercise 25.1-7}\\
%notdone

\noindent\textbf{Exercise 25.1-9}\\
For the modification, keep computing for one step more than the original, that is, we compute all the way up to $L^{(2^{k+1})}$ where $2^{k} >n-1$. Then, if there aren't any negative weight cycles, then, we will have that the two matrices should be equal since having no negative weight cycles means that between any two vertices, there is a path that is tied for shortest and contains at most $n-1$ edges. However, if there is a cycle of negative total weight, we know that it's length is at most $n$, so, since we are allowing paths to be larger by $2^k\ge n$ between these two matrices, we have that we would need to have all of the vertices on the cycle have their distance reduce by at least the negative weight of the cycle. Since we can detect exactly when there is a negative cycle, based on when these two matrices are different. This algorithm works. It also only takes time equal to a single matrix multiplication which is littlee oh of the unmodified algorithm.\\

\noindent\textbf{Exercise 25.2-1}\\
$
\begin{array}{|c|c|}
k & D^k\\
\hline
0&\left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&\infty&\infty\\
\infty&2&0&\infty&\infty&-8\\
-4&\infty&\infty&0&3&\infty\\
\infty&7&\infty&\infty&0&\infty\\
\infty&5&10&\infty&\infty&0\\
\end{array}
\right)\\
\hline
1&\left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&0&\infty\\
\infty&2&0&\infty&\infty&-8\\
-4&\infty&\infty&0&-5&\infty\\
\infty&7&\infty&\infty&0&\infty\\
\infty&5&10&\infty&\infty&0\\
\end{array}
\right)\\
\hline
2&\left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&0&\infty\\
3&2&0&4&2&-8\\
-4&\infty&\infty&0&-5&\infty\\
8&7&\infty&9&0&\infty\\
6&5&10&7&5&0\\
\end{array}
\right)\\
\hline
3&\left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
1&0&\infty&2&0&\infty\\
3&2&0&4&2&-8\\
-4&\infty&\infty&0&-5&\infty\\
8&7&\infty&9&0&\infty\\
6&5&10&7&5&0\\
\end{array}
\right)\\
\hline
4&\left(\begin{array}{cccccc}
0&\infty&\infty&\infty&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
0&2&0&4&-1&-8\\
-4&\infty&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}
\right)\\
\hline
5&\left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
0&2&0&4&-1&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}
\right)\\
\hline
6&\left(\begin{array}{cccccc}
0&6&\infty&8&-1&\infty\\
-2&0&\infty&2&-3&\infty\\
-5&-3&0&-1&-6&-8\\
-4&2&\infty&0&-5&\infty\\
5&7&\infty&9&0&\infty\\
3&5&10&7&2&0\\
\end{array}
\right)\\
\hline\end{array}
$\\

\noindent\textbf{Exercise 25.2-3}\\
See the modified version of the Floyd-Warshall algorithm:
\begin{algorithm}
\caption{MOD-FLOYD-WARSHALL(W)}
\begin{algorithmic}
\State n= W.rows
\State $D^0 = W$
\State $\pi^0$ is a matrix with nil in every entry
\For{i=1 to n}
\For{j = 1 to n}
\If{$i\neq j$ and $D^0_{i,j} <\infty$}
\State $\pi^0_{i,j} = i$
\EndIf
\EndFor
\EndFor
\For{ k=1 to n}
\State let $D^k$ be a new $n\times n$ matrix.
\State let $\pi^k$ be a new $n\times n$ matrix
\For{i=1 to n}
\For{j = 1 to n}
\If{$d_{ij}^{k-1} \le d_{i,k}^{k-1}+ d_{k,j}^{k-1}$}
\State $d_{i,j}^{k} = d_{i,j}^{k-1}$
\State $\pi_{i,j}^k  =\pi_{i,j}^{k-1}$ 
\Else
\State $d_{i,j}^k =  d_{i,k}^{k-1}+ d_{k,j}^{k-1}$
\State $\pi_{i,j}^k = \pi_{k,j}^{k-1}$
\EndIf
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

In order to have that $\pi_{ij}^{(k)} = l$, we need that $d_{ij}^{(k)} \ge d_{il}^{(k)} + w_{lj}$. To see this fact, we will note that having $\pi_{ij}^{(k)} = l$ means that a shortest path from $i$ to $j$ last goes through $l$. A path that last goes through $l$ corresponds to taking a chepest path from $i$ to $l$ and then following the single edge from $l$ to $j$. However, This means that $d_{il} \le d_{ij} - w_{ij}$, which we can rearrange to get the desired inequality. We can just continue following this inequality around, and if we ever get some cycle, $i_1,i_2,\ldots i_c$, then we would have that $d_{i i_1} \le d_{i i_1} + w_{i_1 i_2} + w_{i_2 i_3} + \cdots + w_{i_c i_1}$. So, if we subtract the common term from both sides, we get that $0 \le w_{i_c i_1}+ \sum_{q =1}^{c-1} w_{i_q i_{q+1}}$. So, we have that we would only have a cycle in the precedessor graph if we ahvt that there is a zero weight cycle in the original graph. However, we would never have to go around the weight zero cycle since the constructed path of shortest weight favors ones with a fewer number of edges because of the way that we handle the equality case in equation (25.7).\\


\noindent\textbf{Exercise 25.2-5}\\
If we change the way that we handle the equality case, we willl still be generating a the correct values for the $\pi$ matrix. This is because updating the $\pi$ values to make paths that are longer but still tied for the lowest weight. Making $\pi_{ij} = \pi_{kj}$ means that we are making the shortest path from $i$ to $j$ passes through $k$ at some point. This has the same cost as just going from $i$ to $j$, since $d_{ij} = d_{ik}+d_{kj}$.\\


\noindent\textbf{Exercise 25.2-7}\\


\noindent\textbf{Exercise 25.2-9}\\
First, compute the strongly connected components of the directed graph, and look at it's component graph. This component graph is going to be acyclic and have at most as many vertices and at most as many edges as the original graph. Since it is acyclic, we can run our transitive closure algorithm on it. Then, for every edge $(S_1,S_2)$ that shows up in the transitive closure of the component graph, we add an edge from each vertex in $S_1$ to a vertex in $S_2$. This takes time equal to $O(V+E^*)$. So, the total time required is $\le f(|V|,|E|) + O(V+E)$.\\

\noindent\textbf{Exercise 25.3-1}\\



\noindent\textbf{Exercise 25.3-3}\\
If all the edge weights are nonnegative, then the values computed as the shortest distances when running Bellman-Ford will be all zero. This is because when constructing $G'$ on the first line of Johnson's algorithm, we place an edge of weight zero from $s$ to every other vertex. Since any path within the graph has no negative edges, its cost cannot be negative, and so, cannot beat the trivial path that goes straight from $s$ to any given vertex. Since we have that $h(u) = h(v)$ for every $u$ and $v$, the reweighting that occurs only adds and subtracts 0, and so we have that $w(u,v) = \hat w(u,v)$\\


\noindent\textbf{Exercise 25.3-5}\\
By lemma 25.1, we have that the total weight of any cycles is unchanged as a result of the reweighting procedure. This can be seen in a way similar to how the last claim of lemma 25.1 was proven. Namely, we consider the cycle c as a path that has the same starting and ending vertices, so, by the first half od lemma 25.1, we have that
\[
\hat w(c) = w(c) + h(v_0) -h(v_k) = w(c) = 0
 \]
This means that in the reweighted graph, we still have that the same cycle as before had a total weight of zero. Since there are no longer any negative weight edges after we reweight, this is precicely the second property of the reweighting procedure shown in the section. Since we have that the sum of all the edge weights in $c$ is still equal to zero, but each of them individually has a non-negative weight, it must be the case that each of them individually is equal to 0.

\noindent\textbf{Problem 25-1}\\
\begin{enumerate}[a.]
\item
We can update the transitive closure in time $O(V^2)$ as follows. Suppose that we add the edge $(x_1,x_2)$. Then, we will consider every pair of vertices $(u,v)$. In order to of created a path between them, we would need some part of that path that goes from $u$ to $x_1$ and some second part of that path that goes from $x_2$ to $v$. This means that we add the edge $(u,v)$ to the transitive closure if and only if the transitive closure contains the edges $(u,x_1)$ and $(x_2,v)$. Since we only had to consider every pair of vertices once, the runtime of this update is only $O(V^2)$.
\item
Suppose that we currently have two strongly connected components, each of size $|V|/2$ with no edges between them. Then their transitive closures computed so far will consist of two complete directed graphs on $|V|/2$ vertices each. So, there will be a total of $|V|^2/2$ edges adding the number of edges in each together.

Then, we add a single edge from one component to the other. This will mean that every vertex in the component the edge is coming from will have an edge going to every vertex in the component that the edge is going to. So, the total number of edges after this operation will be $|V|/2 +|V|/4$. So, the number of edges increased by $|V|/4$. Since each time we add an edge, we need to use at least constant time, since there is no cheap way to add many edges at once, the total amount of time needed is $\Omega(|V|^2)$.

\item


\end{enumerate}



\end{document}
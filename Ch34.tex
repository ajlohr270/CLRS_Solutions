\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 34}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\Z}{\mathbb{Z}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle

\noindent\textbf{Exercise 34.1-1}\\

Showing that LONGEST-PATH-LENGTH being polynomial implies that LONGEST-PATH is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of LONGEST-PATH if and only if k is larger than the number we computed as the length of the longest path.

Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of LONGEST-PATH with the given parameters along with $k = \frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\lg(|E|))$ many steps. However, running a polynomial time subroutine $\lg(n)$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of LONGEST-PATH into the next.\\



\noindent\textbf{Exercise 34.1-3}\\

A formal encoding of the adjacency matrix representation is to first encode an integer $n$ in the usual binary encoding representing the number of vertices. Then, there will be $n^2$ bits following. The value of bit $m$ will be 1 if there is an edge from vertex $\lfloor m/n \rfloor$ to vertex $(m\%n)$, and zero if there is not such an edge.

An encoding of the adjacency list representation is a bit more finessed. We'll be using a different encoding of integers, call it $g(n)$. In particular, we will place a 0 immediately after every bit in the usual representation. Since this only doubles the length of the encoding, it is still polynomially related. Also, the reason we will be using this encoding is because any sequence of integers encoded in this way cannot contain the string $11$ and must contain at least one zero. Suppose that we have a vertex with edges going to the vertices indexed by $i_1,i_2,i_3,\ldots, i_k$. Then, the encoding corresponding to that vertex is $g(i_1)11g(i_2)11 \cdots 11g(i_k)1111$. Then, the encoding of the entire graph will be the concatenation of all the encodings of the vertices. As we are reading through, since we used this encoding of the indices of the vertices, we won't ever be confused about where each of the vertex indices end, or when we are moving on to the next vertex's list.

To go from the list to matrix representation, we can read off all the adjacent vertices, store them, sort them, and then output a row of the adjacency matrix. Since there is some small constant amount of space for the adjacency list representation for each vertex in the graph, the size of the encoding blows up by at most a factor of $O(n)$, which means that the size of the encoding overall is at most squared.

To go in the other direction, it is just a matter of keeping track of the positions in a given row that have ones, encoding those numerical values in the way described, and doing this for each row. Since we are only increasing the size of the encoding by a factor of at most $O(\lg(n))$(which happens in the dense graph case), we have that both of them are polynomially related.\\



\noindent\textbf{Exercise 34.1-5}\\

We show the first half of this exercise by induction on the number of times that we call the polynomial time subroutine. If we only call it zero times, all we are doing is the polynomial amount of extra work, and therefore we have that the whole procedure only takes polynomial time.

Now, suppose we want to show that if we only make $n+1$ calls to the polynomial time subroutine. Consider the execution of the program up until just before the last call. At this point, by the inductive hypothesis, we have only taken a polynomial amount of time. This means that all of the data that we have constructed so far fits in a polynomial amount of space. This means that whatever argument we pass into the last polynomial time subroutine will have size bounded by some polynomial. The time that the last call takes is then the composition of two polynomials, and is therefore a polynomial itself. So, since the time before the last call was polynomial and the time of the last call was polynomial, the total time taken is polynomial in the input. This proves the claim of the first half of the input.

To see that it could take exponential time if we were to allow polynomially many calls to the subroutine, it suffices to provide a single example. In particular, let our polynomial time subroutine be the function that squares its input. Then our algorithm will take an integer $x$ as input and then square it $\lg(x)$ many times. Since the size of the input is $\lg(x)$, this is only linearly many calls to the subroutine. However, the value of the end result will be $x^{2^{\lg(x)}} = x^x = 2^{x\lg(x)} =2^{\lg(x)2^{\lg(x)}} \in \omega(2^{2^{\lg(x)}})$. So, the output of the function will require exponentially many bits to represent, and so the whole program could not of taken polynomial time.\\



\noindent\textbf{Exercise 34.2-1}\\

To verify the language, we should let the certificate be the mapping $f$ from the vertices of $G_1$ to the vertices of $G_2$ that is an isomorphism between the graphs. Then all the verifier needs to do is verify that for all pairs of vertices $u$ and $v$, they are adjacent in $G_1$ if and only if $f(u)$ is adjacent to $f(v)$ in $G_2$. Clearly it is possible to produce an isomorphism if and only if the graphs are isomorphic, as this is how we defined what it means for graphs to be isomorphic.\\



\noindent\textbf{Exercise 34.2-3}\\

Suppose that $G$ is hamiltonian. This means that there is a hamiltonian cycle. Pick any one vertex $v$ in the graph, and consider all the possibilities of deleting all but two of the edges passing through that vertex. For some pair of edges to save, the resulting graph must still be hamiltonian because the hamiltonian cycle that existed originally only used two edges. Since the degree of a vertex is bounded by the number of vertices minus one, we are only less than squaring that number by looking at all pairs ($\binom{n-1}{2} \in O(n^2)$). This means that we are only running the polynomial tester polynomially many independent times, so the runtime is polynomial. Once we have some pair of vertices where deleting all the others coming off of $v$ still results in a hamiltonian graph, we will remember those as special, and ones that we will never again try to delete. We repeat the process with both of the vertices that are now adjacent to $v$, testing hamiltonicity of each way of picking a new vertex to save. We continue in this process until we are left with only $|V|$ edge, and so, we have just constructed a hamiltonian cycle.\\ 



\noindent\textbf{Exercise 34.2-5}\\

Suppose that we know that the length of the certificates to the verifier are bounded by $n^{k-1}$, we know it has to be bounded by some polynomial because the vertifier can only look at polynomially many bits because it runs in polynomial time. Then, we try to run the verifier on every possible assignment to each bit of the certificates of length up to that much. Then, the runtime of this will be a polynomial times $2^{n^{k-1}}$ which is little oh of $2^{n^k}$.\\



\noindent\textbf{Exercise 34.2-7}\\

For a directed acyclic graph, we can compute the topological sort of the graph by the method of section 22.4. Then, looking at this sorting of the vertices, we say that there is a Hamiltonian path if as we read off the vertices, each is adjacent to the next, and they are not if there is any pair of vertices so that one is not adjacent to the next.

If we are in the case that each is adjacent to the next, then the topological sort itself gives us the Hamiltonian path. However, if there is any pair of vertices so that one is not adjacent to the next, this means that that pair of vertices do not have any paths going from one to the other. This would clearly imply that there was no Hamiltonian path, because the Hamiltonian path would be going from one of them to the other.

To see the claim that a pair of vertices $u$ and $v$ that are adjacent in a topological sort but are not adjacent have no paths going from one to the other, suppose to a contradiction there were such a path from $u$ to $v$. If there were any vertices along this path they would have to be after $u$ since they are descendants of $u$ and they would have to be before $v$ because they are ancestors of $v$. This would contradict the fact that we said $u$ and $v$ were adjacent in a topological sort. Then the path would have to be a single edge from $u$ to $v$, but we said that they weren't adjacent, and so, we have that there is no such path.\\



\noindent\textbf{Exercise 34.2-9}\\

A language is in $coNP$ if there is a procedure that can verify that an input is not in the language in polynomial time given some certificate. Suppose that for that language we a procedure that could compute whether an input was in the language in polynomial time receiving no certificate. This is exactly what the case is if we have that the language is in P. Then, we can pick our procedure to verify that an element is not in the set to be running the polynomial time procedure and just looking at the result of that, disregarding the certificate that is given. This then shows that any language that is in $P$ is in $coNP$, giving us the inclusion that we wanted.\\



\noindent\textbf{Exercise 34.2-11}\\

As the hint suggests, we will perform a proof by induction. For the base case, we will have $3$ vertices, and then, by enumeration, we can see that the only hamiltonian graph on three vertices is $K_3$. For any connected graph on three vertices, the longest the path connecting them can be is $2$ edges, and so we will have $G^3 = K_3$, meaning that the graph $G$ was hamiltonian.

Now, suppose that we want to show that the graph $G$ on $n+1$ vertices has the property that $G^3$ is hamiltonian. Since the graph $G$ is connected we know that there is some spanning tree by Chapter 23. Then, let $v$ be any internal vertex of that tree. Suppose that  if we were to remove the vertex $v$, we would be splitting up the original graph in the connected components $V_1, V_2,\ldots, V_k$, sorted in increasing order of size. Suppose that the first $\ell_1$ of these components have a single vertex. Suppose that the first $\ell_2$ of these components have fewer than $3$ vertices. Then, let $v_i$ be the vertex of $V_i$ that is adjacent to $v$ in the tree. For $i>\ell_1$, let $x_i$ be any vertex of $V_i$ that is distance two from the vertex $v$. By induction, we have hamiltonian cycles for each of the components $V_{\ell_2+1}, \ldots, V_k$. In particular, there is a hamiltonian path from $v_i$ to $x_i$. Then, for each $i$ and $j$ there is an edge from $x_j$ to $v_i$, because there is a path of length three between them passing through $v$. This means that we can string together the hamiltonian paths from each of the components with $i>\ell_1$. Lastly, since $V_1, \ldots, V_{\ell}$ all consist of single vertices that are only distance one from $v$, they are all adjacent in $G^3$. So, after stringing together the hamiltonian paths for $i>\ell_1$, we just visit all of the single vertices in $v_1,v_2,\ldots, v_{\ell_1}$ in order, then, go to $v$ and then to the vertex that we started this path at, since it was selected to be adjacent to $v$, this is possible. Since we have constructed a hamiltonian cycle, we have completed the proof.\\



\noindent\textbf{Exercise 34.3-1}\\

The formula in figure 34.8b is 

\[((x_1\vee x_2)\wedge(\neg(\neg x_3)))\wedge(\neg(\neg x_3) \vee ((x_1)\wedge(\neg x_3)\wedge(x_2)))\wedge((x_1)\wedge(\neg x_3)\wedge(x_2))\]

We can cancel out the double negation to get that this is the same expression as

\[((x_1\vee x_2)\wedge(x_3))\wedge(( x_3) \vee ((x_1)\wedge(\neg x_3)\wedge(x_2)))\wedge((x_1)\wedge(\neg x_3)\wedge(x_2))\]

Then, the first clause can only be true if $x_3$ is true. But the last clause can only be true if $\neg x_3$ is true. This would be a contradiction, so we cannot have both the first and last clauses be true, and so the boolean circuit is not satisfiable since we would be taking the and of these two quantities which cannot both be true.\\


\noindent\textbf{Exercise 34.3-3}\\

Suppose first that we had some polynomial time reduction from $L$ to $\bar{L}$. Suppose that $f$ is our polynomial time reduction that says $x\in L$ if and only if $f(x)\in \bar{L}$. Then, if we are given an instance of $c$ and want to check if $c\in \bar{L}$, and we want to find a $g$ so that $c\in \bar{L}$ if and only if $g(c)\in L$, we can define $g$ to be 


\noindent\textbf{Exercise 34.3-5}\\



\noindent\textbf{Exercise 34.3-7}\\

By exercise 33.3-3, we know that for every $L$, we have that there is a polynomial time reduction from $L$ to $\bar{L}$ if and only if there is a polynomial time reduction from $\bar{L}$ to $L$. 


L being complete for $NP$ means that $L\in NP$ and for every $S$ in $NP$, we have that $S\le_P L$. Since $L$ is in $NP$, we have that $\bar{L}\in coNP$ because we could just run our verification algorithm to verify that a given x is not in the complement of $L$, this is the same as verifying that $x$ is in $L$. Suppose that we have some polynomial time reduction $f$ from $S\in NP$ to $L$. Then, we can just take the same function, mapping a string $x\not\in \bar{S}$ into $\bar{L}$. 



\noindent\textbf{Exercise 34.4-1}\\


\end{document}
\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 34}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\Z}{\mathbb{Z}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle

\noindent\textbf{Exercise 34.1-1}\\

Showing that LONGEST-PATH-LENGTH being polynomial implies that LONGEST-PATH is polynomial is trivial, because we can just compute the length of the longest path and reject the instance of LONGEST-PATH if and only if k is larger than the number we computed as the length of the longest path.

Since we know that the number of edges in the longest path length is between $0$ and $|E|$, we can perform a binary search for it's length. That is, we construct an instance of LONGEST-PATH with the given parameters along with $k = \frac{|E|}{2}$. If we hear yes, we know that the length of the longest path is somewhere above the halfway point. If we hear no, we know it is somewhere below. Since each time we are halving the possible range, we have that the procedure can require $O(\lg(|E|))$ many steps. However, running a polynomial time subroutine $\lg(n)$ many times still gets us a polynomial time procedure, since we know that with this procedure we will never be feeding output of one call of LONGEST-PATH into the next.\\



\noindent\textbf{Exercise 34.1-3}\\

A formal encoding of the adjacency matrix representation is to first encode an integer $n$ in the usual binary encoding representing the number of vertices. Then, there will be $n^2$ bits following. The value of bit $m$ will be 1 if there is an edge from vertex $\lfloor m/n \rfloor$ to vertex $(m\%n)$, and zero if there is not such an edge.

An encoding of the adjacency list representation is a bit more finessed. We'll be using a different encoding of integers, call it $g(n)$. In particular, we will place a 0 immediately after every bit in the usual representation. Since this only doubles the length of the encoding, it is still polynomially related. Also, the reason we will be using this encoding is because any sequence of integers encoded in this way cannot contain the string $11$ and must contain at least one zero. Suppose that we have a vertex with edges going to the vertices indexed by $i_1,i_2,i_3,\ldots, i_k$. Then, the encoding corresponding to that vertex is $g(i_1)11g(i_2)11 \cdots 11g(i_k)1111$. Then, the encoding of the entire graph will be the concatenation of all the encodings of the vertices. As we are reading through, since we used this encoding of the indices of the vertices, we won't ever be confused about where each of the vertex indices end, or when we are moving on to the next vertex's list.

To go from the list to matrix representation, we can read off all the adjacent vertices, store them, sort them, and then output a row of the adjacency matrix. Since there is some small constant amount of space for the adjacency list representation for each vertex in the graph, the size of the encoding blows up by at most a factor of $O(n)$, which means that the size of the encoding overall is at most squared.

To go in the other direction, it is just a matter of keeping track of the positions in a given row that have ones, encoding those numerical values in the way described, and doing this for each row. Since we are only increasing the size of the encoding by a factor of at most $O(\lg(n))$(which happens in the dense graph case), we have that both of them are polynomially related.\\



\noindent\textbf{Exercise 34.1-5}\\

We show the first half of this exercise by induction on the number of times that we call the polynomial time subroutine. If we only call it zero times, all we are doing is the polynomial amount of extra work, and therefore we have that the whole procedure only takes polynomial time.

Now, suppose we want to show that if we only make $n+1$ calls to the polynomial time subroutine. Consider the execution of the program up until just before the last call. At this point, by the inductive hypothesis, we have only taken a polynomial amount of time. This means that all of the data that we have constructed so far fits in a polynomial amount of space. This means that whatever argument we pass into the last polynomial time subroutine will have size bounded by some polynomial. The time that the last call takes is then the composition of two polynomials, and is therefore a polynomial itself. So, since the time before the last call was polynomial and the time of the last call was polynomial, the total time taken is polynomial in the input. This proves the claim of the first half of the input.

To see that it could take exponential time if we were to allow polynomially many calls to the subroutine, it suffices to provide a single example. In particular, let our polynomial time subroutine be the function that squares its input. Then our algorithm will take an integer $x$ as input and then square it $\lg(x)$ many times. Since the size of the input is $\lg(x)$, this is only linearly many calls to the subroutine. However, the value of the end result will be $x^{2^{\lg(x)}} = x^x = 2^{x\lg(x)} =2^{\lg(x)2^{\lg(x)}} \in \omega(2^{2^{\lg(x)}})$. So, the output of the function will require exponentially many bits to represent, and so the whole program could not of taken polynomial time.\\



\noindent\textbf{Exercise 34.2-1}\\

To verify the language, we should let the certificate be the mapping $f$ from the vertices of $G_1$ to the vertices of $G_2$ that is an isomorphism between the graphs. Then all the verifier needs to do is verify that for all pairs of vertices $u$ and $v$, they are adjacent in $G_1$ if and only if $f(u)$ is adjacent to $f(v)$ in $G_2$. Clearly it is possible to produce an isomorphism if and only if the graphs are isomorphic, as this is how we defined what it means for graphs to be isomorphic.\\



\noindent\textbf{Exercise 34.2-3}\\

Suppose that $G$ is hamiltonian. This means that there is a hamiltonian cycle. Pick any one vertex $v$ in the graph, and consider all the possibilities of deleting all but two of the edges passing through that vertex. For some pair of edges to save, the resulting graph must still be hamiltonian because the hamiltonian cycle that existed originally only used two edges. Since the degree of a vertex is bounded by the number of vertices minus one, we are only less than squaring that number by looking at all pairs ($\binom{n-1}{2} \in O(n^2)$). This means that we are only running the polynomial tester polynomially many independent times, so the runtime is polynomial. Once we have some pair of vertices where deleting all the others coming off of $v$ still results in a hamiltonian graph, we will remember those as special, and ones that we will never again try to delete. We repeat the process with both of the vertices that are now adjacent to $v$, testing hamiltonicity of each way of picking a new vertex to save. We continue in this process until we are left with only $|V|$ edge, and so, we have just constructed a hamiltonian cycle.\\ 



\noindent\textbf{Exercise 34.2-5}\\

Suppose that we know that the length of the certificates to the verifier are bounded by $n^{k-1}$, we know it has to be bounded by some polynomial because the vertifier can only look at polynomially many bits because it runs in polynomial time. Then, we try to run the verifier on every possible assignment to each bit of the certificates of length up to that much. Then, the runtime of this will be a polynomial times $2^{n^{k-1}}$ which is little oh of $2^{n^k}$.\\



\noindent\textbf{Exercise 34.2-7}\\

For a directed acyclic graph, we can compute the topological sort of the graph by the method of section 22.4. Then, looking at this sorting of the vertices, we say that there is a Hamiltonian path if as we read off the vertices, each is adjacent to the next, and they are not if there is any pair of vertices so that one is not adjacent to the next.

If we are in the case that each is adjacent to the next, then the topological sort itself gives us the Hamiltonian path. However, if there is any pair of vertices so that one is not adjacent to the next, this means that that pair of vertices do not have any paths going from one to the other. This would clearly imply that there was no Hamiltonian path, because the Hamiltonian path would be going from one of them to the other.

To see the claim that a pair of vertices $u$ and $v$ that are adjacent in a topological sort but are not adjacent have no paths going from one to the other, suppose to a contradiction there were such a path from $u$ to $v$. If there were any vertices along this path they would have to be after $u$ since they are descendants of $u$ and they would have to be before $v$ because they are ancestors of $v$. This would contradict the fact that we said $u$ and $v$ were adjacent in a topological sort. Then the path would have to be a single edge from $u$ to $v$, but we said that they weren't adjacent, and so, we have that there is no such path.\\



\noindent\textbf{Exercise 34.2-9}\\

A language is in $coNP$ if there is a procedure that can verify that an input is not in the language in polynomial time given some certificate. Suppose that for that language we a procedure that could compute whether an input was in the language in polynomial time receiving no certificate. This is exactly what the case is if we have that the language is in P. Then, we can pick our procedure to verify that an element is not in the set to be running the polynomial time procedure and just looking at the result of that, disregarding the certificate that is given. This then shows that any language that is in $P$ is in $coNP$, giving us the inclusion that we wanted.\\



\noindent\textbf{Exercise 34.2-11}\\

As the hint suggests, we will perform a proof by induction. For the base case, we will have $3$ vertices, and then, by enumeration, we can see that the only hamiltonian graph on three vertices is $K_3$. For any connected graph on three vertices, the longest the path connecting them can be is $2$ edges, and so we will have $G^3 = K_3$, meaning that the graph $G$ was hamiltonian.

Now, suppose that we want to show that the graph $G$ on $n+1$ vertices has the property that $G^3$ is hamiltonian. Since the graph $G$ is connected we know that there is some spanning tree by Chapter 23. Then, let $v$ be any internal vertex of that tree. Suppose that  if we were to remove the vertex $v$, we would be splitting up the original graph in the connected components $V_1, V_2,\ldots, V_k$, sorted in increasing order of size. Suppose that the first $\ell_1$ of these components have a single vertex. Suppose that the first $\ell_2$ of these components have fewer than $3$ vertices. Then, let $v_i$ be the vertex of $V_i$ that is adjacent to $v$ in the tree. For $i>\ell_1$, let $x_i$ be any vertex of $V_i$ that is distance two from the vertex $v$. By induction, we have hamiltonian cycles for each of the components $V_{\ell_2+1}, \ldots, V_k$. In particular, there is a hamiltonian path from $v_i$ to $x_i$. Then, for each $i$ and $j$ there is an edge from $x_j$ to $v_i$, because there is a path of length three between them passing through $v$. This means that we can string together the hamiltonian paths from each of the components with $i>\ell_1$. Lastly, since $V_1, \ldots, V_{\ell}$ all consist of single vertices that are only distance one from $v$, they are all adjacent in $G^3$. So, after stringing together the hamiltonian paths for $i>\ell_1$, we just visit all of the single vertices in $v_1,v_2,\ldots, v_{\ell_1}$ in order, then, go to $v$ and then to the vertex that we started this path at, since it was selected to be adjacent to $v$, this is possible. Since we have constructed a hamiltonian cycle, we have completed the proof.\\



\noindent\textbf{Exercise 34.3-1}\\

The formula in figure 34.8b is 

\[((x_1\vee x_2)\wedge(\neg(\neg x_3)))\wedge(\neg(\neg x_3) \vee ((x_1)\wedge(\neg x_3)\wedge(x_2)))\wedge((x_1)\wedge(\neg x_3)\wedge(x_2))\]

We can cancel out the double negation to get that this is the same expression as

\[((x_1\vee x_2)\wedge(x_3))\wedge(( x_3) \vee ((x_1)\wedge(\neg x_3)\wedge(x_2)))\wedge((x_1)\wedge(\neg x_3)\wedge(x_2))\]

Then, the first clause can only be true if $x_3$ is true. But the last clause can only be true if $\neg x_3$ is true. This would be a contradiction, so we cannot have both the first and last clauses be true, and so the boolean circuit is not satisfiable since we would be taking the and of these two quantities which cannot both be true.\\



\noindent\textbf{Exercise 34.3-3}\\

Suppose first that we had some polynomial time reduction from $L$ to $\bar{L}$. This means that for every $x$ there is some $f(x)$ so that $x\in L$ iff $f(x)\in \bar{L}$. This means that $x\in \bar{L}$ iff $x\not\in L$ iff $f(x)\not\in\bar{L}$ iff $f(x)\in L$. So, our polytime computable function for the reduction is the same one that we had from $L\le_P \bar{L}$. We can do an identical thing for the other direction.\\



\noindent\textbf{Exercise 34.3-5}\\
We do not have any loss of generality by this assumption. This is because since we bounded the amount of time that the program has to run to be polynomial, there is no way that the program can access more than an polynomial amount of space. That is, there is no way of moving the head of the turning machine further than polynomially far in only polynomial time because it can move only a single cell at a time.\\


\noindent\textbf{Exercise 34.3-7}\\

Since $L$ is in $NP$, we have that $\bar{L}\in coNP$ because we could just run our verification algorithm to verify that a given x is not in the complement of $L$, this is the same as verifying that $x$ is in $L$. Since every coNP language has its complement in NP, suppose that we let $S$ be any language in $coNP$ and let $\bar{S}$ be its compliment. Suppose that we have some polynomial time reduction $f$ from $\bar{S}\in NP$ to $L$. Then, consider using the same reduction function. We will have that $x\in S$ iff $x\not\in \bar{S}$ iff $f(x)\not\in L$ iff $f(x)\in \bar{L}$. This shows that this choice of reduction function does work. So, we have shown that the compliment of any NP complete problem is also NP complete. To see the other direction, we just negate everything, and the proof goes through identically.\\



\noindent\textbf{Exercise 34.4-1}\\

Suppose that it is a circuit on two inputs, and then, we have n rounds of two and gates each, both of which take both of the two wires from the two gates from the previous round. Since the formulas for each round will consist of two copies of the formulas from the previous round, it will have an exponential size formula.\\



\noindent\textbf{Exercise 34.4-3}\\

The formula could have $\Omega(n)$ free variables, then, the truth table corresponding to this formula would a number of rows that is $\Omega(2^n)$ since it needs to consider every possible assignment to all the variables. This then means that the reduction as described is going to incrase the size of the problem exponentially.\\



\noindent\textbf{Exercise 34.4-5}\\

Since the problem is in disjunctive normal form, we can write it as $\vee_i \phi_i$ where each $\phi_i$ looks like the and of a bunch of variables and their negations. Then, we know that the formula is satisfiable if and only if any one if the $\phi_i$ are satisfiable. If a $\phi_i$ contains both a variable and its negation, then it is clearly not satisfiable, as one of the two must be false. However, if each variable showing up doesn't have its negation showing up, then we can just pick the appropriate value to assign to each variable. This is a property that can be checked in linear time, by just keeping two bit vectors of length equal to the number of variables, one representing if the variable has shown up negated and one for if the variable has shown up without having been negated.\\



\noindent\textbf{Exercise 34.4-7}\\
Suppose that the original formula was $\wedge_i (x_i \vee y_i)$, and the set of variables were $\{a_i\}$. Then, consider the directed graph which has a vertex corresponding both to each variable, and each negation of a variable. Then, for each of the clauses $x \vee y$, we will place an edge going from $\neg x$ to $y$, and an edge from $\neg y$ to $x$. Then, anytime that there is an edge in the directed graph, that means if the vertex the edge is coming from is true, the vertex the edge is going to has to be true. Then, what we would need to see in order to say that the formula is satisfiable is a path from a vertex to the negation of that vertex, or vice versa. The naive way of doing this would be to run all pairs shortest path, and see if there is a path from a vertex to its negation. This however takes time $O(n^2\lg(n))$, and we were charged with making the algorithm as efficient as possible. First, run the procedure for detecting strongly connected components, which takes linear time. For every pair of variable and negation, make sure that they are not in the same strongly connected component.%notdone



\noindent\textbf{Exercise 34.5-1}\\

To do this, first, notice that it is in NP, where the certificate is just the injection from $G_1$ into $G_2$ so that $G_1$ is isomorphic to its image.

Now, to see that it is NP complete, we will do a reduction to clique. That is, to detect if a graph has a clique of size $k$, just let $G_1$ be a complete graph on $k$ vertices and let $G_2$ be the original graph. If we could solve the subgraph isomorphism problem quickly, this would allow us to solve the clique problem quickly.\\



\noindent\textbf{Exercise 34.5-3}\\

We will try to show a reduction to the 0-1 integer programming problem. To see this, we will take our $A$ from the 0-1 integer programming problem, and tack on a copy of the $n\times n$ identity matrix to its bottom, and tack on $n$ ones to the end of $b$ from teh 0-1 integer programming problem. This has the effect of adding the restrictions that every entry of $x$ must be at most $1$. However, since, for every $i$, we needed $x_i$ to be an integer anyways, this only leaves the option that $x_i=0$ or $x_i=1$. This means that by adding these restrictions, we have that any solution to this system will be a solution to the 0-1 integer programming problem given by $A$ and $b$.\\




\noindent\textbf{Exercise 34.5-5}\\

We will be performing a reduction from the subset sum problem. Suppose that $S$ and $t$ are our set and target from our subset sum problem. Let $x$ be equal to $ \sum_{s\in S} s$. Then, we will add the elements $x+t, 2x-t$. Once we have added the elements, note that the sum of all of the elements in the new set $S'$ will be $4x$. We also know that we cannot have both of the new elements that we added be on same side of the partition, because they add up to $3x$ which is three times all the other elements combined. Now, this set of elements will be what we pass into our set partition solver. Note that since the total is $4x$, each side will add up to $2x$. This means that if we look at the elements that on the same side as, but not equal to $2x-t$, they must add up to $t$. Since they were also members of the original set $S$, this means that they are a subset with the desired sum, solving the original instance of subset sum. Since it was proved in the section that subset sum is NP-complete, this proves that the set-partition problem is NP hard. 

To see that it is in NP, just let the certificate be the set of elements of $S$ that forms one side of the partition. It is linear time to add them up and make sure that they are exactly half the sum of all the elements in $S$.\\



\noindent\textbf{Exercise 34.5-7}\\

The related decision problem is to, given a graph $G$ and integer $k$ decide if there is a simple cycle of length at least $k$ in the graph $G$. To see that this problem is in $NP$, just let the certificate be the cycle itself. It is really easy just to walk along this cycle, keeping track of what vertices you've already seen, and making sure they don't get repeated.

To see that it is NP-hard, we will be doing a reduction to Hamilton cycle. Suppose we have a graph $G$ and want to know if it is Hamilton. We then create an instance of the decision problem asking if the graph has a simply cycle of length at least $|V|$ vertices. If it does then there is a hamilton cycle. If there is not, then there cannot be any hamilton cycle.\\



\noindent\textbf{Problem 34-1}\\
\begin{enumerate}[a)]
\item The related decision problem should be to, given a graph and a number $k$ decide whether or not there is some independent set of size at least $k$. If we take the compliment of the given graph, then it will have a clique of size at least $k$ if and only if the original graph has an independent set of size at least $k$. This is because if we take any set of vertices in the original graph, then it will be an independent set if and only if there are no edges between those vertices. However, in the compliment graph, this means that between every one of those vertices, there is an edge, which means they form a clique. So, to decide independent set, just decide clique in the compliment.
\item
We know that since all independent sets are subsets of the set of vertices, then the size of the largest independent set will be an integer in the range $1..|V|$. Then, we will perform a binary search on this space of valid sizes of the largest independent set. That is, we pick the middle element, ask if there is an independent set of that size, if there is, we know we are in the upper half of this range of values for the size of the largest independent set, if not, then we are in the lower half. The total runtime of this procedure to find the size of the largest independent set will only be a factor of $\lg(|V||)$ higher than the solution to the decision problem. Call the size of the largest independent set $k$.

Now, for every pair of vertices, try adding an edge, and check if the procedure from before determines that the size of the largest independent set has decreased. If it hasn't that means that that pair of vertices doesn't prevent us from attaining an independent set of the given size. That is, we aren't in the case that there is only one maximal set of the given size and that pair of vertices belongs to it. So, add that edge to the graph, and continue in this fashion for every pair of vertices. Once we are done, the size of the largest independent set will be the same, and we will have that every edge is filled in except for those going between an independent set of the given size. So, we just list off all the vertices whose degree is less than $|V|-1$ as being members of our independent set.
\item
Since every vertex has degree 2, and so self edges are allowed, the graph must look like a number of disjoint cycles. We can then consider the independent set problem separately for each of the cycles. If we have an even cycle, the largest independent set possible is half the vertices, by selecting them to be alternating. If it is an odd cycle, then we can do half rounded down, since when we get back to the start, we are in the awkward place where there are two unselected vertices between two selected vertices. It's easy to see that these are tight, because there is so little freedom in selecting an independent set in a cycle. So, to calculate the size of the smallest independent set, look at the sizes of each cycle $c_i$, then, the largest independent set will have size $\lfloor \frac{c_i}{2}\rfloor$.
\item
First we add some edges in the following way
\end{enumerate}

\noindent\textbf{Problem 34-3}\\
\begin{enumerate}[a)]
\item To two color a graph, we will do it a connected component at a time, so, suppose that the graph is a single component. Pick a vertex and color arbitrarily, and color that vertex that color. Then, we repeatedly find a vertex that has a colored neighbor and color it the other color. If we are ever in the case that a vertex has neighbors of both colors, then the graph is not 2-colorable. This procedure is able to 2-color if the graph is 2-colorable, since the only point where our hand isn't forced is at the beginning when we pick a vertex and a color, but this choice is only a false one because of the symmetry of the two colors. If it finds it is 2-colorable, it also outputs a valid 2-coloring.
\item
The equivalent decision problem is to, given a graph $G$ and an integer $k$ say if there is a coloring that uses at most $k$ colors. The easy direction is showing that if the original problem is solvable in poly thime, then the decision problem is solvable in poly time. To do this, just compute the minimum number of colors needed and output true if this is $\le k$. 

The other direction is a bit harder. Suppose that we can solve the decision problem in polynomial time, then, we will try to show how we can actually compute the minimum number of colors needed. A trivial bound on the number of colors needed is the number of vertices, because if each vertex has it's own color, then the coloring has to be valid. So, we perform a binary search on the number of colors, starting with the range $1.. |V|$, halving it each time until we are down to a single possible number of colors needed in order to color the graph. This will only add a log factor to the runtime of the decision problem, and so will run in polynomial time.
\item
For this problem, we need to show that if we can solve the decision problem quickly, then we can decide the language 3-COLOR quickly. This is just a matter of running the decision procedure with the same graph and with $k=3$. This gets us the reduction we need to show that 3-COLOR being NP-complete implies the decision problem is NP-hard. The decision problem is in $NP$ because we can just have the certificate explicitly be the coloring of the vertices of the graph.
\item
When we restrict the graph to the vertices $x_i, \neg x_i, RED$, then we will obtain a $K_3$ because of the literal edges. This means that all three colors must show up in it. Since there is already $c(RED)$, then the other two must be $c(TRUE)$ and $c(FALSE)$. No matter whether we choose $x_i$ or $\neg x_i$ to be $c(TRUE)$, we can just select the other one to be $c(FALSE)$, this gets us that, if we only care about the literal edges, we always have a 3 coloring regardless of whether we want each $x_i$ to be true or false.
\item
For convenience, we will call the vertices $a,b,c,d,e$ from the figure, where we are reading from top to bottom and left to right for vertices that are horizontal from one another. Since we are trying to check that it is 3 colorable if and only if at least one of $x,y,z$ are $c(TRUE)$, we can negate the only if direction. That is, we suppose they are all colored $c(FALSE)$ and show that the graph is not 3 colorable.

Suppose $c(x) = c(y) = c(z) = c(FALSE)$. Then, we have that the only possibility for vertex $e$ is to be $c(RED)$. This means the only possibility for $c$ is to be $c(FALSE)$. However, this means that $c(a)\neq c(x) = c(FALSE)$, $c(d)\neq c(y) = c(FALSE)$, and $c(b)\neq c(c) = c(FALSE)$. So, we have a contradiction because any $K_3$ must have one of each color, and none of the vertices in this $K_3$ can be $c(FALSE)$. This shows that the graph is not 3-colorable.

For the other direction, we do not negate. So, we assume there is a vertex colored $c(TRUE)$ and we show that the graph is 3-colorable. We will split into the following cases. Note that because $x$ and $y$ play a symmetric role, we can reduce the number of cases from $7$ to $5$

\[
\begin{array}{|c|c|c|c|c|c|c|c|}
\hline
x&y&z&a&b&c&d&e\\
\hline
\hline
c(TRUE)&c(TRUE)&c(TRUE)&c(FALSE)&c(TRUE)&c(FALSE)&c(RED)&c(RED)\\
\hline
c(FALSE)&c(TRUE)&c(TRUE)&c(RED)&c(TRUE)&c(FALSE)&c(FALSE)&c(RED)\\
\hline
c(FALSE)&c(FALSE)&c(TRUE)&c(RED)&c(FALSE)&c(RED)&c(TRUE)&c(FALSE)\\
\hline
c(TRUE)&c(TRUE)&c(FALSE)&c(FALSE)&c(TRUE)&c(FALSE)&c(RED)&c(RED)\\
\hline
c(FALSE)&c(TRUE)&c(FALSE)&c(TRUE)&c(RED)&c(FALSE)&c(FALSE)&c(RED)\\
\hline
\end{array}
\]

Then, in every case where at least one of the inputs is true, there is an assignment of colors to the other vertices that produces a valid 3-coloring.
\item
Suppose we are given any instance of 3-CNF-SAT, we will be using exactly the same construction as described in the problem for the reduction. First, we note that each of the vertices corresponding to a variable and its negation must only have the colors $c(TRUE)$ and $c(FALSE)$, and exactly one can be $c(TRUE)$ because of the literal edges. This means that each of the clause vertices will only be colorable if we assign a color of true to one of the variable vertices that are in the clause. This means that if we set each variable that has $c(x_i)=c(TRUE)$ true and each variable that has $c(\neg x_i) = c(TRUE)$ to be false, we will of obtained an assignment that makes at least one of the entries in each clause true, and so, is a satisfying assignment of the formula. Since 3-CNF-SAT is NP-complete, this means that 3-COLOR is NP-hard.

To see that it is in NP, just let the certificate be the coloring. Checking the coloring can be done in linear time.
\end{enumerate}


\end{document}
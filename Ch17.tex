\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 17}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}

\begin{document}
\maketitle


\noindent\textbf{Exercise 17.1-1}\\
It woudn't because we could make an arbitrary sequence of $MULTIPUSH(k), MULTIPOP(k)$. The cost of each will be $\Theta(k)$, so the average runtime of each will be $\Theta(k)$ not $O(1)$.\\

\noindent\textbf{Exercise 17.1-3}\\
Note that this setup is similar to the dynamic tables discussed in section 17.4. Let $n$ be arbitrary, and have the cost of operation $i$ be $c(i)$. Then,
\[
\sum_{i=1}^n c(i) = \sum_{i=1}^{\lceil\lg(n)\rceil} 2^i + \sum_{i\le n \mbox{ not a power of 2}} 1 \le \sum_{i=1}^{\lceil\lg(n)\rceil} 2^i + n = 2^{1 + \lceil\lg(n)\rceil} -1 + n \le 4n -1 +n \le 5n \in O(n)
\]
So, since to find the average, we divide by $n$, the average runtime of each command is $O(1)$.\\

\noindent\textbf{Exercise 17.2-1}\\
To every stack operation, we charge twice. First we charge the actual cost of the stack operation. Second we charge the coase of copying an element later on. Since we have the size of the stack never excede $k$, and there are always $k$ operations between backups, we always overpay by at least enough. So, the ammortized cost of the operation is constant. So, the cost of the n operation is $O(n)$.\\

\noindent\textbf{Exercise 17.2-3}\\
For each time we set a bit to 1, we both pay a dollar for eventually setting it back to zero (in the usual manner as the counter is incremented). But we also pay a third dollar in the event that even after the position has been set back to zero, we check about zeroing it out during a reset operation. We also increment the position of the highest order bit (as needed). Then, while doing the reset operation, we will only need consider those positions less signifigant than the highest order bit. Because of this, we have at least paid one extra dollar before, because we had set the bit at that position to one at least once for the highest order bit to be where it is. Since we have only put down a constant ammortized cost at each setting of a bit to 1, the ammortized cost is constant because each increment operation involves setting only a single bit to 1. Also, the ammortized cost of a reset is zero because it involves setting no bits to one. It's true cost has already been paid for.\\

\noindent\textbf{Exercise 17.3-1}\\
Define $\Phi'(D) = \Phi(D) -\Phi(D_0)$. Then, we have that $\Phi(D) \ge \Phi(D_0)$ implies $\Phi'(D)  = \Phi(D) - \Phi(D_0) \ge \Phi(D_0) -\Phi(D_0) = 0$. and $\Phi'(D_0) = \phi(D_0)-\Phi(D_0) = 0$. Lastly, the ammortized cost using $\Phi'$ is $c_i + \Phi'(D_i) -\Phi'(D_{i-1}) = c_i + (\Phi(D_i) - \Phi(D_0)) - (\Phi(D_i) -\Phi(D_{i-1})) = c_i + \Phi(D_i) -\Phi(D_{i-1})$ which is the ammortized cost using $\Phi$.\\

\noindent\textbf{Exercise 17.3-3}\\
Make the potential function be equal to $n\lg(n)$ where $n$ is the size of the min-heap. Then, there is still a cost of $O(\lg(n))$ to insert, since only an amount of ammortization that is about $\lg(n)$ was spent to increase the size of the heap by 1. However, since extract min decreases the size of the heap by 1, the actual cost of the operation is offset by a change in potential of the same order, so only a constant amount of work is needed.\\
 


\noindent\textbf{Exercise 17.3-5}\\
Suppose that we have that $n\ge c b$. Since the counter begins with $b$ 1's, we'll make all of our ammortized cost $2+ \frac{1}{c}$. Then the additional cost of $\frac{1}{c}$ over the course of $n$ operations amounts to paying an extra$\frac{n}{c} \ge b$ which was how much we were behind by when we started. Since the ammortized cost of each operation is $2+\frac{1}{c}$ it is in $O(1)$ so the total cost is in $O(n)$.\\

\noindent\textbf{Exercise 17.3-7}\\
We'll store all our elements in an array, and if ever it is too large, we will copy all the elements out into an array of twice the length. To delete the larger half, we first find the element $m$ with order statistic $\lceil|S|/2\rceil$ by the algorithm presented in section 9.3. Then, scan through the array and copy out the elements that are smaller or equal to $m$ into an aray of half the size. Since the delete half operation takes time $O(|S|)$ and reduces the number of elemnts by $\lfloor|S|/2\rfloor \in \Omega(|S|)$, we can make these operations take ammortized constant time by selecting our potential function to be linear in $|S|$. Since the insert operation only increases $|S|$ by one, we have that there is only a constant amount of work going towards satisfying the potential, so the total ammortized cost of an insertion is still constant. To output all the elements just iterate through the array and output each.\\

\noindent\textbf{Exercise 17.4-1}\\
By theorems 11.6-11.8, the expected cost of performing insertions and searches in an open address hash table apporaches infinity as the load factor approaches one, for any load factor fixed away from 1, the expected time is bounded by a constant though. The expected value of the actual cost my not be O(1) for every insertion because the actual cost may include copying out the current values from the current table into a larger table because it became too full. This would take time that is linear in the number of elements stored.\\

\noindent\textbf{Exercise 17.4-3}\\
If a resizing is not triggered, we have

\begin{align*}
\hat{c}_i & = C_i + \Phi_i-\Phi_{i-1}\\
&= 1 + |2\cdot num_i - size_i| - |2\cdot num_{i-1} - size_{i-1}|\\
&= 1 + |2\cdot num_i - size_i| - |2\cdot num_i + 2  - size_{i}|\\
&\le 1 + |2\cdot num_i - size_i| - |2\cdot num_i  - size_{i}| + 2\\
&= 3
\end{align*}

However, if a resizing is triggered, suppose that $\alpha_{i-1} <\frac{1}{2}$. Then the actual cost is $num_i+1$ since we do a deletion and move all the rest of the items. Also, since we resize when the load factor drops below $\frac{1}{3}$, we have that $size_{i-1}/3 = num_{i-1} = num_i +1$.

\begin{align*}
\hat{c}_i &= c_i + \phi_i - \Phi_{i-1}\\
&= num_i + 1 + |2\cdot num_i -size_i| - |2\cdot num_{i-1} - size_{i-1}|\\
&\le num_i + 1 + \left(\frac{2}{3}size_{i-1}- 2\cdot num_i\right) - (size_{i-1} - 2\cdot num_i -2 ) \\
&=num_i + 1 + (2\cdot num_i +2 - 2\cdot num_i) - (3\cdot num_i +3 -2\cdot num_i)\\
&= 2
\end{align*}

The last case, that we had the load factor was greater than or equal to $\frac{1}{2}$, will not trigger a resizing because we only resize when the load drops below $\frac{1}{3}$.\\

\noindent\textbf{Problem 17-1}\\
\begin{enumerate}[a.]
\item
Initialize a second array of length $n$ to all trues, then, going through the indices of the original array in any order, if  the corresponding entry in the second array is true, then swap the element at the current index with the element at the bit-reversed position, and set the entry in the second array corresponding to the bit-reversed index equal to false. Since we are running $rev_k$ $< n$ times, the total runtime is $O(nk)$.
\item
Doing a bit reversed increment is the same thing as adding a one to the leftmost position where all carries are going to the left instead of the right. See the algorithm BIT-REVERSED-INCREMENT(a)

\begin{algorithm}
\caption{BIT-REVERSED-INCREMENT(a)}
\begin{algorithmic}
\State{let m be a 1 followed by k-1 zeroes}
\While{m bitwise-AND a is not zero}
\State{a = a bitwise-XOR m}
\State{shift m right by 1}
\EndWhile
\Return m bitwise-OR a
\end{algorithmic}
\end{algorithm}

By a similar analysis to the binary counter (just look at the problem in a mirror), this BIT-REVERSED-INCREMENT will take constant ammortized time. So, to perform the bit-reversed permutation, have a normal binary counter and a bit reversed counter, then, swap the values of the two counters and increment. Do not swap however if those pairs of lements have already been swapped, which can be kept track of in a auxillary array.

\item The BIT-REVERSED-INCREMENT procedure given in the previous part only uses single shifts to the right, not arbitrary shifts.

\end{enumerate}

\noindent\textbf{Problem 17-3}\\
\begin{enumerate}[a.]%notdone
\item

\item
We will show by induction that any tree with $\le \alpha^{-d}+d$ elements has a depth of at most $d$. This is clearly true for $d=0$ because any tree with a single node has depth 0, and since $\alpha^0 = 1$, we have that our restriction on the number of elements requires there to only be one. Now, suppose that in some inductive step we had a contradiction, that is, some tree of depth $d$ that is $\alpha$ balanced but has more than $\alpha^{-d}$ elements. We know that both of the subtrees are alpha balanced, and by being alpha balanced at the root, we have $root.left.size \le \alpha \cdot root.size$ which implies $root.right.size > root.size - \alpha \cdot root.size -1$. So, $root.right.size > (1-\alpha) root.size -1 > (1-\alpha) \alpha^{-d} + d-1 = (\alpha^{-1} -1)\alpha^{-d+1} + d-1 \ge \alpha^{-d+1}+d-1$ which is a contradiction to the fact that it held for all smaller values of $d$ because any child of a tree of depth $d$ has depth $d-1$.  
 \item
The potential function is a sum of $\Delta(x)$ each of which is the absolute value of a quantity, so, since it is a sum of nonnegative values, it is nonnegative regardless of the input BST.

If we suppose that our tree is $1/2-$ balanced, then, for every node $x$, we'll have that $\Delta(x) \le 1$, so, the sum we compute to find the potential will be over no terms.
 
 \item
 
 
 \item
 
\end{enumerate}

\noindent\textbf{Problem 17-5}\\
\begin{enumerate}[a.]
\item 
Since the heuristic is picked in advance, given any sequence of requests given so far, we can simulate what ordering the heristic will call for, then, we will pick our next request to be whatever element will of been in the last position of the list. Continuing until all the requests have been made, we have that the cost of this sequence of accesses is $=mn$.

\item
The cost of finding an element is $=rank_L(x)$ and since it needs to be swapped with all the elements before it, of which there are $rank_L(x)-1$, the total cost is $2\cdot rank_L(x)-1$.

\item
Regardless of the heuristic used, we first need to locate the element, which is left whereever it was after the previous step, so, needs $rank_{L_{i-1}}(x)$. After that, by definition, there are $t_i$ transpositions made, so, $c_i^* = rank_{L_{i-1}}(x) + t_i^*$.

\item
If we perform a transposition of elements $y$ and $z$, where $y$ is towards the left. Then there are two cases. The first is that the final ordering of the list in $L_i^*$ is with $y$ in front of $z$, in which case we have just increased the number of inversions by 1, so the poitential increases by 2. The second is that in $L_I^*$ z occurs before $y$, in which case, we have just reduced the number of inversions by one, reducing the potential by 2. In both cases, whether or not there is an inversion between $y$ or $z$ and any other element has not changed, since the transposition only changed the relative ordering of those two elements.

\item
By definition, $A$ and $B$ are the only two of the four categories to place elements that precede $x$ in $L_{i-1}$, since there are $|A|+|B|$ elements preceding it, it's rank in $L_{i-1}$ is $|A|+|B|+1$. Similarly, the two categories in which an element can be if it precedes $x$ in $L_{i-1}^*$ are $A$ and $C$, so, in $L_{i-1}^*$, $x$ has rank $|A|+|C|+1$.

\item
We have from part d that the potential increases by 2 if we transpose two elements that are being swapped so that their relative order in the final ordering is being screwed up, and decreases by two if they are begin placed into their correct order in $L_i^*$. In particular, they increase it by at most 2. since we are keeping track of the number of inversions that may not be the direct effect of the transpositions that heuristic $H$ made, we see which ones the Move to front heuristic may of added. In particular, since the move to front heuristic only changed the relative order of $x$ with respect to the other elements, moving it in front of the elements that preceeded it in $L_{i-1}$, we only care about sets $A$ and $B$. For an element in $A$, moving it to be behind $A$ created an inversion, since that element preceeded $x$ in $L_i^*$. However, if the element were in $B$, we are removing an inversion by placing $x$ in front of it.

\item
First, we apply parts b and f to the expression for $\hat{c}_i$ to get $\hat{c}_i \le 2\cdot rank_{L}(x) -1 + 2(|A|-|B|+t_i^*)$. Then, applying part e, we get this is $= 2(|A|+|B|+1) - 1+ 2(|A|-|B|+t_i^*) = 4|A| -1+2t_i^*\le 4(|A|+|C|+1) +4t_i^* = 4(rank_{L_{i-1}^*}(x) + t_i^*)$. Finally, by part c, this bound is equal to $4c_i^*$.
\item
We showed that the ammortized cost of each operation under the move to front heuristic was at most four times the cost of the operation using any other heuristic. Since the ammortized cost added up over all these operation is at most the total (real) cost, so we have that the total cost with movetofront is at most four times the total cost with an aribitrary other heuristic.

\end{enumerate}



\end{document}
\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fancybox}
\usepackage{tikz}


	
\pagestyle{fancy}
\title{Chapter 7}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}

\begin{document}
\maketitle

\noindent\textbf{Exercise 7.1-1}\\
$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
13&19&9&5&12&8&7&4&21&2&6&11\\
\hline
13&19&9&5&12&8&7&4&21&2&6&11\\
\hline
13&19&9&5&12&8&7&4&21&2&6&11\\
\hline
9&19&13&5&12&8&7&4&21&2&6&11\\
\hline
9&5&13&19&12&8&7&4&21&2&6&11\\
\hline
9&5&13&19&12&8&7&4&21&2&6&11\\
\hline
9&5&8&19&12&13&7&4&21&2&6&11\\
\hline
9&5&8&7&12&13&19&4&21&2&6&11\\
\hline
9&5&8&7&4&13&19&12&21&2&6&11\\
\hline
9&5&8&7&4&13&19&12&21&2&6&11\\
\hline
9&5&8&7&4&2&19&12&21&13&6&11\\
\hline
9&5&8&7&4&2&6&12&21&13&19&11\\
\hline
9&5&8&7&4&2&6&11&21&13&19&12\\
\hline
\end{array}
$
\\


\noindent\textbf{Exercise 7.1-2}\\

If all elements in the array have the same value, PARTITION returns $r$.  To make PARTITION return $q = \lfloor(p+r)/2\rfloor$ when all elements have the same value, modify line 4 of the algorithm to say this: if $A[j] \leq x$ and $j (mod 2) = (p+1) (mod 2)$.  This causes the algorithm to treat half of the instances of the same value to count as less than, and the other half to count as greater than. \\

\noindent\textbf{Exercise 7.1-3}\\

The for loop makes exactly $r-p$ iterations, each of which takes at most constant time. The part outside the for loop takes at most constant time. Since $r-p$ is the size of the subarray, PARTITION takes at most time proportional to the size of the subarray it is called on.\\

\noindent\textbf{Exercise 7.1-4}\\

To modify QUICKSORT to run in nonincreasing order we need only modify line 4 of PARTITION, changing $\leq$ to $\geq$. \\

\noindent\textbf{Exercise 7.2-1}\\
By definition of $\Theta$, we know that there exists $c_1,c_2$ so that the $\Theta(n)$ term is between $c_1 n$ and $c_2 n$. We make that inductive hypothesis be that $c_1 m^2 \le T(m) \le c_2 m^2$ for all $m < n$, then, for large enough $n$, 
\[c_1 n^2 \le c_1 (n-1)^2 + c_1 n \le T(n-1) + \Theta(n)\] \[= T(n) = T(n-1) + \Theta(n) \le c_2 (n-1)^2 +c_2 n \le c_2 n^2\]\\


\noindent\textbf{Exercise 7.2-2}\\

The running time of QUICKSORT on an array in which evey element has the same value is $n^2$.  This is because the partition will always occur at the last position of the array (Exercise 7.1-2) so the algorithm exhibits worst-case behavior. \\

\noindent\textbf{Exercise 7.2-3}\\
If the array is already sorted in decreasing order, then, the pivot element is less than all the other elements. The partition step takes $\Theta(n)$ time, and then leaves you with a subproblem of size $n-1$ and a subproblem of size 0. This gives us the recurrence considered in 7.2-1. Which we showed has a solution that is $\Theta(n^2)$.\\

\noindent\textbf{Exercise 7.2-4}\\

Let's say that by ``almost sorted'' we mean that $A[i]$ is at most $c$ positions from its correct place in the sorted array, for some constant $c$. For INSERTION-SORT, we run the inner-while loop at most $c$ times before we find where to insert $A[j]$ for any particular iteration of the outer for loop.  Thus the running time time is $O(cn) = O(n)$, since $c$ is fixed in advance.  Now suppose we run QUICKSORT.  The split of PARTITION will be \emph{at best} $n-c$ to $c$, which leads to $O(n^2)$ running time. \\

\noindent\textbf{Exercise 7.2-5}\\
The minimum depth corresponds to repeatedly taking the smaller subproblem, that is, the branch whoose size is proportional to $\alpha$. Then, this will fall to 1 in $k$ steps where $1 \approx (\alpha)^k n$. So, $k \approx \log_{\alpha}(1/n) = - \frac{\lg(n)}{\lg(\alpha)} $. The longest depth corresponds to always taking the larger subproblem. we then have and identical expression, replacing $\alpha$ with $1-\alpha$.\\

\noindent\textbf{Exercise 7.2-6}\\

Without loss of generality, assume that the entries of the input array are distinct.  Since only the relative sizes of the entries matter, we may assume that $A$ contains a random permutation of the numbers 1 through $n$.  Now fix $0 < \alpha \leq 1/2$.  Let $k$ denote the number of entries of $A$ which are less than $A[n]$. PARTITION produces a split more balanced than $1-\alpha$ to $\alpha$ if and only if $\alpha n \leq k \leq (1-\alpha)n$.  This happens with probability $\frac{(1-\alpha)n - \alpha n + 1}{n} = 1-2\alpha + 1/n \approx 1 - 2\alpha$.\\

\noindent\textbf{Exercise 7.3-1}\\
We analyize the exprected run time because it represents the more typical time cost. Also, we are doing the expected run time over the possible randomness used during computation because it can't be produced adversatially, unlike when doing expected run time over all possible inputs to the algorithm.\\

\noindent\textbf{Exercise 7.3-2}\\

In the worst case, RANDOM returns the index of the largest element each time it's called, so $\Theta(n)$ calls are made.  In the best case, RANDOM returns the index of the element in the middle of the array and the array has distinct elements, so $\Theta(\lg n)$ calls are made. \\

\noindent\textbf{Exercise 7.4-1}\\
By definition of $\Theta$, we know that there exists $c_1,c_2$ so that the $\Theta(n)$ term is between $c_1 n$ and $c_2 n$. We make that inductive hypothesis be that $c_1 m^2 \le T(m) \le c_2 m^2$ for all $m < n$, then, for large enough $n$, 
\[
c_1 n^2 \le c_1 \max_{q\in [n]} n^2 -2 n(q+2) + (q+1)^2 +(q+1)^2 + n = \max_{q\in [n]} c_1 (n-q-2)^2 + c_1 (q+1)^2 +c_1 n \le \max_{q\in [n]} T( n-q-2) + T(q+1) + \Theta(n) = T(n)
\]
Similarly for the other direction
\\

\noindent\textbf{Exercise 7.4-2}\\

We'll use the substitution method to show that the best-case running time is $\Omega(n \lg n)$.  Let $T(n)$ be the best-case time for the procedure QUICKSORT on an input of size $n$.  We have the recurrence
\[ T(n) = \min_{1 \leq q \leq n-1} (T(q) + T(n-q-1)) + \Theta(n).\]
Suppose that $T(n) \geq c(n\lg n + 2n)$ for some constant $c$.  Substituting this guess into the recurrence gives
\begin{align*}
T(n) &\geq \min_{1 \leq q \leq n-1} (cq\lg q + 2cq + c(n-q-1)\lg(n-q-1) + 2c(n-q-1)) + \Theta(n) \\
&= \frac{cn}{2}\lg(n/2) + cn + c(n/2 - 1)\lg(n/2 - 1) + cn - 2c+ \Theta(n) \\
&\geq (cn/2)\lg n - cn/2 + c(n/2 - 1)(\lg n - 2) + 2cn - 2c \Theta(n) \\
&= (cn/2)\lg n - cn/2 + (cn/2)\lg n - cn - \lg n + 2 + 2cn - 2c\Theta(n) \\
&=cn\lg n + cn/2 - \lg n + 2 - 2c + \Theta(n)
\end{align*}
Taking a derivative with respect to $q$ shows that the minimum is obtained when $q = n/2$.  Taking $c$ large enough to dominate the $-\lg n + 2 - 2c + \Theta(n)$ term makes this greater than $cn\lg n$, proving the bound. \\

\noindent\textbf{Exercise 7.4-3}\\
We will treat the given expression to be continuous in $q$, and then, any extremal values must be either adjacent to a critical point, or one of the endpoints. The second derivative with respect to $q$ is $4$, So, we have that any ciritcal points we find will be minima. The expression has a derivative with respect to q of $2q - 2 (n-q-2) = -2n + 4q +4$ which is zero when we have $2q+2 =n$. So, there will be a minima at $q= \frac{n-2}{2}$. So, the maximal values must only be the endpoints. We can see that the endpoints are equally large because at $q=0$, it is $(n-1)^2$, and at $q=n-1$, it is $(n-1)^2+ (n-n+1 -1)^2 = (n-1)^2$.


\noindent\textbf{Exercise 7.4-4}\\

We'll use the lower bound (A.13) for the expected running time given in the section:
\begin{align*}
E[X] &= \sum_{i=1}^{n-1}\sum_{j=i+1}^n \frac{2}{j-i+1} \\
&= \sum_{i=1}^{n-1} \sum_{k=1}^{n-i} \frac{2}{k} \\
&\geq \sum_{i=1}^{n-1} 2\ln(n-i+1) \\
&= 2\ln\left(\prod_{i=1}^{n-1} n-i+1\right) \\
&= 2\ln(n!) \\
&= \frac{2}{\lg e} \lg(n!)
&\geq cn \lg n
\end{align*}
for some constant $c$ since $\lg(n!) = \Theta(n \lg n)$ by Exercise 3.2-3. Therefore RANDOMIZED-QUICKSORT's expected running time is $\Omega(n \lg n)$. \\

\noindent\textbf{Exercise 7.4-5}\\

If we are only doing quick-sort until the problem size becomes $\le k$, then, we will have to take $\lg(n/k)$ steps, since, as in the original analysis of randomized quick sort, we expect there to be $\lg(n)$ levels to the recursion tree. Since we then just call quicksort on the entire array, we know that each element is within $k$ of its final position. This means that an insertion sort will take the shifting of at most $k$ elements for every element that needed to change position. This gets us the running time described.

In theory, we should pick $k$ to minimze this expression, that is, taking a derivative with respect to $k$, we want it to be evaluating to zero. So, $n - \frac{n}{k} =0$, so $k \sim \frac{1}{n^2} $. The constant of proportianality will depend on the relative size of the constants in the $nk$ term and in the $n\lg(n/k)$ term. In practice, we would try it with a large number of input sizes for various values of $k$, because there are gritty properites of the machine not considered here such as cache line size.
\\

\noindent\textbf{Exercise 7.4-6}\\

For simplicity of analysis, we will assume that the elements of the array $A$ are the numbers 1 to $n$.  If we let $k$ denote the median value, the probability of getting at worst an $\alpha$ to $1-\alpha$ split is the probability that $\alpha n \leq k \leq (1-\alpha)n$.  The number of ``bad'' triples is equal to the number of triples such that at least two of the numbers come from $[1,\alpha n]$ or at least two of the numbers come from $[(1-\alpha)n, n]$.  Since both intervals have the same size, the probability of a bad triple is $2(\alpha^3 +3\alpha^2(1-\alpha))$.  Thus the probability of selecting a ``good'' triple, and thus getting at worst an $\alpha$ to $1-\alpha$ split, is $1 - 2(\alpha^3 +3\alpha^2(1-\alpha)) = 1 + 4\alpha^3 - 6\alpha^2$.

\noindent\textbf{Problem 1}\\
\begin{enumerate}[a.]
\item


\item
\end{enumerate}\\


\end{document}
\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 22}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}

\begin{document}
\maketitle


\noindent\textbf{Exercise 22.1-1}\\

Since it seems as though the list for the neighbors of each vertex $v$ is just an undecorated list, to find the length of each would take time $O(out-degree(v))$. So, the total cost will be $\sum_{v\in V} O(outdegree(v)) = O(|E| +|V|)$. Note that the $|V|$ showing up in the asymptotics is necessary, because it still takes a constant amount of time to know that a list is empty. This time could be reduced to $O(|V|)$ if for each list in the adjacency list representation, we just also stored its length.

To compute the in degree of each vertex, we will have to scan through all of the adjacency lists and keep counters for how many times each vertex has appeared. As in the previous case, the time to scan through all of the adjacency lists takes time $O(|E|+|V|)$.\\

\noindent\textbf{Exercise 22.1-2}\\

The adjacency list representation:
\begin{align*}
1&: 2,3\\
2&: 1,4,5 \\
3&: 1,6,7\\
4&: 2\\
5&: 5\\
6&: 3\\
7&: 3.\\
\end{align*}
The adjacency matrix representation:
\[ \left[\begin{array}{ccccccc} 0&1&1&0&0&0&0 \\ 1&0&0&1&1&0&0 \\ 1&0&0&0&0&1&1 \\ 0&1&0&0&0&0&0 \\ 0&1&0&0&0&0&0 \\ 0&0&1&0&0&0&0 \\ 0&0&1&0&0&0&0  \end{array}\right]. \]

\noindent\textbf{Exercise 22.1-3}\\

For the adjacency matrix representation, to compute the graph transpose, we just take the matrix transpose. This means looking along every entry above the diagonal, and swapping it with the entry that occurs below the diagonal. This takes time $O(|V|^2)$.

For the adjacency list representation, we will maintain an initially empty adjacency list representation of the transpose. Then, we scan through every list in the original graph. If we are in the list corresponding to vertex $v$ and see $u$ as an entry in the list, then we add an entry of $v$ to the list in the transpose graph corresponding to vertex $u$. Since this only requires a scan through all of the lists, it only takes time $O(|E|+|V|)$\\

\noindent\textbf{Exercise 22.1-4}\\

Create an array $A$ of size $|V|$. For a list in the adjacency list corresponding to vertex $v$, examine items on the list one by one.  If any item is equal to $v$, remove it.  If vertex $u$ appears on the list, examine $A[u]$.  If it's not equal to $v$, set it equal to $v$.  If it's equal to $v$, remove $u$ from the list.  Since we have constant time lookup in the array, the total runtime is $O(V+E)$.  \\


\noindent\textbf{Exercise 22.1-5}\\

From the adjacency matrix representation, if we take the square of the matrix, we are left an edge between all pairs of vertices that are separated by a path of exactly 2, so, to get the desired notion of the square of a graph, we also just have to add in the vertices that are separated by only a single edge in $G$, that is the entry $u,v$ in the final resulting matrix should be one iff either $G^2[u,v]$ or $G[u,v]$ are one. Taking the square of a matrix can be done with a matrix multiplication, which at the time of writing, can be most efficiently done by the Coppersmith-Windograd algorithm which takes time $O(|V|^{2.3728639})$. Since the other operation for computing the final result only takes time $O(|V|^2)$, the total runtime is $O(|V|^{2.3728639})$. 

If we are given an adjacency list representation, we can find the desired resulting graph by first computing the transpose graph $G^T$ from exercise 22.1-3 in $O(|V|+|E|)$ time. Then, our initally empty adjacency list representation of $G^2$ will be added to as follows. As we scan though the list of each vertex, say $v$, and see a entry going to $u$, then we add $u$ to the list corresponding to $v$, but also add $u$ to the list of everything on $v$'s list in $G^T$. This means that we may take as much as $O(|E||V|+|V|)$ time since, we have to spend potentially $|V|$ time as we process each edge. \\

\noindent\textbf{Exercise 22.1-6}\\

Start by examining position (1,1) in the adjacency matrix.  When examining position $(i,j)$, if a 1 is encountered, examine position $(i+1,j)$.  If a 0 is encountered, examine position $(i,j+1)$.  Once either $i$ or $j$ is equal to $|V|$, terminate.  I claim that if the graph contains a universal sink, then it must be at vertex $i$.  To see this, suppose that vertex $k$ is a universal sink.  Since $k$ is a universal sink, row $k$ in the adjacency matrix is all 0's, and column $k$ is all 1's except for position $(k,k)$ which is a 0.  Thus, once row $k$ is hit, the algorithm will continue to increment $j$ until $j = |V|$.  To be sure that row $k$ is eventually hit, note that once column $k$ is reached, the algorithm will continue to increment $i$ until it reaches $k$.  This algorithm runs in $O(V)$ and checking whether or not $i$ in fact corresponds to a sink is done in $O(V)$.  Therefore the entire process takes $O(V)$. \\

\noindent\textbf{Exercise 22.1-7}\\

We have two cases, one for the diagonal entries and one for the non-diagonal entries.

The entry of $[i,i]$ for some $i$ represents the sum of the in and ourt degrees of the vertex that $i$ corresponds to. To see this, we recall that an entry in a matrix product is the dot product of row $i$ in $B$ and column $i$ in $B^T$. But, column $i$ in $B^T$ is the same as row $i$ in $B$. So, we have that the entry is just row $i$ of $B$ dotted with itself, that is
\[
\sum_{j=1}^{|E|} b_{ij}^2
\]
However, since $b_{ij}$ only takes values in $\{-1,0,1\}$, we have that $b_{ij}^2$ only takes values in $\{0,1\}$, taking zero iff $b_{i,j}$ is zero. So, the entry is the sum of all nonzero entries in row $i$ of B, Since each edge leaving $i$ is $-1$ and each edge going to $i$ is 1, we are counting all the edges that either leave or enter $i$, as we wanted to show.

Now, suppose that our entry is indexed by $[i,j]$ where $i\neq j$. This is the dot product of row $i$ in $B$ with column $j$ in $B^T$, which is row $j$ in $B$. So, the entry is equal to 
\[
\sum_{k=1}^{|E|} b_{i,k}\cdot b_{j,k}
\]
Each term in this sum is $-1$ if $k$ goes between  $i$ and $j$, or $0$ if it doesn't. Since we can't have that two different vertices ar both on the same side of an edge, no terms may ever be 1. So, the entry is just -1 if there is an edge between $i$ and $j$, and zero otherwise.\\

\noindent\textbf{Exercise 22.1-8}\\

The expected loopup time is $O(1)$, but in the worst case it could take $O(V)$. If we first sorted vertices in each adjacency list then we could perform a binary search so that the worst case lookup time is $O(\lg V)$, but this has the disadvantage of having a much worse expected lookup time.  \\

\noindent\textbf{Exercise 22.2-1}\\

\begin{center}
$\begin{array}{|c|c|c|}
\hline
vertex& d&\pi\\
\hline
1&\infty&NIL\\
2&3&4\\
3&0&NIL\\
4&2&5\\
5&1&3\\
6&1&3\\
\hline
\end{array}
$
\end{center}

\noindent\textbf{Exercise 22.2-2}\\

These are the results when we examine adjacent vertices in lexicographic order:

\begin{tabular}{c|c|c}
Vertex & d & $\pi$ \\ \hline
r & 4 & s \\ \hline
s & 3 & w \\ \hline
t & 1 & u \\ \hline
u & 0 & NIL \\ \hline
v & 5 & r \\ \hline
w & 2 & t \\ \hline
x & 1 & u \\ \hline
y & 1 & u \\ \hline
\end{tabular}\\

\noindent\textbf{Exercise 22.2-3}\\

As mentioned in the errata, the question should state that we are to show that a single bit suffices by removing line $18$. To see why it is valid to remove line 18, consider the possible transitions between colors that can occur. In particular, it is impossible for a white vertex to go straight to black. This is because inorder for a vertex to be colored black, it must of been assigned to $u$ on line $11$. This means that we have to of enqueued the vertex in the queue at some point. This can only occur on line 17, however, if we are running line 17 on a vertex, we have to of run line 14 on it, giving it the color GRAY. Then, notice that the only testing of colors that is done anywhere is on line 13, in which we test whiteness. Since line 13 doesn't care if a vertex is GRAY or BLACK, and we only ever assign black to a gray vertex, we don't affect the running of the algorithm at all by removing line 18. Since, once we remove line 18, we ever assign BLACK to a vertex, we can represent the color by a single bit saying whether the vertex is WHITE or GRAY.\\

\noindent\textbf{Exercise 22.2-4}\\

If we use an adjacency matrix, for each vertex $u$ we dequeue we'll have to examine all vertices $v$ to decide whether or not $v$ is adjacent to $u$.  This makes the for-loop of line 12 $O(V)$.  In a connected graph we enqueue every vertex of the graph, so the worst case runtime becomes $O(V^2)$. \\

\noindent\textbf{Exercise 22.2-5}\\

First, we will show that the vale $d$ assigned to a vertex is independent of the order that entries appear in adjacency lists. To do this, we rely on theorem 22.5 which proves correctness of BFS. In particular, that we have $\nu.d = \delta(s,\nu)$ at the end of the procedure. Since $\delta(s,\nu)$ is a property of the underlying graph, no matter which representation of the graph in terms of adjacency lists that we choose, this value will not chage. Since the $d$ values are equal to this thing that doesn't change when we mess with the adjacency lists, it too doesn't change when we mess with the adjacency lists.

Now, to show that $\pi$ does depend on the ordering of the adjacency lists, we will be using Figure 22.3 as a guide. First, we note that in the given worked out procedure, we have that in the adjacency list for $w$, $t$ precedes $x$. Also, in the worked out procedure, we have that $u.\pi = t$. Now, suppose instead that we had $x$ preceding $t$ in the adjacency list of $w$. Then, it would get added to the queue before $t$, which means that it would $u$ as it's child before we have a chance to process the children of $t$. This will mean that $u.\pi = x$ in this different ordering of the adjacency list for $w$.\\

\noindent\textbf{Exercise 22.2-6}\\

Let $G$ be the graph shown in the first picture, $G' = (V, E_\pi)$ be the graph shown in the second picture, and 1 be the source vertex.  Let's see why $E_\pi$ can never be produced by running BFS on $G$.  Suppose that 2 preceeds 5 in the adjacency list of 1.  We'll dequeue 2 before 5, so $3.\pi$ and $4.\pi$ must both equal 2.  However, this is not the case.  Thus, 5 must have preceded 2 in the adjacency list.  However, this implies that $3.\pi$ and $4.\pi$ both equal 5, which again isn't true. Nonetheless, it is easily seen that the unique simple path in $G'$ from 1 to any vertex is a shortest path in $G$. \\

\begin{tikzpicture}
  [scale=.8,auto=left,every node/.style={circle = black}]
  \node (n4) at (4,8)  {4};
  \node (n5) at (8,9)  {5};
  \node (n1) at (11,8) {1};
  \node (n2) at (9,6)  {2};
  \node (n3) at (5,5)  {3};

  \foreach \from/\to in {n2/n4,n5/n3,n4/n5,n5/n1,n1/n2,n2/n3}
    \draw (\from) -- (\to);
\end{tikzpicture}

\begin{tikzpicture}
  [scale=.8,auto=left,every node/.style={circle = black}]
  \node (n4) at (4,8)  {4};
  \node (n5) at (8,9)  {5};
  \node (n1) at (11,8) {1};
  \node (n2) at (9,6)  {2};
  \node (n3) at (5,5)  {3};

  \foreach \from/\to in {n4/n5,n5/n1,n1/n2,n2/n3}
    \draw (\from) -- (\to);
\end{tikzpicture}

\noindent\textbf{Exercise 22.2-7}\\

This problem is basically just a obfuscated version of two coloring. We will try to color the vertices of this graph of rivalries by two colors, ``babyface'' and ``heel''. To have that no two babyfaces and no two heels have a rivalry is the same as saying that the coloring is proper. To two color, we perform a breadth first search of each connected component to get the d values for each vertex. Then, we give all the odd ones one color say ``heel'', and all the even d values a different color. We know that no other coloring will succeed where this one fails since if we gave any other coloring, we would have that a vertex $v$ has the same color as $v.\pi$ since $v$ and $v.\pi$ must have different parities for their $d$ values. Since we know that there is no better coloring, we just need to check each edge to see if this coloring is valid. If each edge works, it is possible to find a designation, if a single edge fails, then it is not possible. Since the BFS took time $O(n+r)$ and the checking took time $O(r)$, the total runtime is $O(n+r)$.\\

\noindent\textbf{Exercise 22.2-8}\\

Suppose that $a$ and $b$ are the endpoints of the path in the tree which achieve the diameter, and without loss of generality assume that $a$ and $b$ are the unique pair which do so.  Let $s$ be any vertex in $T$.  I claim that the result of a single BFS will return either $a$ or $b$ (or both) as the vertex whose distance from $s$ is greatest.  To see this, suppose to the contrary that some other vertex $x$ is shown to be furthest from $s$. (Note that $x$ cannot be on the path from $a$ to $b$, otherwise we could extend).  Then we have $d(s,a) < d(s,x)$ and $d(s,b) < d(s,x)$.  Let $c$ denote the vertex on the path from $a$ to $b$ which minimizes $d(s,c)$.  Since the graph is in fact a tree, we must have $d(s,a) = d(s,c) + d(c,a)$ and $d(s,b) = d(s,c) + d(c,b)$.  (If there were another path, we could form a cycle).  Using the triangle inequality and inequalities and equalities mentioned above we must have
\[  d(a,b) + 2d(s,c) = d(s,c) + d(c,b) + d(s,c) + d(c,a) < d(s,x) + d(s,c) + d(c,b).\]

I claim that $d(x,b) = d(s,c) + d(s,b)$.  It not, then by the triangle inequality we must have a strict less-than.  In other words, there is some path from $x$ to $b$ which does not go through $c$.  This gives the contradiction, because it implies there is a cycle formed by concatenating these paths.  Then we have

\[ d(a,b) < d(a,b) + 2d(s,c) < d(x,b).\]

Since it is assumed that $d(a,b)$ is maximal among all pairs, we have a contradiction. Therefore, since trees have $|V| - 1$ edges, we can run BFS a single time in $O(V)$ to obtain one of the vertices which is the endpoint of the longest simple path contained in the graph. Running BFS again will show us where the other one is, so we can solve the diameter problem for trees in $O(V)$. \\


\noindent\textbf{Exercise 22.2-9}\\

First, the algorithm computes a minimum spanning tree of the graph. Note that this can be done using the procedures of Chapter 23. It can also be done by performing a breadth first search, and restricting to the edges between $v$ and $v.\pi$ for every v. To aide in not double counting edges, fix any ordering $\le$ on the vertices before hand. Then, we will construct the sequence of steps by calling $MAKE-PATH(s)$ where $s$ was the root used for the BFS. 
\begin{algorithm}
\caption{MAKE-PATH(u)}
\begin{algorithmic}
\For{v adjacent to u in the original graph, but not in the tree such that $u\le v$}
\State go to v and back to u
\EndFor
\For{v adjacent to u in the tree, but not equal to $u.\pi$}
\State go to $v$
\State perform the path proscribed by MAKE-PATH(v)
\EndFor
\State go to $u.\pi$
\end{algorithmic}
\end{algorithm}



\noindent\textbf{Exercise 22.3-1}\\
For directed graphs:

$
\begin{array}{c|c|c|c|}
from\backslash to& BLACK&GRAY&WHITE\\
\hline
BLACK&All kinds& Back, Cross& Back, Cross\\
\hline
GRAY&Tree, Forward, Cross&Tree, Forward, Back&Back, Cross\\
\hline
WHITE&Cross, Tree, Forward&Cross, Back&all kinds\\
\hline
\end{array}
$\\

For undirected graphs, note that the lower diagonal is defined by the upper diagonal:

$
\begin{array}{c|c|c|c|}
from\backslash to& BLACK&GRAY&WHITE\\
\hline
BLACK&All kinds&All kinds&All kinds\\
\hline
GRAY&-&Tree, Forward, Back&All kinds\\
\hline
WHITE&-&-&All kinds\\
\hline
\end{array}
$
%22.3-1 could use a check for correctness

\noindent\textbf{Exercise 22.3-2}\\

The following table gives the discovery time and finish time for each vetex in the graph. \\

$\begin{tabular}{c|c|c} 
Vertex & Discovered & Finished \\ \hline
q & 1 & 16 \\ \hline
r & 17 & 20 \\ \hline
s & 2 & 7 \\ \hline
t & 8 & 15 \\ \hline
u & 18 & 19 \\ \hline
v & 3 & 6 \\ \hline
w & 4 & 5 \\ \hline
x & 9 & 12 \\ \hline
y & 13 & 14 \\ \hline
z & 10 & 11 \\ 
\end{tabular}$\\

The tree edges are: $(q,s), (s,v), (v,w), (q,t), (t,x), (x,z), (t,y), (r,u)$.  The back edges are: $(w,s), (y,q), (z,x) $. The forward edge is: $(q,w)$.  The cross edges are: $(u,y), (r,y)$. \\

\noindent\textbf{Exercise 22.3-3}\\

As pointed out in figure 22.5, the parentheses structure of the DFS of figure 22.4 is $(((())()))(()())$\\

\noindent\textbf{Exercise 22.3-4}\\

Treat white vertices as 0 and non-white vertices as 1.  Since we never check whether or not a vertex is gray, deleting line 3 doesn't matter.  We need only know whether a vertex is white to get the same results. \\

\noindent\textbf{Exercise 22.3-5}\\

\begin{enumerate}[a.]
\item
Since we have that $u.d < v.d$, we know that we have first explored $u$ before $v$. This rules out back edges and rules out the possibility that $v$ is on a tree that has been explored before exploring $u$'s tree. Also, since we return from $v$ before returning from $u$, we know that it can't be on a tree that was explored after exploring $u$. So, This rules out it being a cross edge. Leaving us with the only possibilities of being a tree edge or forward edge.

To show the other direction, suppose that $(u,v)$ is a tree or forward edge. In that case, since $v$ occurs further down the tree from $u$, we know that we have to explored $u$ before $v$, this means that $u.d< v.d$. Also, since we have to of finished $v$ before coming back up the tree, we have that $v.f<u.f$. The last inequality to show is that $v.d < v.f$ which is trivial.
\item
By similar reasoning to part $a$, we have that we must have v being an ancestor of $u$ on the DFS tree. This means that the only type of edge that could go from u to v is a back edge.

To show the other direction, suppose that $(u,v)$ is a back edge. This means that we have that $v$ is above $u$ on the DFS tree. This is the same as the second direction of part a where the roles of u and v are reversed. This means that the inequalities follow for the same reasons.


\item
Since we have that $v.f < u.d$, we know that either $v$ is a descendant of $u$ or it comes on some branch that is explored before $u$. Similarly, since $v.d < u.d$, we either have that $u$ is a descendant of $v$ or it comes on some branch that gets explored before $u$. Putting these together, we see that it isn't possible for both to be descendants of each other. So, we must have that $v$ comes on a branch before $u$, So, we have that $u$ is a cross edge.

To See the other direction, suppose that $(u,v)$ is a cross edge. This means that we have explored $v$ at some point before exploring $u$, otherwise, we would have taken the edge from $u$ to $v$ when exploring $u$, which would make the edge either a forward edge or a tree edge. Since we explored $v$ first, and the edge is not a back edge, we must of finished exploring $v$ before starting $u$, so we have the desired inequalities.


\end{enumerate}

\noindent\textbf{Exercise 22.3-6}\\

By Theorem 22.10, every edge of an undirected graph is either a tree edge or a back edge.  First suppose that $v$ is first discovered by exploring edge $(u,v)$.  Then by definition, $(u,v)$ is a tree edge.  Moreover, $(u,v)$ must have been discovered before $(v,u)$ because once $(v,u)$ is explored, $v$ is necessarily discovered.  Now suppose that $v$ isn't first discovered by $(u,v)$.  Then it must be discovered by $(r,v)$ for some $r \neq u$.  If $u$ hasn't yet been discovered then if $(u,v)$ is explored first, it must be a back edge since $v$ is an ancestor of $u$. If $u$ has been discovered then $u$ is an ancestor of $v$, so $(v,u)$ is a back edge. \\

\noindent\textbf{Exercise 22.3-7}\\

See the algorithm DFS-STACK(G). Note that by a similar justification to 22.2-3, we may remove line 8 from the original DFS-VISIT algorithm without changing the final result of the program, that is just working with the colors white and gray.
\begin{algorithm}
\caption{DFS-STACK(G)}
\begin{algorithmic}
\For{every $u\in G.V$}
\State u.color = WHITE
\State $u.\pi$ = NIL
\EndFor
\State time = 0
\State S is an empty stack
\While{there is a white vertex u in $G$}
\State S.push(u)
\While{S is nonempty}
\State $v = S.pop$
\State time++
\State v.d = time
\For{all neighbors w of v}
\If{w.color == WHITE}
\State w.color = GRAY
\State $w.\pi$ = v
\State $S.push(w)$
\EndIf
\EndFor
\State time++
\State v.f = time
\EndWhile
\EndWhile
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Exercise 22.3-8}\\

Consider a graph with 3 vertices $u$, $v$, and $w$, and with edges $(w,u), (u,w)$, and $(w,v)$.  Suppose that DFS first explores $w$, and that $w$'s adjecency list has $u$ before $v$.  We next discover $u$.  The only adjacent vertex is $w$, but $w$ is already grey, so $u$ finishes. Since $v$ is not yet a descendent of $u$ and $u$ is finished, $v$ can never be a descendent of $u$. \\

\noindent\textbf{Exercise 22.3-9}\\

Consider the Directed graph on the vertices $\{1,2,3\}$, and having the edges $(1,2),(1,3),(2,1)$ then there is a path from $2$ to $3$, however, if we start a DFS at $1$ and process $2$ before $3$, we will have $2.f = 3 < 2 = 2.d$ which provides a counterexample to the given conjecture.\\

\noindent\textbf{Exercise 22.3-10}\\

We need only update DFS-VISIT.  If $G$ is undirected we don't need to make any modifications.  We simply note that lines 11 through 16 will never be executed. \\
\begin{algorithm}
\caption{DFS-VISIT-PRINT(G,u)}
\begin{algorithmic}[1]
\State $time = time + 1$
\State $u.d = time$
\State $u.color = GRAY$
\For{each $v \in G.Adj[u]$}
	\If{$v.color == white$}
		\State Print ``$(u,v)$ is a Tree edge''
		\State $v.\pi = u$
		\State DFS-VISIT-PRINT$(G,v)$
	\ElsIf{$v.color == grey$}
		\State Print ``$(u,v)$ is a Back edge''
	\Else
		\If{$v.d > u.d$}
			\State Print ``$(u,v)$ is a Forward edge''
		\Else
			\State Print ``$(u,v)$ is a Cross edge''
		\EndIf
	\EndIf
\EndFor 
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Exercise 22.3-11}\\

Suppose that we have a directed graph on the vertices $\{1,2,3\}$ and having edges $(1,2),(2,3)$ then, 2 has both incoming and outgoing edges. However, if we pick our first root to be $3$, that will be in it's own DFS tree. Then, we pick our second root to be $2$, since the only thing it points to has already been marked BLACK, we wont be exploring it. Then, picking the last root to be $1$, we don't screw up the fact that $2$ is along in a DFS tree despite the fact that it has both an incoming and outgoing edge in $G$.\\

\noindent\textbf{Exercise 22.3-12}\\

The modifications work as follows:  Each time the if-condition of line 8 is satisfied in DFS-CC, we have a new root of a tree in the forest, so we update its cc label to be a new value of $k$.  In the recursive calls to DFS-VISIT-CC, we always update a descendent's connected component to agree with its ancestor's. \\

\begin{algorithm}
\caption{DFS-CC(G)}
\begin{algorithmic}[1]
\For{each vertex $u \in G.V$}
	\State $u.color = white$
	\State $u.\pi = NIL$
\EndFor
\State $time = 0$
\State $k = 1$
\For{each vertex $u \in G.V$}
	\If{$u.color == white$}
		\State $u.cc = k$
		\State $k = k + 1$
		\State DFS-VISIT-CC(G,u)
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{DFS-VISIT-CC(G,u)}
\begin{algorithmic}[1]
\State $time = time + 1$
\State $u.d = time$
\State $u.color = GRAY$
\For{each $v \in G.Adj[u]$}
	\State $v.cc = u.cc$
	\If{$v.color == white$}
		\State $v.\pi = u$
		\State DFS-VISIT-CC$(G,v)$
	\EndIf
\EndFor 
\State $u.color = black$
\State $time = time + 1$
\State $u.f = time$
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Exercise 22.3-13}\\

Samir Khuller reveals through personal communication with Cormen that at the time of writing, the best solution known to Cormen takes time $O(|V||E|)$. This is mentioned in his paper ``An $O(|V|^2)$ algorithm for single connectedness''. We here present an even better algorithm which only takes time $O(|E| + |V|\alpha(|V|))$.

First, we construct the component graph for the given directed graph. Next, we check each of the strongly connected components to make sure that they are singly connected. To do this, we first obtain a ear decomposition of the component, which we are assure exists because it is strongly connected. So, we are given a sequence of directed graphs $G_1, \ldots,G_j$ where $G_j$ is equal to the whole component, $G_1$ is just a cycle, and each $G_{i+1}$ is obtained from $G_i$ by adding in a path on new vertices from one existing vertex of $G_i$ to another. Then, if and only if on each step in the ear decomposition, path that gets added in starts and ends on the same vertex in the previous step's graph, we have that the graph will be singly connected.

First, we show the ``only if'' direction. To see this, suppose that one ear that we add on goes from $u$ to $v\neq u$. However, the graphs all along the way are also strongly connected because a prefix of an ear decomposition is also an ear decomposition. This means that there already was a path from $u$ to $v$, and we just added one in. This means that there are now at least two paths from $u$ to $v$, which contradicts being singly connected.

Now, for the more difficult ``if'' direction. We will do it inductively on how many ears we have added. First, we notice that $G_1$ is singly connected because in a cycle there is no choice in picking a path from one vertex to another, the only option is to keep going around the cycle until you reach your destination. Now, suppose that $G_i$ is singly connected, and $G_{i+1}$ is obtained by adding a path on new vertices going from $v\in G_i$ back to $v$. We then pick two vertices $u$, $w$, and consider the simple paths from $u$ to $w$. There are four possibilities for where they could lie. First, we could have $u, w \in G_i$. Then, there was only one simple path in $G_i$ from $u$ to $w$, because $G_i$ was singly connected. Also, if we were to try and use any of the edges in the ear we added when going to $G_{i+1}$, we would have to go to $v$ first, however, to get back to $w$, we would need to visit $v$ again, since that is the only way to get from the vertices in the ear back to the original graph. The next possibility is that $u\in G_i$, and $w\in G_{i+1}$. Then, any path from $u$ to $w$ must pass through $v$. Since $v\in G_i$, there is at most one path using edges in $G_i$ to get from $u$ to $v$. Then, going from $v$ to $w$, our only option is to start walking along the ear. The third possibility is that $u\in G_{i+1}$, but $w\in G_i$. In this case, traveling from $u$, our only possibility is to keep walking along the edge until we get to $v$. From $v$, we cannot take the edge going into the added ear, because that would mean that we would hit $u$ again. So, the rest of our path, going from $v$ to $w$ must lie entirely in $G_i$. This means that there is only one, since we had that $G_i$ was singly connected. The last case is that both $u$ and $w$ are in the added ear. If $u$ comes before $w$ in the ear, we just plod along, and have no hope of flexibility in our path. If $w$ comes before $u$, then, leaving $u$, we arrive at $v$. If we were to take any edge than out along the ear, we would only be able to get out on the ear later by revising $v$, making the path be non-simple. Since this is the only acceptable option, there is only a single simple path from $u$ to $w$.


Once we have checked all of the strongly connected components, must also check that between each strongly connected component, there is at most one edge, since otherwise we would be able to find two distinct simple paths between the same pairs of vertices.

Then, we are left with the component graph, which is a directed acyclic graph. This means that we can perform a topological sort on it, to get an ordering of vertices in the component graph so that edges only go to the later vertices. Call this ordering $\{v_1,v_2, \ldots, v_k\}$. Since each component consists of at least one vertex, we know that $k\le |V|$. Now, recalling the structure for storing disjoint sets given in chapter 21, we make a set for each of the vertices of the component graph. Then, as we process the vertices from earlier to later in the topological ordering, we first make sure that running FIND-SET for the current vertex returns a different representative than find set for any of its children. Then, we call UNION between the current vertex and all of its children. If at any point we have that the vertex under consideration has the same disjoint set representative as any of its children, this means that there is some component in the past that is both the ancestor of the current vertex, and, through a different path, the ancestor of the child. This would mean that the graph is not singly connected. Once we have run through all the vertices of the component graph in this fashion, if we have not had any such conflict, then the graph is going to be singly connected. This can be seen by induction on the number of vertices in the component graph. See the algorithm IS-SINGLY-CONNECTED(G).

This last part that operates on the component graph takes time $O(|E|+ |V|\alpha(|V|))$ because we have a sequence of disjoint set operations where at most $|V|$ of them are make set operations, and there are at most $|E|+|V|$ find set operations, and $|V|-1$ union operations. Since all of the things we did before this took only linear time, we have that the total runtime of this algorithm is $O((|E|+|V|)\alpha(|V|))$. This is not too great news for us as it stands because in the case of having very high edge density ($E = \Omega(n^2)$ is possible by taking half the vertices and giving them an edge to all of the other half of vertices), this algorithm is slightly slower than the existing ones which do it in time $O(n^2)$. However, this analysis of the runtime was based on an arbitrary ordering of all the different operations. We however know that we will be alternating at most $|V|$ times from performing $FIND-SET$ operations and performing $UNION$ operations, getting all of our $MAKE-SET$ operations out of the way at the beginning.

%The last issue to get around is that there may be more than $V\alpha(V)$ edges in the orginal graph, so, we would end up taking as much as $O(|E|)$ time to perform the procedure for finding strongly connected components or for performing a topological sort. To deal with this, we notice that from the structure we have proven on the possible singly connected graphs, there can be at most $4|V|$ edges in any singly connected graph. So, we can just keep a count of how many distinct edges we've encountered so far, and if it ever grows above this, we immediately say that the graph is not singly connected. Since we have that there are at most $|V|$ make set operations, any sequence of $O(|V|)$ union and find on these will take time only $O(|V|\alpha(|V|)$ by theorem 21.1. This means that the total runtime is just $O(|V|\alpha(|V|))$.

\begin{algorithm}
\caption{IS-SINGLY-CONNECTED(G)}
\begin{algorithmic}
\State{Compute the strongly connected components of $G$, as described in 22.5}
\State{For each strongly connected component, perform an ear decomposition, and make sure each ear starts and ends at the same vertex}
\State{Construct the component graph as shown in exercise 22.5-5, checking to make sure you never have more than one edge going between the same two components}
\State{Perform Topological sort on the component graph, to get $\{v_i\}_{i\in[k]}$}
\State{Run MAKE-SET for each vertex of the component graph}
\For{i=1,2,\ldots, k}
\For{children u of $v_i$}
\If{FIND-SET($v_i$)=FIND-SET(u)}
\State \Return FALSE
\EndIf
\EndFor
\For{children u of $v_i$}
\State{UNION(u,$v_i$)}
\EndFor
\EndFor
\State \Return TRUE
\end{algorithmic}
\end{algorithm}

\begin{comment}
First, we check for the graph to be acyclic. This can be done with a DFS looking for black edges, as proven in lemma 22.11. Once we know it is  acyclic, we perform a topological sort. Both of these steps take time $O(|V|+|E|)$. Then, to check for single contentedness, we create a disjoint set structure in which each vertex is in its own set. Then, looking at the vertices from left to right in the topological sort, we will union together all the children of the vertex under consideration. Before this though, we check to make sure that the vertex under consideration is in a different set than all of its children. If we had that the current object were in the same set as it's children that means that there is some ancestor of the current vertex that is also a parent of the current vertex's child. This would mean that there are two paths from that ancestor to the child. One that's direct, and one that passes through the current vertex.\end{comment}

\noindent\textbf{Exercise 22.4-1}\\

Our start and finish times from performing the DFS are

\begin{center}
$
\begin{array}{|c|c|c|}
\hline
label & d&f\\
\hline
m&1&20\\
\hline
q&2&5\\
\hline
t&3&4\\
\hline
r&6&19\\
\hline
u&7&8\\
\hline
y&9&18\\
\hline
v&10&17\\
\hline
w&11&14\\
\hline
z&12&13\\
\hline
x&15&16\\
\hline
n&21&26\\
\hline
o&22&25\\
\hline
s&23&24\\
\hline
p&27&28\\
\hline
\end{array}
$
\end{center}

And so, by reading off the entries in decreasing order of finish time, we have the sequence $p,n,o,s,m,r,y,v,x,w,z,u,q,t$.\\

\noindent\textbf{Exercise 22.4-2}\\

The algorithm works as follows.  The attribute $u.paths$ of node $u$ tells the number of simple paths from $u$ to $v$, where we assume that $v$ is fixed throughout the entire process.  To count the number of paths, we can sum the number of paths which leave from each of $u$'s neighbors.  Since we have no cycles, we will never risk adding a partially completed number of paths.   Moreover, we can never consider the same edge twice amongst the recursive calls.  Therefore, the toal number of executions of the for-loop over all recursive calls is $O(V + E)$.  Calling SIMPLE-PATHS$(s,t)$ yields the desired result. \\
\begin{algorithm}
\caption{SIMPLE-PATHS(u,v)}
\begin{algorithmic}[1]
\If{$u == v$}
	\State Return 1
\ElsIf{$u.paths \neq NIL$}
	\State Return $u.paths$
\Else
	\For{each $w \in Adj[u]$}
		\State $u.paths = u.paths +$ SIMPLE-PATHS$(w,v)$
	\EndFor
	\State Return $u.paths$
\EndIf
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Exercise 22.4-3}\\

We can't just use a depth first search, since that takes time that could be worst case linear in $|E|$. However we will take great inspiration from DFS, and just terminate early if we end up seeing an edge that goes back to a visited vertex. Then, we should only have to spend a constant amount of time processing each vertex. Suppose we have an acyclic graph, then this algorithm is the usual DFS, however, since it is a forest, we have $|E| \le |V|-1$ with equality in the case that it is connected. So, in this case, the runtime of $O(|E|+|V|)$ $O(|V|)$. Now, suppose that the procedure stopped early, this is because it found some edge coming from the currently considered vertex that goes to a vertex that has aleady been considered. Since all of the edges considered up to this point didn't do that, we know that they formed a forest. So, the number of edges considered is at most the number of vertices considered, which is $O(|V|)$. So, the total runtime is $O(|V|)$.\\

\noindent\textbf{Exercise 22.4-4}\\

This is not true.  Consider the graph $G$ consisting of vertices $a, b, c, $ and $d$.  Let the edges be $(a,b), (b,c), (a,d), (d,c),$ and $(c,a)$.  Suppose that we start the DFS of TOPOLOGICAL-SORT at vertex $c$.  Assuming that $b$ appears before $d$ in the adjacency list of $a$, the order, from latest to earliest, of finish times is $c, a, d, b$. The ``bad'' edges in this case are $(b,c)$ and $(d,c)$.  However, if we had instead ordered them by $a,b,d,c$ then the only bad edges would be $(c,a)$.  Thus TOPOLOGICAL-SORT doesn't always minimizes the number of ``bad'' edges. \\

\noindent\textbf{Exercise 22.4-5}\\

Consider having a list for each potential in degree that may occur. We will also make a pointer from each vertex to the list that contains it. The initial construction of this can be done in time $O(|V|+|E|)$ because it only requires computing the in degree of each vertex, which can be done in time $O(|V|+|E|)$ (see problem 22.1-3). Once we have constructed this sequence of lists, we repeatedly extract any element from the list corresponding to having in degree zero. We spit this out as the next element in the topological sort. Then, for each of the children $c$ of this extracted vertex, we remove it from the list that contains it and insert it into the list of in degree one less. Since a deletion and an insertion in a doubly linked list can be done in constant time, and we only have to do this for each child of each vertex, it only has to be done $|E|$ many times. Since at each step, we are outputting some element of in degree zero with respect to all the vertices that hadn't yet been output, we have successfully output a topological sort, and the total runtime is just $O(|E|+|V|)$. We also know that we can always have that there is some element to extract from the list of in degree 0, because otherwise we would have a cycle somewhere in the graph. To see this, just pick any vertex and traverse edges backwards. You can keep doing this indefinitely because no vertex has in degree zero. However, there are only finitely many vertices, so at some point you would need to find a repeat, which would mean that you have a cycle.

If the graph was not acyclic to begin with, then we will have the problem of having an empty list of vertices of in degree zero at some point. That is, if the vertices left lie on a cycle, then none of them will have in degree zero.\\

\noindent\textbf{Exercise 22.5-1}\\

It can either stay the same or decrease. To see that it is possible to stay the same, just suppose you add some edge to a cycle. To see that it is possible to decrease, suppose that your original graph is on three vertices, and is just a path passing through all of them, and the edge added completes this path to a cycle. To see that it cannot increase, notice that adding an edge cannot remove any path that existed before. So, if $u$ and $v$ are in the same connected component in the original graph, then there are a path from one to the other, in both directions. Adding an edge wont disturb these two paths, so we know that $u$ and $v$ will still be in the same SCC in the graph after adding the edge. Since no components can be split apart, this means that the number of them cannot increase since they form a partition of the set of vertices.\\

\noindent\textbf{Exercise 22.5-2}\\

The finishing times of each vertex were computed in exercise 22.3-2.  The forest consists of 5 trees, each of which is a chain.  We'll list the vertices of each tree in order from root to leaf: $r$, $u$, $q-y-t$, $x-z$, and $s-w-v$.\\

\noindent\textbf{Exercise 22.5-3}\\

Professor Bacon's suggestion doesn't work out. As an example, suppose that our graph is on the three vertices $\{1,2,3\}$ and consists of the edges $(2,1),(2,3),(3,2)$. Then, we should end up with $\{2,3\}$ and $\{1\}$ as our SCC's. However, a possible DFS starting at 2 could explore 3 before 1, this would mean that the finish time of 3 is lower than of 1 and 2. This means that when we first perform the DFS starting at 3. However, a DFS starting at 3 will be able to reach all other vertices. This means that the algorithm would return that the entire graph is a single SCC, even though this is clearly not the case since there is neither a path from 1 to 2 of from 1 to 3.\\

\noindent\textbf{Exercise 22.5-4}\\

First observe that $C$ is a strongly connected component of $G$ if and only if it is a strongly connected component of $G^T$.  Thus the vertex sets of $G^{SCC}$ and $(G^{T})^{SCC}$ are the same, which implies the vertex sets of $((G^T)^{SCC})^T$ and $G^{SCC}$ are the same.  It suffices to show that their edge sets are the same.  Suppose $(v_i, v_j)$ is an edge in $((G^T)^{SCC})^T$.  Then $(v_j, v_i)$ is an edge in $(G^{T})^{SCC}$.  Thus there exist $x \in C_j$ and $y \in C_i$ such that $(x,y)$ is an edge of $G^T$, which implies $(y,x)$ is an edge of $G$.  Since components are preserved, this means that $(v_i, v_j)$ is an edge in $G^{SCC}$.  For the opposite implication we simply note that for any graph $G$ we have $(G^T)^T = G$. \\

\noindent\textbf{Exercise 22.5-5}\\

Given the procedure given in the section, we can compute the set of vertices in each of the strongly connected components. For each vertex, we will give it an entry SCC, so that $v.SCC$ denotes the strongly connected component (vertex in the component graph) that v belongs to. Then, for each edge $(u,v)$ in the original graph, we add an edge from $u.SCC$ to $v.SCC$ if one does not already exist. This whole process only takes a time of $O(|V|+|E|)$. This is because the procedure from this section only takes that much time. Then, from that point, we just need a constant amount of work checking the existence of an edge in the component graph, and adding one if need be.\\

\noindent\textbf{Exercise 22.5-6}\\

By Exercise 22.5-5 we can compute the component graph in $O(V+E)$ time, and we may as well label each node with its component as we go (see exercise 22.3-12 for the specifics), as well as creating a list for each component which contains the vertices in that component by forming an array $A$ such that $A[i]$ contains a list of the vertices in the $i^{th}$ connected component.  Then run DFS again, and for each edge encountered, check whether or not it connects two different components.  If it doesn't, delete it.  If it does, determine whether it is the first edge connecting them.  If not, delete it.  This can be done in constant time per edge since we can store the component edge information in a $k$ by $k$ matrix, where $k$ is the number of connected components.  The runtime of this is thus $O(V+E)$.  Now the only edges we have are a minimal number which connect distinct connected components. The last step is place edges within the connected compoenents in a minimal way.  The fewest edges which can be used to create a connected component with $n$ vertices is $n$, and this is done with a cycle. For each connected component, let $v_1, v_2, \ldots, v_k$ be the vertices in that component.  We find these by using the array $A$ created earlier.  Add in the edges $(v_1, v_2), (v_2,v_3), \ldots, (v_k, v_1)$. This is linear in the number of vertices, so the total runtime is $O(V+E)$.\\

\noindent\textbf{Exercise 22.5-7}\\ 

First compute the component graph as in 22.5-5. Then, in order to have that every vertex either has a path to or from every other vertex, we need that this component graph also has this property. Since this is acyclic, we can perform a topological sort on it. For this to be the case, we want that there is a single path through this dag that hits every single vertex. This can only happen in the DAG if each vertex has an edge going to the vertex that appears next in the topological ordering. See the algorithm IS-SEMI-CONNECTED(G).\\

\begin{algorithm}
\caption{IS-SEMI-CONNECTED(G)}
\begin{algorithmic}
\State Compute the component graph of $G$, call it $G'$
\State Perform a topological sort on $G'$ to get the ordering of its vertices $v_1,v_2, \ldots, v_k$.
\For{i=1..k-1}
\If{there is no edge from $v_i$ to $v_{i+1}$}
\State \Return FALSE
\EndIf
\EndFor
\State \Return TRUE
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Problem 22-1}\\

\begin{enumerate}[a)]
\item
\begin{enumerate}[1.]
\item
If we found a back edge, this means that there are two vertices, one a descendant of the other, but there is already a path from the ancestor to the child that doesn't involve moving up the tree. This is a contradiction since the only children in the bfs tree are those that are a single edge away, which means there cannot be any other paths to that child because that would make it more than a single edge away. To see that there are no forward edges, We do a similar procedure. A forward edge would mean that from a given vertex we notice it has a child that has already been processed, but this cannot happen because all children are only one edge away, and for it to of already been processed, it would need to have gone through some other vertex first.
\item
An edge is placed on the list to be processed if it goes to a vertex that has not yet been considered. This means that the path from that vertex to the root must be at least the distance from the current vertex plus 1. It is also at most that since we can just take the path that consists of going to the current vertex and taking its path to the root.
\item
We know that a cross edge cannot be going to a depth more than one less, otherwise it would be used as a tree edge when we were processing that earlier element. It also cannot be going to a vertex of depth more than one more, because we wouldn't of already processed a vertex that was that much further away from the root. Since the depths of the vertices in the cross edge cannot be more than one apart, the conclusion follows by possibly interchanging the roles of $u$ and $v$, which we can do because the edges are unordered.
\end{enumerate}
\item
\begin{enumerate}[1.]
\item
To have a forward edge, we would need to have already processed a vertex using more than one edge, even though there is a path to it using a single edge. Since breadth first search always considers shorter paths first, this is not possible.
\item
Suppose that $(u,v)$ is a tree edge. Then, this means that there is a path from the root to $v$ of length $u.d+1$ by just appending $(u,v)$ on to the path from the root to $u$. To see that there is no shorter path, we just note that we would of processed $v$ sooner, and so wouldn't currently have a tree edge if there were.
\item
To see this, all we need to do is note that there is some path from the root to $v$ of length $u.d+1$ obtained by appending $(u,v)$ to $v.d$. Since there is a path of that length, it serves as an upper bound on the minimum length of all such paths from the root to $v$.
\item
It is trivial that $0\le v.d$, since it is impossible to have a path from the root to $v$ of negative length. The more interesting inequality is $v.d\le u.d$. We know that there is some path from $v$ to $u$, consisting of tree edges, this is the defining property of $(u,v)$ being a back edge. This means that is $v,v_1,v_2,\ldots, v_k,u$ is this path (it is unique because the tree edges form a tree). Then, we have that $u.d = v_k.d +1 = v_{k-1}.d + 2 = \cdots = v_1.d + k = v.d + k+1$. So, we have that $u.d > v.d$.

In fact, we just showed that we have the stronger conclusion, that $0\le v.d < u.d$.
\end{enumerate}
\end{enumerate}

\noindent\textbf{Problem 22-2}\\
\begin{enumerate}[a.]
\item First suppose the root $r$ of $G_\pi$ is an articulation point. Then the removal of $r$ from $G$ would cause the graph to disconnect, so $r$ has at least 2 children in $G$.  If $r$ has only one child $v$ in $G_\pi$ then it must be the case that there is a path from $v$ to each of $r$'s other children. Since removing $r$ disconnects the graph, there must exist vertices $u$ and $w$ such that the only paths from $u$ to $w$ contain $r$. To reach $r$ from $u$, the path must first reach one of $r$'s children. This child is connect to $v$ via a path which doesn't contain $r$. To reach $w$, the path must also leave $r$ through one of its children, which is also reachable by $v$. This implies that there is a path from $u$ to $w$ which doesn't contain $r$, a contradiction.

Now suppose $r$ has at least two children $u$ and $v$ in $G_\pi$.  Then there is no path from $u$ to $v$ in $G$ which doesn't go through $r$, since otherwise $u$ would be an ancestor of $v$.  Thus, removing $r$ disconnects the component containing $u$ and the component containing $v$, so $r$ is an articulation point. \\

\item Suppose that $v$ is a nonroot vertex of $G_\pi$ and that $v$ has a child $s$ such that neither $s$ nor any of $s$'s descendents have back edges to a proper ancestor of $v$. Let $r$ be an ancestor of $v$, and remove $v$ from $G$.   Since we are in the undirected case, the only edges in the graph are tree edges or back edges, which means that every edge incident with $s$ takes us to a descendent of $s$, and no descendents have back edges, so at no point can we move up the tree by taking edges.  Therefore $r$ is unreachable from $s$, so the graph is disconnected and $v$ is an articulation point.  

Now suppose that for every child of $v$ there exists a descendent of that child which has a back edge to a proper ancestor of $v$.  Remove $v$ from $G$. Every subtree of $v$ is a connected component.  Within a given subtree, find the vertex which has a back edge to a proper ancestor of $v$.  Since the set $T$ of vertices which aren't descendents of $v$ form a connected component, we have that every subtree of $v$ is connected to $T$.  Thus, the graph remains connected after the deletion of $v$ so $v$ is not an articulation point. \\

\item Since $v$ is discovered before all of its descendants, the only back edges which could affect $v.low$ are ones which go from a descendant of $v$ to a proper ancestor of $v$. If we know $u.low$ for every child $u$ of $v$, then we can compute $v.low$ easily since all the information is coded in its descendents.  Thus, we can write the algorithm recursively:  If $v$ is a leaf in $G_\pi$ then $v.low$ is the minimum of $v.d$ and $w.d$ where $(v,w)$ is a back edge.  If $v$ is not a leaf, $v$ is the minimum of $v.d$, $w.d$ where $w$ is a back edge, and $u.low$, where $u$ is a child of $v$.  Computing $v.low$ for a vertex is linear in its degree.  The sum of the vertices' degrees gives twice the number of edges, so the total runtime is $O(E)$.\\

\item First apply the algorithm of part (c) in $O(E)$ to compute $v.low$ for all $v \in V$.  If $v.low = v.d$ if and only if no descendent of $v$ has a back edge to a proper ancestor of $v$, if and only if $v$ is not an articulation point.  Thus, we need only check $v.low$ versus $v.d$ to decide in constant time whether or not $v$ is an articulation point, so the runtime is $O(E)$.\\

\item An edge $(u,v)$ lies on a simple cycle if and only if there exists at least one path from $u$ to $v$ which doesn't contain the edge $(u,v)$, if and only if removing $(u,v)$ doesn't disconnect the graph, if and only if $(u,v)$ is not a bridge. \\

\item A edge $(u,v)$ lies on a simple cycle in an undirected graph if and only if either both of its endpoints are articulation points, or one of its endpoints is an articulation point and the other is a vertex of degree 1.  Since we can compute all articulation points in $O(E)$ and we can decide whether or not a vertex has degree 1 in constant time, we can run the algorithm in part $d$ and then decide whether each edge is a bridge in constant time, so we can find all bridges in$O(E)$ time. \\

\item It is clear that every nonbridge edge is in some biconnected component, so we need to show that if $C_1$ and $C_2$ are distinct biconnected components, then they contain no common edges.  Suppose to the contrary that $(u,v)$ is in both $C_1$ and $C_2$.  Let $(a,b)$ be any edge in $C_1$ and $(c,d)$ be any edge in $C_2$.  Then $(a,b)$ lies on a simple cycle with $(u,v)$, consisting of the path $a, b, p_1, \ldots, p_k, u, v, p_{k+1}, \ldots, p_n, a$.  Similarly, $(c,d)$ lies on a simple cycle with $(u,v)$ consisting of the path $c,d,q_1, \ldots, q_m, u,v, q_{m+1}, \ldots, q_l, c$.  This means $a,b,p_1, \ldots, p_k, u, q_m, \ldots, q_1, d, c, q_l, \ldots, q_{m+1}, v, p_{k+1}, \ldots, p_n, a$ is a simple cycle containing $(a,b)$ and $(c,d)$, a contradiction.  Thus, the biconnected components form a partition. \\

\item Locate all bridge edges in $O(E)$ time using the algorithm described in part f.  Remove each bridge from $E$.  The biconnected components are now simply the edges in the connected components.  Assuming this has been done, run the following algorithm, which clearly runs in $O(E)$ where $E$ is the number of edges \emph{originally} in $G$.\\

\begin{algorithm}
\caption{BCC(G)}
\begin{algorithmic}[1]
\For{each vertex $u \in G.V$}
	\State $u.color = white$
\EndFor
\State $k = 1$
\For{each vertex $u \in G.V$}
	\If{$u.color == white$}
		\State $k = k + 1$
		\State VISIT-BCC(G,u, k)
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{VISIT-BCC(G,u,k)}
\begin{algorithmic}[1]
\State $u.color = GRAY$
\For{each $v \in G.Adj[u]$}
	\State $(u,v).bcc = k$
	\If{$v.color == white$}
		\State VISIT-BCC$(G,v,k)$
	\EndIf
\EndFor 
\end{algorithmic}
\end{algorithm}
\end{enumerate}

\noindent\textbf{Problem 22-3}\\

\begin{enumerate}[a.]
\item
First, we'll show that it is necessary to have in degree equal out degree for each vertex. Suppose that there was some vertex $v$ for which the two were not equal, suppose wlog that in-degree - out-degree = a > 0. Note that we may assume that in degree is greater because otherwise we would just look at the transpose graph in which we traverse the cycle backwards. If $v$ is the start of the cycle as it is listed, just shift the starting and ending vertex to any other one on the cycle. Then, in whatever cycle we take going though $v$, we must pass through $v$ some number of times, in particular, after we pass through it $a$ times, the number of unused edges coming out of $v$ is zero, however, there are still unused edges goin in that we need to use. This means that there is no hope of using those while still being a tour, becase we would never be able to escape $v$ and get back to the vertex where the tour started.

Now, we show that it is sufficient to have the in degree and out degree equal for every vertex. To do this, we will generalize the problem slightly so that it is more amenable to an inductive approach. That is, we will show that for every graph $G$ that has two vertices $v$ and $u$ so that all the vertices have the same in and out degree except that the indegree is one greater for $u$ and the out degree is one greater for $v$, then there is an Euler path from $v$ to $u$. This clearly lines up with the original statement if we pick $u=v$ to be any vertex in the graph. We now perform induction on the number of edges. If there is only a single edge, then taking just that edge is an Euler tour. Then, suppose that we start at v and take any edge coming out of it. Consider the graph that is obtained from removing that edge, it inductively contains an Euler tour that we can just post-pend to the edge that we took to get out of $v$.\\

\item
To actually get the Euler circuit, we can just arbitrarily walk any way that we want so long as we don't repeat an edge, we will necessarily end up with a valid Euler tour. This is implemented in the following algorithm, EULER-TOUR(G) which takes time $O(|E|)$. It has this runtime because the for loop will get run for every edge, and takes a constant amount of time. Also, the process of initializing each edge's color will take time proportional to the number of edges.\\
\begin{algorithm}
\caption{EULER-TOUR(G)}
\begin{algorithmic}
\State color all edges white
\State let $(v,u)$ be any edge
\State let L be a list containing just $v$.
\While{there is some white edge $(v,w)$ coming out of $v$}
\State{color (v,w) black}
\State{v = w}
\State{append $v$ to $L$}
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{enumerate}

\noindent\textbf{Problem 22-4}\\

Begin by locating the element $v$ of minimal label.  We would like to make $u.min = v.label$ for all $u$ such that $u \rightsquigarrow v$.  Equivalently, this is the set of vertices $u$ which are reachable from $v$ in $G^T$.  We can implement the algorithm as follows, assuming that $u.min$ is initially set equal to $NIL$ for all vertices $u \in V$, and simply call the algorithm on $G^{T}$.\\

\begin{algorithm}
\caption{REACHABILITY(G)}
\begin{algorithmic}[1]
\State Use counting sort to sort the vertices by label from smallest to largest
\For{each vertex $u \in V$}
	\If{$u.min == NIL$}
		\State REACHABILITY-VISIT$(u, u.label)$
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}	
	
\begin{algorithm}
\caption{REACHABILITY-VISIT(u, k)}
\begin{algorithmic}[1]
\State $u.min = k$
\For{$v \in G.Adj[u]$}
	\If{$v.min == NIL$}
		\State REACHABILITY-VISIT$(v,k)$
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}



\end{document}
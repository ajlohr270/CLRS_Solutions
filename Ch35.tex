\documentclass{article}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{tikz}
	
\pagestyle{fancy}
\title{Chapter 35}
\author{Michelle Bodnar, Andrew Lohr}

\newcounter{curnum}
\setcounter{curnum}{0}

\newtheorem{th1}{Exercise} 
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\Z}{\mathbb{Z}}



\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parallel for}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}

\begin{document}
\maketitle

\noindent\textbf{Exercise 35.1-1}\\

We could select the graph that consists of only two vertices and a single edge between them. Then, the approximation algorithm will always select both of the vertices, whereas the minimum vertex cover is only one vertex. more generally, we could pick our graph to be k edges on a graph with $2k$ vertices so that each connected component only has a single edge. In these examples, we will have that the approximate solution is off by a factor of two from the exact one.\\



\noindent\textbf{Exercise 35.1-3}\\

We will construct a bipartite graph with $V = R \cup L$. We will try to construct it so that $R$ is uniform, not that $R$ is a vertex cover. However, we will make it so that the heuristic that the professor (professor who?) suggests will cause us to select all the vertices in $L$, and show that $|L| > 2|R|$.

Initially start off with $|R|=n$ fixed, and $L$ empty. Then, for each $i$ from 2 up to $n$, we do the following. Let $k = \lfloor \frac{n}{i}\rfloor$. Select $S$ a subset of the vertices of $R$ of size $ki$, and so that all the vertices in $R-S$ have a greater or equal degree. Then, we will add $k$ vertices to $L$, each of degree $i$, so that the union of their neighborhoods is $S$. Note that throughout this process, the furthest apart the degrees of the vertices in $R$ can be is $1$, because each time we are picking the smallest degree vertices and increasing their degrees by 1. So, once this has been done for $i=n$, we can pick a subset of $R$ whose degree is one less than the rest of $R$ (or all of $R$ if the degrees are all equal), and for each vertex in that set we picked, add a single vertex to $L$ whose only neighbor is the one in $R$ that we picked. This clearly makes $R$ uniform.

Also, lets see what happens as we apply the heuristic. Note that each vertex in $R$ only has at most a single vertex of each degree in $L$. This means that as we remove vertices from $L$ along with their incident edges, we will always have that the highest degree vertex remaining in $L$ is greater or equal to the highest degree vertex in $R$. This means that we can always, by this heuristic, continue to select vertices from $L$ instead of $R$. So, when we are done, we have selected, by this heuristic, that our vertex cover is all of $L$. 

Lastly, we need to show that $L$ is big enough relative to $R$. The size of $L$ is greater or equal to $\sum_{i=2}^n \lfloor\frac{n}{i}\rfloor $ where we ignore any of the degree 1 vertices we may of added to $L$ in our last step. This sum can be bounded below by the integral $\int_{i=2}^{n+1} \frac{n}{x}dx$ by formula (A.12). Elementary calculus tells us that this integral evaluates to $n(\lg(n+1) - 1)$, so, we can clearly select $n>8$ to make it so that 
\begin{align*}
2|R| &= 2n\\
 &< n(\lg(8+1) -1)\\
  &\le n(\lg(n+1) - 1)\\
  &=\int_{i=2}^{n+1} \frac{n}{x}dx\\
  &\le \sum_{i=2}^n \lfloor\frac{n}{i}\rfloor\\
  &\le |L|
  \end{align*}
  Since we selected $L$ as our vertex cover, and $L$ is more than twice the size of the vertex cover $R$, we do not have a 2- approximation using this heuristic. In fact, we have just shown that this heuristic is not a $\rho$ approximation for any constant $\rho$.\\
  
  


\noindent\textbf{Exercise 35.1-5}\\

It does not imply the existence of an approximation algorithm for the maximum size clique. Suppose that we had in a graph of size $n$, the size of the smallest vertex cover was $k$, and we found one that was size $2k$. This means that in the compliment, we found a clique of size $n-2k$, when the true size of the largest clique was $n-k$. So, to have it be a constant factor approximation, we need that there is some $\lambda$ so that we always have $n-2k \ge \lambda (n-k)$. However, if we had that $k$ was close to $\frac{n}{2}$ for our graphs, say $\frac{n}{2}-\epsilon$, then we would have to require $2\epsilon \ge \lambda \frac{n}{2} + \epsilon$. Since we can make $\epsilon$ stay tiny, even as $n$ grows large, this inequality cannot possibly hold.\\




\noindent\textbf{Exercise 35.2-1}\\

Suppose that $c(u,v) < 0$ and $w$ is any other vertex in the graph. Then, to have the trianle inequality satisfied, we need $c(w,u) \le c(w,v) + c(u,v)$. Now though, subtract $c(u,v)$ from both sides, we get $c(w,v) \ge c(w,u) - c(u,v) > c(w,u) +c(u,v)$ so, it is impossible to have $c(w,v) \le c(w,u)+c(u,v)$ as the triangle inequality would require.\\



\noindent\textbf{Exercise 35.2-3}\\

From the chapter on minimum spanning trees, recall Prim's algorithm. That is, a minimum spanning tree can be found by repeatedly finding the nearest vertex to the vertices already considered, and adding it to the tree, being adjacent to the vertex among the already considered vertices that is closest to it. Note also, that we can recusively define the preorder traversal of a tree by saying that we first visit our parent, then ourselves, then our children, before returning priority to our parent. This means that by inserting vertices into the cycle in this way, we are always considering the parent of a vertex before the child. To see this, suppose we are adding vertex $v$ as a child to vertex $u$. This means that in the minimum spanning tree rooted at the first vertex, $v$ is a child of $u$. So, we need to consider $u$ first before considering $v$, and then consider the vertex that we would of considered after $v$ in the previous preoreder traversal. This is precisely achieved by inserting $v$ into the cycle in such a manner. Since the property of the cycle being a preoreder traversal for the minimum spanning tree constructed so far is maintained at each step, it is the case at the end as well, once we have finished considering all the vertices.  So, by the end of the process, we have constructed the preorder traversal of a minimum spanning tree, even though we never explicity built the spanning tree. It was shown in the section that such a Hamiltonian cycle will be a 2 approximation for the cheapest cycle under the given assumption that the weights satisfy the triangle inequality.



\noindent\textbf{Exercise 35.2-5}\\

To show that the optimal tour never crosses itself, we will suppose that it did cross itself, and then show that we could produce a new tour that had lower cost, obtaining our contradiction because we had said that the tour we had started with was optimal, and so, was of minimal cost. If the tour crosses itself, there must be two pairs of vertices that are both adjacent in the tour, so that the edge between the two pairs are crossing each other. Suppose that our tour is $S_1x_1 x_2 S_2 y_1 y_2 S_3$ where $S_1,S_2,S_3$ are arbitrary sequences of vertices, and $\{x_1,x_2\}$ and $\{y_1,y_2\}$ are the two crossing pairs. Then, We claim that the tour given by $S_1 x_1 y_2 \hbox{Reverse}(S_2) y_1 x_2 S_3$ has lower cost. Since $\{x_1,x_2,y_1,y_2\}$ form a quadrilateral, and the original cost was the sum of the two diagonal lengths, and the new cost is the sums of the lengths of two opposite sides, this problem comes down to a simple geometry problem.

Now that we have it down to a geometry problem, we just exercise our grade school geometry muscles. Let $P$ be the point that diagonals of the quadrilateral intersect. Then, we have that the vertices $\{x_1,P,y_2\}$ and $\{x_2,P,y_1\}$ form triangles. Then, we recall that the longest that one side of a triangle can be is the sum of the other two sides, and the inequality is strict if the triangles are non-degenerate, as  they are in our case. This means that $||x_1y_2|| + ||x_2y_1|| < ||x_1P|| +||Py_2|| + ||x_2P|| + ||Py_1|| = ||x_1x_2|| + ||y_1y_2||$. The right hand side was the original contribution to the cost from the old two edges, while the left is the new contribution. This means that this part of the cost has decreased, while the rest of the costs in the path have remained the same. This gets us that the new path that we constructed has strictly better cross, so we must not of had an optimal tour that had a crossing in it.\\



\noindent\textbf{Exercise 35.3-1}\\

Since all of the words have no repeated letters, the first word selected will be the one that appears earliest on among those with the most letters, this is ``thread''. Now, we look among the words that are left, seeing how many letters that aren't already covered that they contain. Since ``lost'' has four letters that have not been mentioned yet,and it is first among those that do, that is the next one we select. The next one we pick is ``drain'' because it is has two unmentioned letters. This only leave ``shun'' having any unmentioned letters, so we pick that, completing our set. So, the final set of words in our cover is $\{thread,lost,drain,shun\}$.\\



\noindent\textbf{Exercise 35.3-3}\\

See the algorithm LINEAR-GREEDY-SET-COVER. Note that everything in the inner most for loop takes constant time and will only run once for each pair of letter and a set containing that letter. However, if we sum up the number of such pairs, we get $\sum_{S\in\mathcal{F}} |S|$ which is what we wanted to be linear in.

\begin{algorithm}
\caption{LINEAR-GREEDY-SET-COVER($\mathcal{F}$)}
\begin{algorithmic}
\State compute the sizes of every $S\in \mathcal{F}$, storing them in $S.size$.
\State let $A$ be an array of length $\max_S |S|$, consisting of empty lists
\For{$S\in \mathcal{F}$}
\State add $S$ to $A[S.size]$.
\EndFor
\State let $A.max$ be the index of the largest nonempty list in $A$.
\State let $L$ be an array of length $|\cup_{S\in\mathcal{F}} S|$ consisting of empty lists
\For{$S\in \mathcal{F}$}
\For{$\ell \in S$}
\State add $S$ to $L[\ell]$
\EndFor
\EndFor
\State let $C$ be the set cover that we will be selecting, initially empty.
\State let $T$ be the set of letters that have been covered, initially empty.
\While{$A.max >0$}
\State Let $S_0$ be any element of $A[A.max]$.
\State add $S_0$ to $C$
\State remove $S_0$ from $A[A.max]$
\For{$\ell\in S_0\setminus T$}
\For{$S \in L[\ell]$}
\State Remove $S$ from $A[A.size]$
\State S.size = S.size -1
\State Add $S$ to $A[S.size]$
\If{A[A.max] is empty}
\State A.max = A.max-1
\EndIf
\EndFor
\State add $\ell$ to $T$
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}



\noindent\textbf{Exercise 35.3-5}\\

Suppose we pick the number of elements in our base set to be a multiple of four. We will describe a set of sets on a base set of size $4$, and then take $n$ disjoint copies of it. For each of the four element base sets, we will have two choices for how to select the set cover of those 4 elements. For a base set consisting of $\{1,2,3,4\}$, we pick our set of sets to be $\{ \{1,2\},\{1,3\},\{3,4\},\{2,4\}\}$, then we could select either $\{\{1,2\},\{3,4\}\}$ or $\{\{1,3\},\{2,4\}\}$. This means that we then have a set cover problem with $4n$ sets, each of size two where the set cover we select is done by $n$ independent choices, and so there are $2^n$ possible final set covers based on how ties are resolved.\\



\noindent\textbf{Exercise 35.4-1}\\

Any clause that contains both a variable and its negation is automatically satisfied, since we always have $x \vee \neg x$ is true regardless of the truth value of $x$. This means that if we separate out the clauses that contain no variable and its negation, we have an 8/7ths approximation on those, and we have all of the other clauses satisfied regardless of the truth assignments to variables. So, the quality of the approximation just improves when we include the clauses containing variables and their negation.\\



\noindent\textbf{Exercise 35.4-3}\\

Lets consider the expected value of the weight of the cut. For each edge, the probability that that edge crosses the cut is the probability that our coin flips came up differently for the two vertices that are in the edge. This happens with probability $\frac{1}{2}$. Therefore, by linearity of expectation, we know that the expected weight of this cut is $\frac{|E|}{2}$. We know that for the best cut we can make, the weight will be bounded by the total number of edges in the graph, so we have that the weight of the best cut is at most twice the expected weight of this random cut.\\



\noindent\textbf{Exercise 35.5-1}\\

Every subset of $\{1,2,\ldots, i\}$ can either contain $i$ or not contain $i$. If it does contain $i$, the sum corresponding to that subset will be in $P_{i-1} +x_i$, since the sum corresponding to the restriction of the subset to $\{1,2,3,\ldots\}$ will be in $P_{i-1}$. If we are in the other case, then when we restrict to $\{1,2,\ldots, i-1\}$ we aren't changing the subset at all. Similarly, if an element is in $\P_{i-1}$, that same subset is in $P_i$. If an element is in $P_{i-1}+x_i$, then it is in $P_i$ where we just make sure our subset contains $i$. This proves (35.23).

It is clear that $L$ is a sorted list of elements less than $T$ that corresponds to subsets of the empty set before executing the for loop, because it is empty. Then, since we made $L_i$ from elements in $L_{i-1}$ and $x_i + L{i-1}$, we know that it will be a subset of those sums obtained from the first $i$ elements. Also, since both $L_{i-1}$ and $x_i + L_{i-1}$ were sorted, when we merged them, we get a sorted list. Lastly, we know that it all sums except those that are more than $t$ since we have all the subset sums that are not more than $t$ in $L_{i-1}$, and we cut out anything that gets to be more than $t$.\\



\noindent\textbf{Exercise 35.5-3}\\

As we have many times in the past, we'll but on our freshman calculus hats. 
\begin{align*}
\frac{d}{dn} \left( 1+ \frac{\epsilon}{2n}\right)^n&=\\
\frac{d}{dn} e^{n\lg\left( 1+ \frac{\epsilon}{2n}\right)}&=\\
e^{n\lg\left( 1+ \frac{\epsilon}{2n}\right)}\left(\left( 1+ \frac{\epsilon}{2n}\right) + \frac{n\frac{-\epsilon}{2n^2}}{1 + \frac{\epsilon}{2n}}\right)&=\\
e^{n\lg\left( 1+ \frac{\epsilon}{2n}\right)}\left(\frac{\left( 1+ \frac{\epsilon}{2n}\right)^2  - \frac{\epsilon}{2n}}{1 + \frac{\epsilon}{2n}}\right)&=\\
e^{n\lg\left( 1+ \frac{\epsilon}{2n}\right)}\left(\frac{ 1+ \frac{\epsilon}{2n}+ \left(\frac{\epsilon}{2n}\right)^2  }{1 + \frac{\epsilon}{2n}}\right)&=\end{align*}

Since all three factors are always positive, the original expression is always positive.\\



\noindent\textbf{Exercise 35.5-5}\\

We can modify the procedure APPROX-SUBSET-SUM to actually report the subset corresponding to each sum by giving each entry in our $L_i$ lists a piece of satellite data which is a list the elements that add up to the given number sitting in $L_i$. Even if we do a stupid representation of the sets of elements, we will still have that the runtime of this modified procedure only takes additional time by a factor of the size of the original set, and so will still be polynomial. Since the actual selection of the sum that is our approximate best is unchanged by doing this, all of the work spent in the chapter to prove correctness of the approximation scheme still holds. Each time that we put an element into $L_i$, it could of been copied directly from $L_{i-1}$, in which case we copy the satellite data as well. The only other possibility is that it is $x_i$ plus an entry in $L_{i-1}$, in which case, we add $x_i$ to a copy of the satellite data from $L_{i-1}$ to make our new set of elements to associate with the element in $L_i$.\\



\noindent\textbf{Problem 35-1}\\
\begin{enumerate}[a.]
\item 
First, we will show that it is np hard to determine if, given a set of numbers, there is a subset of them whose sum is equal to the sum of that set's compliment, this is called the partition problem. We will show this is NP hard by using it to solve the subset sum problem. Suppose that we wanted to fund a subset of $\{y_1,y_2, \ldots y_n\}$ that summed to $t$. Then, we will run partition on the set $\{y_1,y_2,\ldots, y_n, \left(\sum_i y_i\right) - 2t\}$. Note then, that the sum of all the elements in this set is $2\left(\sum_i y_i \right) -2t$. So, any partition must have both sides equal to half that. If there is a subset of the original set that sums to $t$, we can put that on one side together with the element we add, and put all the other original elements on the other side, giving us a partition. Suppose that we had a partition, then all the elements that are on the same side as the special added element form a subset that is equal to $t$. This show that the partition problem is NP hard.

Let $\{x_1,x_2,\ldots,x_n\}$ be an instance of the partition problem. Then, we will construct an instance of a packing problem that can be packed into 2 bins if and only if there is a subset of $\{x_1,x_2,\ldots x_n\}$ with sum equal to that of its compliment. To do this, we will just let our values be $c_i = \frac{2x_i}{\sum_{j=1}^n x_j}$. Then, the total weight is exactly 2, so clearly two bins are needed. Also, it can fit in two bins if there is a partition of the original set.

\begin{comment}
  First, sort the items from largest to smallest. We will use a polynomial time solution to the bin packing problem to, in polynomial time, decide whether or not the largest item is in a solution to the subset sum problem. Then, based on that we get an instance of the subset sum problem which has one fewer items. Continuing in this, way, we either construct a subset that sums to the given value, or show that none of the items can be in a solution, meaning that no solution can exist.

Let $\epsilon$ be a number less than $\frac{1}{tn}$. Let $x_i' = x_i /t$ and $x_1'' = x_1' + \epsilon$. Then, we consider the instances of the bin packing problem in which we have $1/x_1'$ instances of the elements $x_i'$. We also consider the instance of the bin packing problem in which we have $1/x_1'$ instances of $\{x_1'', x_2',\ldots,x_n'\}$. If these two values for the bin packing differ, then we must have that $x_1'$ was in a bin that added up to exactly $1$ in the first instance, which means that there was a solution to the original subset sum problem containing the first element. If they are the same then that means that whatever bin that $x_1'$ ended up in the first instance, it was summing up to some value less than one, so was not part of a solution to the subset sum problem.
\end{comment}
\item
If we could pack the elements in to fewer than $\lceil S \rceil$ bins, that means that the total capacity from our bins is $< \lfloor S \rfloor \le S$, however, we know that to total space needed to store the elements is $S$, so there is no way we could fit them in less than that amount of space. 
\item
Suppose that there was already one bin that is at most half full, then when we go to insert an element that has size at most half, it will not go into an unused bin because it would be placed in this bin that is at most half full. This means that we will never create a second bin that is at most half full, and so, since we start off with fewer that two bins that are at most half full, there will never be two bins that are at most half full (and so never two that are less than half full).
\item
Since there is at most one bin that is less than half full, we know that the total amount of size that we have packed into our $P$ bins is $> \frac{1}{2}(P-1)$. That is, we know that $S > \frac{1}{2} (P-1)$, which tells us that $2S+1 > P$. So, if we were to have $P>\lceil 2S \rceil$, then, we have that $P \ge \lceil 2S\rceil +1 = \lceil 2S +1 \rceil \ge 2S+1 > P$, which is a contradiction.
\item
We know from part $b$ that there is a minimum of $\lceil S\rceil$ bins required. Also, by part $d$, we know that the first fit heuristic causes us to sue at most $\lceil 2S\rceil$ bins. This means that the number of bins we report is at most off by a factor of $\frac{\lceil 2S\rceil}{\lceil S \rceil} \le 2$.
\item
We can implement the first fit heuristic by just keeping an array of bins, and each time just doing a linear scan to find the first bin that can accept it. So, if we let $P$ be the number of bins, this procedure will have a runtime of $O(P^2) = O(n^2)$. However, since all we needed was that there was at most one bin that was less than half full, we could do a faster procedure. We keep the array of bins as before, but we will keep track of the first empty bin and a pointer to a bin that is less than half full. Upon insertion, if the thing we inserted is more than half, put it in the first empty bin and increment it. If it is less than half, it will fit in the bin that is less than half full, if that bin then becomes more than half full, get rid of the less than half full bin pointer. If there wasn't a nonempty bin that was less than half full, we just put the element in the first empty bin. This will run in time $O(n)$ and still satisfy everything we needed to assure we had a 2-approximation.
\end{enumerate}



\noindent\textbf{Problem 35-3}\\

An obvious way to generalize the greedy set cover heuristic is to not just pick the set that covers the most uncovered elements, but instead to repeatedly select the set that maximizes the value of the number of uncovered points it covers divided by its weight. This agrees with the original algorithm in the case that the weights are all 1. The main difference is that instead of each step of the algorithm assigning one unit of cost, it will be assigning $w_i$ units of cost to add the set $S_i$. So, suppose that $\mathcal{C}$ is the cover we selected by this heuristic, and $\mathcal{C}^*$ is an optimal cover. So, we will have $|\sum_{S_i\in \mathcal{C}} w_i| = \sum_{x\in X} w_ic_x \le \sum_{S_i \in \mathcal{C}^*} \sum_{x\in S_i} w_i c_x$. Then, $\sum_{x\in S_i} w_ic_x = w_i \sum_{x\in S_i} c_x \le w_i H(|S|) $. So, $|\mathcal{C}| \le \sum_{S_i\in \mathcal{C}^*}  w_i H(|S_i|) \le |\sum_{S_i\in \mathcal{C}^*} w_i| H(\max(|S_i|) $. This means that the weight of the selected cover is a $H(d)$ approximation for the weight of the optimal cover.



\noindent\textbf{Problem 35-5}\\
\begin{enumerate}[a.]
\item
Suppose that the greatest processing time is $p_i$. Then, any schedule must assign the job $J_i$ to some processor. If it assigns $J_i$ to start as soon as possible, it still won't finish until $p_i$ units have passed. This means that since the makespan is the time that all jobs have finished, it will at or after $p_i$ because it needs job $J_i$ to of finished. This problem can be viewed as a restatement of (27.3), where we have an infinite number of processors and assign each job to its own processor.
\item
The total amount of processing per step of time is one per processor. Since we need to accomplish $\sum_{i} p_i$ much work, we have to of spent at lest $\frac{1}{m}$ times that amount of time. This problem is a restatement of equation (27.2).
\item
See the algorithm GREEDY-SCHEDULE

\begin{algorithm}
\caption{GREEDY-SCHEDULE}
\begin{algorithmic}
\State for every $i=1,\ldots,m$, let $f_i =0$. We will be updating this to be the latest time that we have any task running on processor $i$.
\State put all of the $f_i$ into a min heap, H
\While{There is an unassigned job $J_i$}
\State extract the min element of $H$, let it be $f_j$
\State assign job $J_i$ to processor $M_j$, to run from $f_j$ to $f_j + p_i$.
\State $f_j = f_j +p_i$
\State add $f_j$ to $H$
\EndWhile
\end{algorithmic}
\end{algorithm}

Since we can perform all of the heap operations in time $O(\lg(m))$, and we need to perform one for each of our jobs, the runtime is in $O(n\lg(m))$.

\item
This is Theorem 27.1 and Corollary 27.2.
\end{enumerate}


\noindent\textbf{Problem 35-7}\\
\begin{enumerate}[a.]
\item Though not stated, this conclusion requires the assumption that at least one of the values is non-negative. Since any individual item will fit, selecting that one item would be a solution that is better then the solution of taking no items. We know then that the optimal solution contains at least one item. Suppose the item of largest index that it contains is item $i$, then, that solution would be in $P_i$, since the only restriction we placed when we changed the instance to $I_i$ was that it contained item $i$.
\item
We clearly need to include all of item $j$, as that is a condition of instance $I_j$. Then, since we know that we will be using up all of the remaining space somehow, we want to use it up in such a way that the average value of that space is maximized. This is clearly achieved by maximizing the value density of all the items going in, since the final value density will be an average of the value densities that went into it weighted by the amount of space that was used up by items with that value density. 
\item
Since we are greedily selecting items based off of the amount of value per space, we only stop once we can no longer fit any more, and each time before putting some faction of the item with the next least value per space, we complete putting in the item we currently are. This means that there is at most one item fractionally.
\item
First, it is trivial that $v(Q_j)/2 \ge v(P_j)/2$ since we are trying to maximize a quantity, and there are strictly fewer restrictions on what we can do in the case of the fractional packing than in the 1-0 packing. To see that $v(R_j) \ge v(Q_j)/2$, note that among the items $\{1,2,\ldots, j\}$, we have the highest value item is $v_j$ because of our ordering of the items. We know that the fractional solution must have all of item $j$, and there is some item that it may not have all of that is indexed by less than $j$. This part of an item is all that is thrown out when going from $Q_j$ to $R_j$. Even if we threw all of that item out, it still wouldn't be as valuable as item $j$ which we are keeping, so, we have retained more than half of the value. So, we have proved the slightly stronger statement that $v(R_j) > v(Q_j)/2 \ge v(P_j)/2$.
\item
Since we knew that the optimal solution was the max of all the $P_j$ (part a), and we know that each $P_j$ is at most $2R_j$(part d), by selecting the max of all the the $R_j$, we are obtaining a solution that is at most a factor of two less than the optimal solution.
\[
\max_j R_j \le \max_j P_j \le \max_j 2R_j
\]
\end{enumerate}

\end{document}